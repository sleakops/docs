---
sidebar_position: 3
title: "Volúmenes Compartidos en Kubernetes entre Namespaces"
description: "Solución para compartir volúmenes entre pods en diferentes namespaces y enfoques alternativos"
date: "2025-01-30"
category: "cluster"
tags: ["kubernetes", "volúmenes", "namespaces", "almacenamiento", "s3"]
---

import TroubleshootingItem from "@site/src/components/HomepageFeatures/troubleshootingitem";

# Volúmenes Compartidos en Kubernetes entre Namespaces

**Fecha:** 30 de enero de 2025  
**Categoría:** Clúster  
**Etiquetas:** Kubernetes, Volúmenes, Namespaces, Almacenamiento, S3

## Descripción del Problema

**Contexto:** El usuario necesita compartir datos entre un cronjob que genera archivos y un servicio nginx que los sirve, pero están ejecutándose en diferentes namespaces dentro de un clúster de Kubernetes.

**Síntomas Observados:**

- No se puede montar el mismo volumen en diferentes namespaces
- Necesidad de compartir archivos generados entre el cronjob y el servidor web
- La configuración actual funciona en EC2 con montaje de volumen compartido
- Se busca el equivalente en Kubernetes para acceso compartido a volúmenes

**Configuración Relevante:**

- Plataforma: SleakOps en AWS EKS
- Caso de uso: Cronjob genera archivos, Nginx los sirve
- Configuración actual: EC2 con volumen compartido entre contenedores
- Objetivo: Pods de Kubernetes en diferentes namespaces

**Condiciones de Error:**

- Limitación de Kubernetes: no se puede usar el mismo volumen en diferentes namespaces
- Necesidad de solución alternativa para compartir archivos
- Requisitos de rendimiento para generación de archivos grandes (más de 5GB)

## Solución Detallada

<TroubleshootingItem id="kubernetes-volume-limitation" summary="Entendiendo las limitaciones de volúmenes en Kubernetes">

Kubernetes tiene una limitación fundamental: **el mismo PersistentVolume no puede ser montado por pods en diferentes namespaces**. Esto es por diseño para propósitos de seguridad e aislamiento.

Esto significa que el enfoque actual en EC2 de compartir un volumen entre contenedores no funcionará directamente en Kubernetes cuando los pods estén en diferentes namespaces.

</TroubleshootingItem>

<TroubleshootingItem id="s3-solution" summary="Solución recomendada: Usar S3 como almacenamiento compartido">

El enfoque recomendado es usar **Amazon S3** como almacenamiento intermedio:

### Arquitectura:

1. **Cronjob**: Genera archivos localmente → Los sube a S3
2. **Servicio Nginx**: Descarga archivos desde S3 → Los sirve

### Beneficios:

- Funciona a través de namespaces
- Escalable y confiable
- Rentable para archivos grandes
- Autenticación integrada vía cuentas de servicio de SleakOps

### Ejemplo de configuración:

```yaml
# Configuración del Cronjob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: generador-de-archivos
  namespace: jobs
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: generador
              image: tu-app:latest
              env:
                - name: S3_BUCKET
                  value: "tu-nombre-de-bucket"
                - name: AWS_REGION
                  value: "us-east-1"
              volumeMounts:
                - name: almacenamiento-temporal
                  mountPath: /tmp/files
          volumes:
            - name: almacenamiento-temporal
              emptyDir:
                sizeLimit: 60Gi
```

</TroubleshootingItem>

<TroubleshootingItem id="nodepool-configuration" summary="Configurar nodepool para almacenamiento adecuado">

Para la generación de archivos grandes, configura tu nodepool con suficiente almacenamiento EBS:

### En SleakOps:

1. Ve a **Configuración de Clúster**
2. Selecciona tu **Nodepool**
3. Modifica la **Configuración del Nodo**:
   - **Tamaño del volumen EBS**: 50-60 GB
   - **Tipo de volumen**: gp3 (más rápido y económico)

### Ejemplo de configuración:

```yaml
nodepool_config:
  instance_type: "t3.medium"
  disk_size: 60 # GB
  disk_type: "gp3"
  min_nodes: 1
  max_nodes: 5
```

</TroubleshootingItem>

<TroubleshootingItem id="s3-authentication" summary="Autenticación en S3 con SleakOps">

SleakOps configura automáticamente la autenticación para S3 mediante **cuentas de servicio**. No necesitas gestionar credenciales de AWS manualmente.

### Ejemplo en Java para acceso a S3:

```java
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.model.*;

public class S3FileUploader {
    public static void main(String[] args) {
        // SleakOps maneja la autenticación automáticamente
        S3Client s3 = S3Client.builder()
                               .region(Region.US_EAST_1)
                               .build();

        // Subir archivo a S3
        PutObjectRequest putRequest = PutObjectRequest.builder()
                .bucket("tu-nombre-de-bucket")
                .key("generated-files/data.zip")
                .build();

        s3.putObject(putRequest,
                    RequestBody.fromFile(new File("/tmp/files/data.zip")));
    }
}
```

### Ejemplo en Python:

```python
import boto3
import os

# SleakOps maneja la autenticación vía cuenta de servicio
s3_client = boto3.client('s3')
bucket_name = os.environ['S3_BUCKET']

# Subir archivo generado
s3_client.upload_file(
    '/tmp/files/generated_data.zip',
    bucket_name,
    'generated-files/generated_data.zip'
)
```

</TroubleshootingItem>

<TroubleshootingItem id="nginx-s3-integration" summary="Configurar Nginx para servir archivos desde S3">

Para servir archivos desde S3 a través de Nginx, tienes varias opciones:

### Opción 1: Nginx con proxy a S3

```nginx
server {
    listen 80;
    server_name tu-dominio.com;

    location /files/ {
        proxy_pass https://tu-bucket.s3.amazonaws.com/;
        proxy_set_header Host tu-bucket.s3.amazonaws.com;
        proxy_hide_header x-amz-id-2;
        proxy_hide_header x-amz-request-id;
    }
}
```

### Opción 2: Descargar y servir localmente

```bash
#!/bin/bash
# Script de inicio para el contenedor nginx
aws s3 sync s3://tu-bucket/generated-files/ /usr/share/nginx/html/files/
nginx -g "daemon off;"
```

### Opción 3: Usar hosting estático de sitio web en S3

Habilita el hosting estático en tu bucket S3 y apunta tu dominio directamente a S3.

</TroubleshootingItem>

<TroubleshootingItem id="alternative-solutions" summary="Soluciones alternativas dentro de Kubernetes">

Si debes mantener todo dentro de Kubernetes:

### Opción 1: Mismo namespace

Mueve tanto el cronjob como nginx al mismo namespace para compartir volúmenes.

### Opción 2: NFS o EFS

Usa Amazon EFS (Elastic File System), que puede ser montado a través de namespaces:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-pv
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-12345678  # Tu EFS ID
```

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-claim
  namespace: jobs  # Namespace del cronjob
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 100Gi
```

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-claim
  namespace: web  # Namespace de nginx
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 100Gi
```

### Opción 3: Usar un servicio intermedio

Crea un servicio API que maneje la transferencia de archivos:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: file-service
  namespace: shared
spec:
  replicas: 1
  selector:
    matchLabels:
      app: file-service
  template:
    metadata:
      labels:
        app: file-service
    spec:
      containers:
      - name: file-service
        image: file-service:latest
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: shared-storage
          mountPath: /shared
      volumes:
      - name: shared-storage
        persistentVolumeClaim:
          claimName: shared-pvc
```

</TroubleshootingItem>

<TroubleshootingItem id="performance-optimization" summary="Optimización de rendimiento para archivos grandes">

Para manejar archivos de más de 5GB eficientemente:

### 1. Configuración de recursos del pod:

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: generador-archivos-grandes
spec:
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: generador
            image: tu-app:latest
            resources:
              requests:
                memory: "4Gi"
                cpu: "2"
                ephemeral-storage: "60Gi"
              limits:
                memory: "8Gi"
                cpu: "4"
                ephemeral-storage: "80Gi"
            volumeMounts:
            - name: temp-storage
              mountPath: /tmp/large-files
          volumes:
          - name: temp-storage
            emptyDir:
              sizeLimit: 70Gi
```

### 2. Subida multipart a S3:

```python
import boto3
from boto3.s3.transfer import TransferConfig

def upload_large_file(file_path, bucket, key):
    # Configuración para archivos grandes
    config = TransferConfig(
        multipart_threshold=1024 * 25,  # 25MB
        max_concurrency=10,
        multipart_chunksize=1024 * 25,
        use_threads=True
    )
    
    s3_client = boto3.client('s3')
    s3_client.upload_file(
        file_path, bucket, key,
        Config=config
    )
```

### 3. Compresión antes de subir:

```bash
#!/bin/bash
# Script para comprimir y subir archivos grandes
cd /tmp/files

# Comprimir archivos
tar -czf data_$(date +%Y%m%d_%H%M%S).tar.gz *.data

# Subir a S3
aws s3 cp data_*.tar.gz s3://$S3_BUCKET/compressed-files/

# Limpiar archivos temporales
rm -f *.data *.tar.gz
```

</TroubleshootingItem>

<TroubleshootingItem id="monitoring-and-troubleshooting" summary="Monitoreo y solución de problemas">

### Monitoreo de uso de almacenamiento:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-script
data:
  monitor.sh: |
    #!/bin/bash
    while true; do
      echo "=== Uso de almacenamiento $(date) ==="
      df -h /tmp/files
      echo "=== Archivos en directorio ==="
      ls -lah /tmp/files/
      echo "=== Memoria del pod ==="
      free -h
      sleep 300  # Cada 5 minutos
    done
```

### Verificación de conectividad S3:

```bash
# Script de verificación
#!/bin/bash
echo "Verificando conectividad con S3..."

# Verificar credenciales
aws sts get-caller-identity

# Verificar acceso al bucket
aws s3 ls s3://$S3_BUCKET/

# Probar subida de archivo de prueba
echo "test" > /tmp/test.txt
aws s3 cp /tmp/test.txt s3://$S3_BUCKET/test/
aws s3 rm s3://$S3_BUCKET/test/test.txt
rm /tmp/test.txt

echo "Verificación completada"
```

### Logs y debugging:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: debug-pod
spec:
  containers:
  - name: debug
    image: amazon/aws-cli:latest
    command: ["/bin/bash"]
    args: ["-c", "while true; do sleep 3600; done"]
    env:
    - name: S3_BUCKET
      value: "tu-bucket"
    volumeMounts:
    - name: debug-storage
      mountPath: /debug
  volumes:
  - name: debug-storage
    emptyDir: {}
```

</TroubleshootingItem>

<TroubleshootingItem id="best-practices" summary="Mejores prácticas y recomendaciones">

### 1. Gestión del ciclo de vida de archivos:

```yaml
# Política de ciclo de vida en S3
{
  "Rules": [
    {
      "ID": "ArchiveOldFiles",
      "Status": "Enabled",
      "Filter": {
        "Prefix": "generated-files/"
      },
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 90,
          "StorageClass": "GLACIER"
        }
      ]
    }
  ]
}
```

### 2. Seguridad y acceso:

```yaml
# ServiceAccount con permisos específicos para S3
apiVersion: v1
kind: ServiceAccount
metadata:
  name: s3-access-sa
  namespace: jobs
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT:role/S3AccessRole
```

### 3. Configuración de retry y timeout:

```python
import boto3
from botocore.config import Config

# Configuración robusta para S3
config = Config(
    region_name='us-east-1',
    retries={
        'max_attempts': 10,
        'mode': 'adaptive'
    },
    max_pool_connections=50
)

s3_client = boto3.client('s3', config=config)
```

### 4. Notificaciones y alertas:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: notification-config
data:
  notify.py: |
    import boto3
    import json
    
    def send_notification(message):
        sns = boto3.client('sns')
        sns.publish(
            TopicArn='arn:aws:sns:us-east-1:ACCOUNT:file-processing',
            Message=json.dumps({
                'status': 'completed',
                'timestamp': str(datetime.now()),
                'message': message
            })
        )
```

### 5. Backup y recuperación:

```bash
#!/bin/bash
# Script de backup automático
BACKUP_BUCKET="tu-bucket-backup"
SOURCE_BUCKET="tu-bucket"

# Sincronizar buckets
aws s3 sync s3://$SOURCE_BUCKET/ s3://$BACKUP_BUCKET/ \
  --delete \
  --storage-class STANDARD_IA

# Verificar integridad
aws s3api head-object --bucket $BACKUP_BUCKET --key generated-files/latest.zip
```

</TroubleshootingItem>

---

_Esta sección de preguntas frecuentes fue generada automáticamente el 30 de enero de 2025 basada en una consulta real de usuario._
```
