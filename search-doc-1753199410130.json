{"searchDocs":[{"title":"CLI","type":0,"sectionRef":"#","url":"/cli","content":"","keywords":"","version":"Next"},{"title":"Streamline CI/CD with SleakOps CLIâ€‹","type":1,"pageTitle":"CLI","url":"/cli#streamline-cicd-with-sleakops-cli","content":" SleakOps CLI is a Python package designed to simplify your CI/CD workflows. With just two straightforward subcommands, you can effortlessly create builds and deploy your applications, ensuring a smooth and efficient development process. To get started, simply install SleakOps using pip:  pip install sleakops   ","version":"Next","tagName":"h2"},{"title":"1. Authenticationâ€‹","type":1,"pageTitle":"CLI","url":"/cli#1-authentication","content":" To authenticate with the SleakOps CLI, you need an API_KEY. You can obtain this key from the console by clicking on Generate API-Key. Each company is allowed to have only one active API_KEY at a time. If you request a new API_KEY, the old one will be automatically revoked. The page shows the company keys and who generated them.  Once you have your API_KEY, you can use it as an argument when running SleakOps commands or set it as an environment variable named SLEAKOPS_KEY.  ","version":"Next","tagName":"h3"},{"title":"2. Create a Buildâ€‹","type":1,"pageTitle":"CLI","url":"/cli#2-create-a-build","content":" To create a build for your application, use the following command:  sleakops build [options]   This command initiates the build process, and SleakOps takes care of compiling your code, running tests, and packaging the application for deployment. You can specify additional options to tailor the build process to your specific needs.  There are two required arguments project and branch, which are used to know what to build. Besides, you might add a commit to build a previous commit, a tag for the image, and the provider if you need to specify it.  As previously mentioned the key might be an input here or a environment variable.  Also, you might mark if you want the process to wait the build to be finished or not.  ","version":"Next","tagName":"h3"},{"title":"3. Make a Deployâ€‹","type":1,"pageTitle":"CLI","url":"/cli#3-make-a-deploy","content":" Once your build is ready, you can effortlessly deploy your application using the following command:  sleakops deploy [options]   SleakOps seamlessly handles the deployment process, ensuring that your application is up and running in no time. You can specify deployment options to fine-tune the process according to your requirements.  Here project and environment are the required arguments. User might add a build or tag image to specify an image. Here the wait and key options are present to, the usage is the same as in the build command.  ","version":"Next","tagName":"h3"},{"title":"CI/CD Examplesâ€‹","type":1,"pageTitle":"CLI","url":"/cli#cicd-examples","content":" With SleakOps CLI, you can integrate your CI/CD pipelines, automate the build and deployment process, and focus on delivering exceptional applications without the hassle of manual intervention. Enjoy a seamless development experience with SleakOps, and make your custom CI/CD workflows.  GitHubGitLabBitBucket name: Deploy on: push: branches: - main jobs: build: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v2 - name: Install SleakOps CLI run: pip install sleakops - name: Run SleakOps build env: SLEAKOPS_KEY: ${{ secrets.SLEAKOPS_KEY }} run: sleakops build -p core -b main -w deploy: needs: [build] runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v2 - name: Install SleakOps CLI run: pip install sleakops - name: Run SleakOps deploy env: SLEAKOPS_KEY: ${{ secrets.SLEAKOPS_KEY }} run: sleakops deploy -p core -e main -w  ","version":"Next","tagName":"h2"},{"title":"Basic Concepts","type":0,"sectionRef":"#","url":"/basicconcepts","content":"","keywords":"","version":"Next"},{"title":"Providerâ€‹","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#provider","content":" As the first step to start an infrastructure is to decide which cloud provider to use (AWS, Azure or GCP, etc.), in SleakOps a Provider represents the selection of one of this cloud providers, the credentials granted to SleakOps to it and the set of accounts created in order to properly manage the infrastructure. It is composed by an Organizative Unit on AWS and its accounts.  ","version":"Next","tagName":"h3"},{"title":"Clusterâ€‹","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#cluster","content":" A Kubernetes cluster is a distributed system for managing containerized applications. It consists of nodes (physical or virtual machines) running pods (groups of one or more containers). A central control plane, composed of various software components, coordinates the activity of the nodes and manages the lifecycle of the pods.  ","version":"Next","tagName":"h3"},{"title":"Environmentâ€‹","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#environment","content":" In computing, an environment or namespace typically refers to an isolated area where specific resources, applications, or Workloads operate independently. This isolation enhances organization, security, and resource management within larger systems or platforms.  ","version":"Next","tagName":"h3"},{"title":"Projectâ€‹","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#project","content":" A project is a collection of files and code managed using Git, representing a codebase within a git repository.  ","version":"Next","tagName":"h3"},{"title":"Workloadâ€‹","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#workload","content":" A workload is a fundamental unit of functionality that can be independently deployed and managed within an environment. Workloads perform specific tasks or processes, interacting with other services via defined interfaces. They are scalable and modular, forming the building blocks of architectures like microservices and service-oriented architectures (SOA), allowing for flexible and efficient system development.  ","version":"Next","tagName":"h3"},{"title":"Dependencyâ€‹","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#dependency","content":" A dependency is an external resource or service that an application requires to function properly in a cloud environment. These dependencies include various infrastructure components such as relational databases, storage services, and caches. Each dependency is associated with provider-specific services, ensuring seamless integration and operation within the cloud platform.  ","version":"Next","tagName":"h3"},{"title":"Var Groupâ€‹","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#var-group","content":" In Sleakops, a variable group (or var group) is a set of key-value pairs, similar to a dictionary, that provides configuration settings for workloads within a specific project and environment.  ","version":"Next","tagName":"h3"},{"title":"Buildâ€‹","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#build","content":" A build is the process of creating a new version of your application's code as a container image from a Dockerfile, incorporating the compiled code and necessary dependencies.  ","version":"Next","tagName":"h3"},{"title":"Deployâ€‹","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#deploy","content":" In technology, to deploy means to release a software application, service, or update to a production environment, making it available for use by end-users. ","version":"Next","tagName":"h3"},{"title":"Cluster","type":0,"sectionRef":"#","url":"/cluster","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Cluster","url":"/cluster#faqs","content":" What are the benefits of having a cluster?â€‹ Clusters allow you to deploy multiple instances of the same application effortlessly, ensuring that if one instance fails, others can instantly take over. Additionally, clusters offer auto-scaling capabilities, automatically adjusting the number of worker nodes as traffic fluctuates, which ensures high availability and optimal performance. They also provide an excellent way to isolate your production environment from your staging environment.  How SleakOps manage clusters?â€‹ SleakOps offers you the flexibility and control needed to create an EKS cluster that aligns precisely with your requirements. To define how to create your infra, you can see: Designing your Infra: Single Schema Vs. Multi Schema   What happens when a Cluster is created on SleakOps?â€‹ SleakOps creates a group of Node Pools that will allow you to run your applications using Kubernetes orchestration based on your needs. See Node Pools.  What is Karpenter?â€‹ In the process of creating your EKS cluster, node provisioning is seamlessly managed by advanced Karpenter technology. Karpenter automatically provisions nodes with just the required resources and scales the cluster based on application demands, removing concerns about deprovisioning. As your workload changes, Karpenter adjusts nodes and resources dynamically, optimizing performance and cost. See Karpenter Web  How does SleakOps handle cluster updates and upgrades?â€‹ We notify users about new version coming in approximatively 1 month in advance.Upgrade clusters for a group of selected customers.Upgrade all non-production flagged clusters.Upgrade all clusters.  How do I control cluster's expenses?â€‹ SleakOps allows you to see all your clusters expenses in one place, classified by account, resources, dates. Access Clusters, select one and click the button: $  How do I monitor my cluster?â€‹ You can monitor your cluster's activity by accessing Clusters, and clicking into the button:  ","version":"Next","tagName":"h2"},{"title":"Lets create your first Cluster on SleakOpsâ€‹","type":1,"pageTitle":"Cluster","url":"/cluster#lets-create-your-first-cluster-on-sleakops","content":" warning Creating a cluster incurs costs on your AWS account, as the EKS service includes a fixed fee plus variable costs based on resource consumption.  ","version":"Next","tagName":"h2"},{"title":"1. Select the account under the clusterâ€™ll be created.â€‹","type":1,"pageTitle":"Cluster","url":"/cluster#1-select-the-account-under-the-clusterll-be-created","content":" For more information regarding accounts, see Accounts.  On the left pane, you will see a selector with the account names, select the one youâ€™ll use based on how you decide to manage your clusters and environments.  We suggest to move forward with a Multiple Schema configuration, that aligns with the best practices. To follow this schema select the development account to create the cluster for your staging environments, and the production account for the production cluster.  On the left pane, you will see a selector with the account names, select the one youâ€™ll use based on how you decide to manage your clusters and environments. We suggest to move forward with a Multiple Schema configuration, that aligns with the best practices. To follow this schema select the development account to create the cluster for your staging environments, and the production account for the production cluster. Check Designing your Infra: Single Schema Vs. Multi Schema for more information.  ","version":"Next","tagName":"h3"},{"title":"2. Navigate to create Cluster sectionâ€‹","type":1,"pageTitle":"Cluster","url":"/cluster#2-navigate-to-create-cluster-section","content":" Into the Left Pane, access Clusters option and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"3. Set up your Clusterâ€‹","type":1,"pageTitle":"Cluster","url":"/cluster#3-set-up-your-cluster","content":" With your Account selected, you will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Setting\tDescriptionName\tIdentify your cluster Description\tOptional space to describe what is included into this cluster. Architecture Type\tSelect the architecture type to be used during the creation for your instances: (64-Bit) ARM or (64-Bit) X86, based on your performance and compatibility needs. Then you'll be able of creating new instances using a different architecture. Production\tSuggested when the Production environment is into this Cluster. It enables the &quot;High-Availability&quot; (HA) feature to ensure constant and reliable availability of critical systems. With HA, the EKS cluster is distributed across multiple Availability Zones (AZs) for redundancy and fault tolerance, ensuring uninterrupted operations even if one AZ has an issue.  Once you've completed the form, click on Submit in order to trigger the cluster creation into the selected account on AWS.  This process will create a Cluster with five node pools by default. See Node Pools. ","version":"Next","tagName":"h3"},{"title":"Access your Cluster","type":0,"sectionRef":"#","url":"/cluster/access-cluster","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#faqs","content":" What is Kubeconfig?â€‹ Kubeconfig is a configuration file used by the Kubernetes command-line tool kubectl to interact with Kubernetes clusters. It contains information about the clusters, users, contexts, and namespaces that kubectl uses to communicate with one or more Kubernetes clusters.  What is an IDE?â€‹ An IDE for Kubernetes is a software environment that provides tools and features specifically designed to help developers create, manage, and deploy applications on Kubernetes clusters, integrating Kubernetes commands, resource management, and YAML/Helm chart editing within the development workflow. Lens is an open-source IDE for Kubernetes, providing a user-friendly GUI to manage, monitor, and troubleshoot multiple clusters in real-time.  ","version":"Next","tagName":"h2"},{"title":"How to access your cluster?â€‹","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#how-to-access-your-cluster","content":" ","version":"Next","tagName":"h2"},{"title":"1. Go to the Access Cluster settingâ€‹","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#1-go-to-the-access-cluster-setting","content":" Click on Clusters, select one and access its settings.  Go to the Access Cluster option.    ","version":"Next","tagName":"h3"},{"title":"2. Install the following Dependenciesâ€‹","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#2-install-the-following-dependencies","content":" AWS Cli: AWS DocsKubeCtl: Kubernetes DocsLens: K8SLens DocsPritunl VPN Client: Pritunl Page  ","version":"Next","tagName":"h3"},{"title":"3. Set up a VPNâ€‹","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#3-set-up-a-vpn","content":" Open the Pritunl VPN ClientGenerate a VPN URi, copy it and set it up on the client.    ","version":"Next","tagName":"h3"},{"title":"4. Generate your AWS Keys and generate the kubeconfig fileâ€‹","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#4-generate-your-aws-keys-and-generate-the-kubeconfig-file","content":" Login into AWS with your username.Then, go to AWS Access Key Wizard to generate the keys on AWS.Paste the keys in the form and generate the kubeconfig file.Copy the output.    ","version":"Next","tagName":"h3"},{"title":"5. Add it to Lensâ€‹","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#5-add-it-to-lens","content":" Open Lens, locate the 'Import Kubeconfig' option, and import the YAML file obtained from the Access Cluster section.   ","version":"Next","tagName":"h3"},{"title":"Addons","type":0,"sectionRef":"#","url":"/cluster/addons","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Addons","url":"/cluster/addons#faqs","content":" Which are the essential add-ons?â€‹ By default SleakOps includes in your infra: Metric Server: SleakOps installs the Metric Server to collect cluster and node-level metrics, enabling performance monitoring and informed scaling decisions.External-DNS: SleakOps deploys External-DNS for automatic DNS record management, ensuring seamless connectivity with user-friendly domain names.Automatic Load Balancer: SleakOps provisions load balancers automatically, efficiently distributing traffic and maintaining high availability.Karpenter Deployment: SleakOps deploys Karpenter for intelligent node provisioning, scaling your cluster based on actual resource needs to optimize performance.  Which optional Add-ons are available?â€‹ Grafana: Visualize and analyze data with Grafana's dashboards, making it easier to monitor system performance and troubleshoot issues. Perfect for tracking application memory and CPU usage.LOKI: Use Loki for cost-effective log aggregation. It simplifies log management by labeling log streams without indexing content, making it ideal for browsing and monitoring application logs.Kubecost: Gain real-time insights into Kubernetes cloud costs with Kubecost. This add-on helps you monitor and reduce expenses across projects in your cluster.Prometheus: SleakOps deploys Prometheus for monitoring and alerting, providing detailed insights into cluster performance and resource utilization.OTEL: Use OpenTelemetry to collect and analyze distributed traces, enabling you to monitor and optimize application performance across your cluster.EFS Controller: The EFS Controller allows you to manage EFS volumes within your EKS cluster, providing scalable and shared storage for your applications. For more details, refer to the EFS documentation.EBS Controller: The EBS Controller allows you to manage EBS volumes within your EKS cluster, providing persistent block storage for your applications. For more details, refer to the EBS documentation.  How do I set up an Add-on?â€‹ To set up an add-on, follow these steps: Navigate to the Add-ons section in the Cluster sectionSelect the desired add-on from the list of available options.Configure the add-on settings as needed.Click &quot;Deploy&quot; to install the add-on in your EKS cluster. For more detailed instructions, refer to the Add-ons setup guide. ","version":"Next","tagName":"h2"},{"title":"Grafana","type":0,"sectionRef":"#","url":"/cluster/addons/grafana","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Grafana","url":"/cluster/addons/grafana#faqs","content":" Accessing Grafanaâ€‹ To access Grafana, first you need to be connected to the VPN, then just click the Grafana button on the Sleakops console. tip When creating the grafana addon, Sleakops gives you the user and credential needed. You will be redirected to the Grafana login page. Once logged in, the Grafana interface will present a dashboard like this:  How it worksâ€‹ Grafana as a monitoring tool, let's you directly connect to datasources within your cluster without extensive configuration. When installing Grafana, Sleakops configures Prometheus datasource, providing quick access to essential metrics through a centralized interface. When installing Loki or Otel, those datasources are also connected.  Provisioned Dashboardsâ€‹ Grafana on Sleakops comes with a series of practical dashboards. They cover general system metrics, offering organized views to monitor various aspects of application performance and resource usage. These dashboards can be used out-of-the-box, offering consistent data visualizations that simplify ongoing system monitoring and management. Sleakops ships with these dashboards, to monitor Resource allocations: Kubernetes / Compute Resources / ClusterKubernetes / Compute Resources / Namespaces (Pods)Kubernetes / Compute Resources / Namespaces (Workloads)Kubernetes / Compute Resources / Nodes (Pods) These ones for networking: Kubernetes / Networking / ClusterKubernetes / Networking / Namespaces (Pods)Kubernetes / Networking / Namespaces (Workloads) And some others like CoreDNS which monitors this internal dns and service discovery system. All of them are useful to monitor the health of your cluster and the applications running on it. Specially to resize workloads, investigate if more replicas are needed or if the resources are well allocated. Dashboards are customizable, allowing you to adjust the layout and data displayed to suit your specific monitoring needs. Also you can create your own dashboards.  Viewing Deployment Resourcesâ€‹ To observe the resources used by a deployment, navigate to the resource monitoring dashboard in Grafana. Once logged in, go to: Home -&gt; Dashboards -&gt; Kubernetes / Compute Resources / Namespace (Pods) This dashboard is set up to display real-time data on CPU, memory, and disk usage, allowing you to track and manage the resources allocated to each deployment within your cluster. ","version":"Next","tagName":"h2"},{"title":"Changelog","type":0,"sectionRef":"#","url":"/changelog","content":"","keywords":"","version":"Next"},{"title":"Version 1.7.11â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-1711","content":" ğŸ—“ï¸ 24/04/2025  ğŸš€ New Features:  Kubernetes 1.31 &amp; Karpenter 1.3: SleakOps now provisions clusters on EKS 1.31 and upgrades the autoscaler to Karpenter 1.3.Stronger Secret Management : All secrets are now also stored encrypted in AWS Systems Manager Parameter Store, adding an extra layer of durability beyond the in-cluster copy.  ğŸ Bug Fixes:  Dev-Cluster Workers: Removed the PodDisruptionBudge improving worker reliability in development clusters when the cluster had the scheduler shutdown enabled.Builds: Builds are no longer triggered for every minor project edit.Deployments: Switched deployments jobs away from Fargate; build logs are now persisted for easier troubleshooting.Web Service Details: Refined the service detail page for clearer visibility of endpoints, status, and metrics.Kubecost Add-on: Stability improvements  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.10â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-1710","content":" ğŸ—“ï¸ 01/04/2025  ğŸš€ New Features:  Enhanced Permission Control: Projects can now have additional associated permissions, whether they are AWS IAM Policies or custom permissions.Dependency Details: The configuration details of each dependency are now displayed within its detail view.Cluster Update Screen Improvements: EKS Insights analysis is now included directly in SleakOps to streamline cluster updates.Build &amp; Project Enhancements: Additional information during builds and improved project validation workflows.  ğŸ Bug Fixes:  Improved Text Input: Resolved issues affecting text inputs in forms.Cluster Access Data: Fixed a bug when retrieving cluster connection data under a different selected account.Domain List Filters: Added filters by account to the domain listing.Nodepool List Improvements: Refined visuals for the nodepool list view.Add-on Installation Updates: The list of add-ons now refreshes properly after installation.Variable Group Editing: Fixed an issue with editing variable groups.Subscription Attachment: Addressed a bug that prevented new subscriptions from attaching correctly.Cost Forecast: Fixed forecasting issues for better cost estimations.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.9â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-179","content":" ğŸ—“ï¸ 17/02/2025  ğŸš€ New Features:  Cronjob Enhancements: Configure cronjob policies and easily filter between active and inactive cronjobs.Support Emails on Notifications: When SleakOps generates a notification, users now receive it via email.EKS Insights: During cluster upgrades, SleakOps checks EKS Insights to ensure everything is running smoothly.  ğŸ Bug Fixes:  Project Flow Improvements: Enhanced various settings, forms, and other elements for smoother project management.AWS Account Creation Flow: Now supports inactive AWS accounts, providing clear guidance on how to manually activate them before resuming the process in SleakOps.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.8â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-178","content":" ğŸ—“ï¸ 10/02/2025  ğŸš€ New Features:  Kubernetes 1.30: Updated EKS support to version 1.30.  ğŸ Bug Fixes:  Minor UI Enhancements: Improved the visual design for project and workload screens.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.7â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-177","content":" ğŸ—“ï¸ 05/02/2025  ğŸš€ New Features:  Import from External Buckets: Quickly copy files from an external S3 Bucket into SleakOps via the new Import Bucket feature.Project View Overhaul: See logs and key info in a single screen for better visibility.Executions Renamed to Workloads: Updated terminology to align with internal cluster notation.Cluster Deletion Optimization: Added extra validation for a more secure and stable deletion process.  ğŸ Bug Fixes:  Project Permissions for Jobs: Fixed an issue where Jobs used cluster node permissions instead of Project permissions.Docker Args Modification: Builds now correctly apply any Docker Args changed just before they run.VPN Profile Generation: Resolved an issue preventing third-party user profiles from being generated successfully.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.6â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-176","content":" ğŸ—“ï¸ 06/01/2025  ğŸš€ New Features:  New Nodepool Configurations: You can now set additional parameters, such as minimum instance sizes and more.Job with Specific Images: When creating a job, you can specify the exact image and tag you want to run (e.g., postgres:16.4).(BETA) Chart Extension by Project: SleakOps can now extend the charts used to deploy project workloads, allowing you to add dependencies. For more information, see the Helm documentation.CI/CD Improvements: The file for configuring CI/CD has been simplified and optimized.  ğŸ Bug Fixes:  Internal Web Service URL: Fixed an issue that caused incorrect URLs for â€œinternalâ€ type web services.Volume Deletion: Resolved problems related to volume deletion under various retention policies.UX/UI Enhancements: Improvements in the interface for Projects, Volumes, and Variable Groups.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.5â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-175","content":" ğŸ—“ï¸ 09/12/2024  ğŸš€ New Features:  AWS Integration Error Handling:: Implemented a mechanism to handle delays in AWS account activations created by SleakOps.Add-on Links in Builds: Added links for easily viewing logs and metrics during the build process.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.4â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-174","content":" ğŸ—“ï¸ 05/12/2024  ğŸš€ New Features:  Add-on Accessibility: Added links in SleakOps for easy access to view logs, APM, or metrics for specific resources.OpenTelemetry (Beta): Introduced an add-on to enhance observability in applications deployed with SleakOps. With OpenTelemetry, you can have your own APM to monitor metrics like request rate, latency, and error rate of your application.Add-on Availability Configurations: Added various availability settings for each add-on.Documentation: Updated the add-on documentation and made it available in Spanish.  ğŸ Bug Fixes:  Kubecost Integration Review: Reviewed the Prometheus-Kubecost integration. Kubecost now correctly maps the names of deployed resources to their costs, greatly improving the accuracy of its estimates. It's now possible to enable approximate network traffic cost analysis within the cluster in Kubecost (Beta).  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.3â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-173","content":" ğŸ—“ï¸ 14/11/2024  ğŸš€ New Features:  Oracle RDS Support (Beta): You can now manage Oracle RDS instances as dependencies within SleakOps.Aurora PostgreSQL Serverless Support (Beta): Added the ability to create and manage Aurora PostgreSQL Serverless databases.  ğŸ Bug Fixes:  Various minor bug fixes.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.2â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-172","content":" ğŸ—“ï¸ 05/11/2024  ğŸš€ New Features:  S3 Bucket Deletion: Introduced the ability to delete S3 buckets containing a large number of files.VPN: Updated the Pritunl module to the latest version for enhanced security and performance.Subscription Management Improvements: Enhanced the management of subscriptions for a better user experience.User Registration: Enabled the registration of new users to the platform.  ğŸ Fixes:  Various minor bug fixes.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.1â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-171","content":" ğŸ—“ï¸ 30/10/2024  ğŸš€ New Features:  Environment and Domain Creation: Improved the process for creating environments and domains. You can now use a different domain than the one configured globally without any limitations.Notifications: Added a notification system to inform users about pending manual actions and scheduled infrastructure updates.Documentation: Updated documentation on managing domains, projects, dependencies, and environment variables.  ğŸ Fixes:  Various minor bug fixes.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.0â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-170","content":" ğŸ—“ï¸ 14/10/2024  ğŸš€ New Features:  Advanced Node Management: Introduced node pool management to provide greater control over the types of nodes where workloads are executed.Cluster Module Migration: All modules created with the cluster now run on Graviton instances, enhancing performance and reducing costs.Cluster Add-ons: All add-ons now run on Graviton instances, further improving performance and lowering costs.Isolated Build Nodes: Builds are now executed on dedicated nodes separate from the application nodes, improving the stability of the nodes running applications.  ğŸ Fixes:  Various minor bug fixes.  ","version":"Next","tagName":"h2"},{"title":"Version 1.6.3â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-163","content":" ğŸ—“ï¸ 27/09/2024  ğŸš€ New Features:  Registration: Implemented a new registration flow.  ğŸ Fixes:  Various minor bug fixes and improvements.  ","version":"Next","tagName":"h2"},{"title":"Version 1.6.2â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-162","content":" ğŸ—“ï¸ 09/19/2024  ğŸš€ New Features:  Upgrades: Updated Prometheus, Loki, and EBS CSI Driver to the latest versions as of August 2024.EBS CSI Driver Migration: SleakOps now uses the AWS-managed EKS Addon for the EBS CSI Driver, replacing the self-managed version.Prometheus and Loki with EBS: Prometheus now utilizes EBS volumes for data persistence, preventing data loss even if the pods crash.Loki with SimpleScalable: It adopts a SimpleScalable structure with TSDB storage for logs, enhancing performance.SQS Dead-letter Queues: Now supports the creation of SQS queues with associated dead-letter queues for improved error handling.  ğŸ Fixes:  Various minor bug fixes and improvements to the platform's workload flows.  ","version":"Next","tagName":"h2"},{"title":"Version 1.6.1â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-161","content":" ğŸ—“ï¸ 08/22/2024  ğŸš€ New Features:  Dependency Version Updates: Updated versions of MQ, Elasticsearch, Memcache, and Redis dependencies.Authentication Improvements: Added support for storing authentication tokens via cookies instead of local storage.Added ACM validation record printing on the ACM detail screen, and ACM status is now included in the system.  ğŸ Fixes:  Issues with the provider flow have been resolved.  ","version":"Next","tagName":"h2"},{"title":"Version 1.6.0â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-160","content":" ğŸ—“ï¸ 08/12/2024  ğŸš€ New Features:  Support for ARM Instances and Additional RDS Versions: Added ARM instances and extra versions in RDS.EKS Updated to Version 1.29: EKS has been updated to version 1.29. Changelogs for EKS updates are now displayed.Improvements in Provider Creation and Editing: Screens and fields for provider forms were updated, including changes in states and visual display.Improved Repository Search: Added support for asynchronous search in the repository selector and enhanced the search function for GitHub, GitLab, and Bitbucket.Healthcheck Parameterization: Healthcheck properties can now be parameterized with JSONSchema.New Dashboard: A new dashboard has been added to view consumption by namespace.  Fixes:  Fixed an error when regenerating certificates, as well as issues with builds not running properly.Frontend errors related to listing and API problems that caused filtering errors have been corrected.  ","version":"Next","tagName":"h2"},{"title":"Version 1.5.1â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-151","content":" ğŸ—“ï¸ 06/24/2024  ğŸš€ New Features:  Advanced Resource Configuration: Advanced options for resource configuration in project environments have been implemented.Optimization of Data Collection Scripts: Improved the efficiency of data collection scripts for faster workload.  ğŸ Fixes:  Several interface errors affecting system usability have been resolved.  ","version":"Next","tagName":"h2"},{"title":"Version 1.5.0â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-150","content":" ğŸ—“ï¸ 05/23/2024  ğŸš€ New Features:  Multiple Project Environments Creation: You can now create multiple project environments using the same repository and branch.Domain Validation for Aliases: Improved domain creation validation for aliases by using an existing usable ACM for ingress.Resource Configuration in Project Env: Added the ability to configure build and deploy resources per project environment.Deploy and Build Request Configuration: Added the option to configure deploy and build requests in a ProjectEnv.Grafana Dashboard: A Grafana dashboard was incorporated to visualize consumption by namespace.Loki Configuration: Logs can now be searched by namespace with the new Loki configuration.Data Collection: Improved the billing collection script to be idempotent and executable for specific dates.  ğŸ Fixes:  Fixed an error when creating S3 dependencies and solved a critical problem with vargroups during cluster shutdown updates.Fixed a critical error when inviting collaborators.  ","version":"Next","tagName":"h2"},{"title":"Version 1.4.3â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-143","content":" ğŸ—“ï¸ 05/13/2024  ğŸš€ New Features:  Dashboard Management Improvements: Dashboard loading was improved, allowing it to be viewed even if no account is selected.Billing and Project Screen Improvements: Improvements to the billing screen were made, including a new &quot;others&quot; section to account for previously unconsidered costs. The project environment screen was also improved.Policy Updates: CloudFormation policy has been updated to enhance management and security.  ğŸ Fixes:  Fixed a critical error that prevented the creation of providers.Reviewed and resolved an issue related to NewRelic integration.Fixed a problem with the refresh token when requesting the VPN URI.ACM Validation Screen and Build Logs Errors: Corrections made to the ACM validation table and logs display for builds in creation state.  ","version":"Next","tagName":"h2"},{"title":"Version 1.4.2â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-142","content":" ğŸ—“ï¸ 04/25/2024  ğŸš€ New Features:  New Metrics: Added new metrics for S3 buckets and RabbitMQ, improving service monitoring. An OpenSearch metrics monitoring system was also implemented.Monitoring Schema Reorganization: Monitoring schema structures were reorganized for better management and visualization. The Dependencies monitoring screen now supports different resource types, providing a more detailed view.  ğŸ Fixes:  A critical issue with vargroups was resolved, ensuring their proper functioning.  ","version":"Next","tagName":"h2"},{"title":"Version 1.4.1â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-141","content":" ğŸ—“ï¸ 04/11/2024  ğŸš€ New Features:  Dependency and OpenSearch Monitoring: A new monitoring page was created for dependencies, facilitating the tracking of their status. OpenSearch was included.ECR Lifecycle Policy: A lifecycle policy was configured for ECR, improving image management.  ğŸ Fixes:  Fixed the issue of duplicate names between cluster and node in Redis.Resolved various frontend errors that affected the user experience.Fixed the problem where an error was displayed when attempting to publish a vargroup without an associated service.Issues with performing multiple deployments and releases in a row were fixed.  ","version":"Next","tagName":"h2"},{"title":"Version 1.4.0â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-140","content":" ğŸ—“ï¸ 03/06/2024  ğŸš€ New Features:  Grafana Configuration: The database for the Grafana addon was configured, along with DataSources and Dashboards.Prometheus Metrics Persistence with Thanos: Added support for persisting Prometheus metrics using Thanos.New Volume API: Implemented support for the new volume API, displaying statuses and applying configuration for deployments.The update option in addons has been disabled.Now, when a dependency is deleted, a deploy with &quot;pending-approval&quot; will be created instead of an automatic one.  ğŸ Fixes:  Fixed an issue where pre-hooks and new volumes were added during deploys, preventing them from being generated.Subdomains are now correctly marked as delegated if the parent domains are already delegated.  ","version":"Next","tagName":"h2"},{"title":"Version 1.3.0â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-130","content":" ğŸ—“ï¸ 01/03/2024  ğŸš€ New Features:  Project Details View: A detailed project view is now available in the new interface.RDS Metrics API: A new API for displaying RDS metrics has been added, improving resource visibility.Improved LogViewer: LogViewer loading is now faster and more efficient.Enhanced Onboarding: A new onboarding process has been implemented for easier setup.Redis Monitoring: Redis monitoring has been added, improving infrastructure supervision.RDS Replica Configuration: The option to configure replicas in the RDS Dependency has been added for more flexibility.Domain Deletion Status: Domain deletion now creates a deploy with pending-approval status, rather than an automatic deploy.Job Workload Improvements: Job workload has been improved, allowing automatic retries in case of an initial failure.  ğŸ Fixes:  Bitbucket integration issues have been resolved.Undefined value issues in Vargroups have been fixed.  ","version":"Next","tagName":"h2"},{"title":"Version 1.2.4â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-124","content":" ğŸ—“ï¸ 15/02/2024  ğŸš€ New Features:  Cluster Switcher Optimization: Cluster selector behavior has been optimized.Login in AWS Subscription Flow: The AWS subscription flow now includes the ability to log in directly.  ğŸ Fixes:  Callback issues for Git integrations and Docker file path for GitLab have been resolved.Minor billing screen-related bugs have been fixed.  ","version":"Next","tagName":"h2"},{"title":"Version 1.2.3â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-123","content":" ğŸ—“ï¸ 05/02/2024  ğŸš€ New Features:  Alias Decoupling in Web Services: The creation of aliases is now separated from the web services form.IAM Password Reset: It is now possible to reset the IAM password for a user.  ğŸ Fixes:  A minor issue with release tasks has been corrected.  ","version":"Next","tagName":"h2"},{"title":"Version 1.2.2â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-122","content":" ğŸ—“ï¸ 25/01/2024  ğŸš€ New Features:  Domain Validation Button: A &quot;check validation&quot; button has been added to the domain drawer for easier domain management.Activity Log Table: An activity log table has been created.Access Key Encryption: Access keys for code version providers (GIT) are now encrypted.  ğŸ Fixes:  An issue where the API didn't correctly recreate the ACM module during regeneration has been fixed.  ","version":"Next","tagName":"h2"},{"title":"Version 1.2.1â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-121","content":" ğŸ—“ï¸ 12/01/2024  ğŸš€ New Features:  Vargroup Form Optimization: Usability improvements have been made to the Vargroup forms.Provider and User Account Deletion: Deleting a provider now also deletes associated user accounts.  ğŸ Fixes:  A bug in ACM certificate regeneration has been fixed.A provider deletion issue has been corrected.  ","version":"Next","tagName":"h2"},{"title":"Version 1.2.0â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-120","content":" ğŸ—“ï¸ 05/01/2024  ğŸš€ New Features:  Logs in Grafana: A data source has been configured in Grafana to display logs from S3.Cluster Update Button: A button has been added to allow cluster updates from the interface.User Activity Log: An activity log for user actions has been created.Domain Validation Deploy: You can now create a deploy that runs once domains are validated.Two-Factor Authentication: Two-factor authentication (2FA) has been added to the login for enhanced security.  ğŸ Fixes:  An issue with builds using the same branch as the default has been fixed.Log reading has been improved for faster processing.Various frontend optimizations, including styles, search, and pending resource visibility, have been made.  ","version":"Next","tagName":"h2"},{"title":"Version 1.1.1â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-111","content":" ğŸ—“ï¸ 05/12/2023  ğŸš€ New Features:  Log Viewer in Jobs: Added a log viewer in the job list, similar to what already exists for deployments.Dashboard v2: Improvements in the second version of the Dashboard, with more options and better organization of information.Cluster Certificates: Cluster certificates are now automatically deleted and updated to prevent expiration issues.  ","version":"Next","tagName":"h2"},{"title":"Version 1.1.0â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-110","content":" ğŸ—“ï¸ 06/11/2023  ğŸš€ New Features:  Vargroups Management: Added the option to show vargroups in the forms for services, workers, hooks, and cronjobs.Kubecost: Integrated Kubecost with Prometheus-stack.  ğŸ Fixes:  Solved the issue with Karpenter on spot instances.Fixed user roles and user editing.Corrected problems when deleting an environment and the incorrect deletion of domains.Fixed the error when trying to manually start the cluster.Resolved an error in generating hooks.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.5â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-105","content":" ğŸ—“ï¸ 27/10/2023  ğŸ Fixes:  Solved deployment issues and fixed Karpenter with spot instances.Fixed issues in deleting entities and validating service URLs.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.4â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-104","content":" ğŸ—“ï¸ 11/10/2023  ğŸš€ New Features:  Refactoring and Improvements: Refactored the dashboard and improved log visualization and the management of entity deletion.  ğŸ Fixes:  Fixed user editing issues.Corrected cluster state management.Solved problems with environment domains.Fixed error handling in S3 responses with CloudFront.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.3â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-103","content":" ğŸ—“ï¸ 25/09/2023  ğŸš€ New Features:  Management Buttons and Form Improvements: Added buttons for resource management and improved variable mapping forms.Cronjobs and Domain Regeneration: You can now stop or activate cronjobs and regenerate domains.  ğŸ Fixes:  Solved the issue of obtaining the VPN URI in Pritunl.Fixed the account selection issue for viewer users.Improved the handling of health check information sent to the backend.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.2â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-102","content":" ğŸ—“ï¸ 04/09/2023  ğŸš€ New Features:  Deployment Optimization: Simplified the deployment process and project environment (ProjectEnv) editing, facilitating configuration and deployment.Resource and Configuration Adjustments: You can now create custom aliases for buckets.Health Check Improvements: The readiness probe for services in the development account is now optional.  ğŸ Fixes:  Solved issues related to VPN and security parameter configuration.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.1â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-101","content":" ğŸ—“ï¸ 29/08/2023  ğŸš€ New Features:  Subscription Management: Login and token updates are controlled based on the subscription status. Additionally, a new API was implemented to register users and companies, validating pending subscriptions, with a new model to better manage subscriptions, integrating AwsClient.Marketplace Onboarding: Simplified process for creating users who come from a marketplace.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.0â€‹","type":1,"pageTitle":"Changelog","url":"/changelog#version-100","content":" ğŸ—“ï¸ 23/08/2023  ğŸš€ New Features:  Volume Configuration: You can now configure volumes in project environments directly from the form.Nightly Shutdown with Timezone: Added support for selecting time zones in the nightly shutdown.Manual Cluster Startup: New button to manually start clusters.CloudFront Integration: Support for using CloudFront to improve content delivery.Automatic Backups: You can configure automatic backups for dependencies.Graviton Instances: Support for using Graviton instances on nodes.Encryption: Implemented encryption in StackSettings for added security.  ğŸ Fixes:  Resolved an issue in the billing API and cost estimation.Fixed errors when deleting Providers and VPNs.You can now delete ACM certificates used by a Load Balancer without problems. ","version":"Next","tagName":"h2"},{"title":"EBS (Elastic Block Store)","type":0,"sectionRef":"#","url":"/cluster/addons/ebs","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"EBS (Elastic Block Store)","url":"/cluster/addons/ebs#faqs","content":" What is AWS EBS?â€‹ AWS EBS is a block storage service from AWS designed to provide persistent storage for EC2 instances. EBS volumes are automatically replicated within their Availability Zone, offering high availability and durability, and protecting against hardware failures.  How is EBS used in SleakOps?â€‹ In SleakOps, EBS is used to deliver persistent storage for applications running on EC2 instances. Each EBS volume can be attached to a single EC2 instance at a time, although multiple volumes can be attached to one instance. Within Kubernetes, EBS is used for persistent volumes, ensuring data consistency across pod restarts and rescheduling. SleakOps manages and configures EBS automatically to suit your application needs.  What are the benefits of using EBS?â€‹ EBS provides several key benefits for high-performance applications: High performance: EBS offers consistent and low-latency performance, ideal for applications requiring quick data access.Durability: EBS volumes are replicated within their Availability Zone to ensure high durability.Scalability: Volumes can be easily resized to accommodate growing application demands.  How do I configure volumes with EBS in SleakOps?â€‹ To set up and manage volumes using EBS within SleakOps, please refer to the Volumes documentation. This guide provides instructions on creating and managing EBS volumes to support your applicationâ€™s storage requirements.  How do I use ebs volumes on my own charts?â€‹ To use EBS Volumes you must pass to the chart values the 'StorageClass' name 'default-sc'. You can check your current StorageClasses kubectl get storageclass --all-namespaces   When do I use EBS?â€‹ You should use EBS when I need a volume that will be mounted to only one pod, for example a database running in the Cluster without replicas. ","version":"Next","tagName":"h2"},{"title":"EFS (Elastic File System)","type":0,"sectionRef":"#","url":"/cluster/addons/efs","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"EFS (Elastic File System)","url":"/cluster/addons/efs#faqs","content":" What is AWS EFS?â€‹ AWS EFS is a cloud file storage service from AWS that provides shared and scalable storage for applications and services. It is ideal for workloads that require concurrent access to a common file system across different services.  How is EFS used in SleakOps?â€‹ In SleakOps, EFS is utilized for Project volumes. Each Project can have one or more volumes, which are implemented as EFS file systems within the EKS cluster, providing shared storage that can be accessed by different services and pods.  What are the benefits of using EFS?â€‹ EFS offers several advantages, making it a powerful option for shared storage in distributed applications: Scalability: Automatically scales as files are added or removed.High availability: Designed to be highly available and durable, with data replicated across multiple availability zones.Concurrent access: Multiple EC2 instances can mount the same EFS file system simultaneously, supporting workloads requiring concurrent access.  What is the EFS Retain Policy in SleakOps?â€‹ SleakOps enforces a retain policy for EFS volumes, which prevents the deletion of an EFS volume in AWS when a volume is removed from SleakOps. This ensures data persistence even if the volume is detached from the cluster.  How do I configure volumes with EFS in SleakOps?â€‹ To set up and manage volumes using EFS within SleakOps, follow the instructions in the Volumes documentation. This guide covers creating and managing volumes for your Projects and configuring EFS settings within your cluster.  How do I use EFS volumes on my own charts?â€‹ To use EFS Volumes you must pass to the chart values the 'StorageClass' name 'efs-sc-delete' or 'efs-sc-retain' depending of which retain policy is needed. You can check your current StorageClasses kubectl get storageclass --all-namespaces   When do I use EFS?â€‹ You should use EFS when you need a volume that will be mounted to more than one pod, for example an application running in the Cluster with two replicas or more. ","version":"Next","tagName":"h2"},{"title":"Kubecost: Cluster Cost Monitoring","type":0,"sectionRef":"#","url":"/cluster/addons/kubecost","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Kubecost: Cluster Cost Monitoring","url":"/cluster/addons/kubecost#faqs","content":" What does the idle metric means?â€‹ The 'idle' shown in all Kubecost metrics is a value that shows how much of the capacity of all your filtered options is not being used. Be careful, this value should be prudently analyzed as many of this &quot;idle&quot; capacity will be part of a Node that is not yet fully allocated or should be available for your workloads.  Should I be worried if my idle value is too high?â€‹ No, you could optimize this value by reducing the CPU and Memory requests of the workloads deployed in your Project, but bear in mind that many of this capacity is allocated as a maximum limit for your resource utilization even if it is not being used, this gives space for the internal scaling of each workload in case it needs it. By the other side, many of these workloads are cluster-critical so they will have an &quot;idle&quot; capacity to let them scale freely.  Could I review a Namespace more deeply?â€‹ Kubecost has a good granularity of how much specific you could be when analyzing costs, for example, besides Namespace costs you can start digging and by clicking the Namespace you can analyze the costs of the pods, deployments and others it has allocated on it.  Can I analyze something else beside namespaces?â€‹ Yes, from the main dashboard you can analyze more specifically the costs of a specific Node as an entity. It also allows you to review the Storage costs that is being used. For example, for a specific node you could see this:  Does Kubecost has any feature to analyze Networking costs?â€‹ At this moment SleakOps is making available the capacity to allow 'NetworkCosts' which is a feature of Kubecost that estimates the cost of the network traffic that each workload has. This feature is a really good choice if you want to analyze cluster network traffic more deeply. It can be allowed in the Kubecost form: ","version":"Next","tagName":"h2"},{"title":"Prometheus: Monitoring System","type":0,"sectionRef":"#","url":"/cluster/addons/prometheus","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Prometheus: Monitoring System","url":"/cluster/addons/prometheus#faqs","content":" Does Prometheus store metrics?â€‹ Prometheus is not directly in charge of pushing metrics to S3, this is done by a related entity called Thanos.  How does SleakOps store Prometheus metrics?â€‹ Prometheus has two related storage units: It depends on the EBS CSI Driver Addon for short-term storage.Uses S3 for long-term storage, this S3 is created in your Account in parallel with Prometheus.  Can I use Prometheus alone?â€‹ It's main purpose is to collect metrics but it also includes a frontend that can be consumed portforwarding its Pod to make some specific queries to its data or to watch some metrics, but it is far easier and comfortable to watch them with Grafana. ","version":"Next","tagName":"h2"},{"title":"Open Telemetry","type":0,"sectionRef":"#","url":"/cluster/addons/otel","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Open Telemetry","url":"/cluster/addons/otel#faqs","content":" How it worksâ€‹ In order to use OpenTelemetry, you need to have a project instrumented with OpenTelemetry. Sleakops will deploy the necessary resources to collect and store the data. Instrumentation is the process of adding code to your application to collect telemetry data. OpenTelemetry provides libraries to instrument your application in a variety of languages. Also Sleakops offers Autoinstrumentation for some languages, learn more about it in the section Autoinstrumentation.  Traces and Metricsâ€‹ Telemetry data consist of three main components: traces, metrics and logs. For logs Sleakops offers Loki. Traces are the path of a request through the system, while metrics are the values of the system at a given time. The OpenTelemetry addon collects traces from the pods running your project and sends them to the OpenTelemetry collector. The collector stores the traces throw Tempo . Traces could be visualized in Grafana. Also the collector generates metrics via the SpanMetrics Connector and stores them in Prometheus. A dashboard is available in Grafana for every project that gets instrumented.  Using the Addonâ€‹ Let's dive in with a view of the OpenTelemetry dashboard. First three metrics are Request rates, Error rates and Durations, or RED metrics. These metrics are the most important to monitor the health of your application. Then we see a table that list Top operations (endpoints) and their error rate as well. Tipically the dashboard gives a quick look to problematic endpoints, application performance bottlenecks, as well as the overall health of the application.  Autoinstrumentationâ€‹ Sleakops offers autoinstrumentation for some languages. This means that Sleakops will automatically instrument your project with OpenTelemetry. This is done by deploying an init container alongside your project. The sidecar container will collect the telemetry data and send it to the OpenTelemetry collector.  Manual instrumentationâ€‹ Manual instrumentation resolves the implementation through code of OpenTelemetry in your project. This is done by adding the OpenTelemetry libraries to your project and adding the necessary code to collect the telemetry data. Sleakops presents the endpoint where the telemetry data should be sent.  What does Sleakops install when installing OpenTelemetryâ€‹ The stack deployed when the addon is installed is the following: OpenTelemetry Operator OpenTelemetry Collector Custom resource (CRD)OpenTelemetry Instrumentation Custom resource (CRD) for every autoinstrumentated projectTempo with a frontend, and caching enabledS3 Bucket as Tempo Backend  ","version":"Next","tagName":"h2"},{"title":"Start using OpenTelemetryâ€‹","type":1,"pageTitle":"Open Telemetry","url":"/cluster/addons/otel#start-using-opentelemetry","content":" To start using OpenTelemetry, you need to install the addon. Then go to the Project list page, activate the project Instrumentation using the small white icon at the left of the name of the project.    These are the options you can choose from:  Option\tDescriptionEnabled\tEnable or disable instrumentation on this proyect. Autoinstrumentation\tOpt for autoinstrumentation. Read more on Autoinstrumentation and Manual Instrumentation Language\tIf autoinstrumentation is enabled, this option marks the language of the project. Currently GO, Java, NodeJS, Python and DotNet are available. Sample Rate\tIf autoinstrumentation is enabled, this option marks the sampling rate, where 0 is none and 1 is all the traces.  Projects that are instrumented are visible in the Project list page. Marked with a green icon, as in the image:   ","version":"Next","tagName":"h2"},{"title":"Node Pools","type":0,"sectionRef":"#","url":"/cluster/nodepools","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Node Pools","url":"/cluster/nodepools#faqs","content":" What are the different kind of node pools?â€‹ On-Demand: are instances in a Kubernetes cluster that run with a fixed pricing model, providing reliable access to compute resources without the risk of interruption. Can be used: Critical Workloads: Applications that require consistent uptime, such as databases, financial systems, or other critical services.Long-Running Tasks: Tasks that cannot be interrupted without significant consequence. Spot: are instances that take advantage of spare capacity in a cloud providerâ€™s data center. They are available at a significant discount compared to On-Demand instances, but they come with the risk of being terminated if the provider needs the capacity back. Ideal for: Stateless Applications: Suitable for workloads that can tolerate interruptions, such as batch processing jobs, testing environments, and distributed computing.Cost-Sensitive Workloads: Ideal for tasks where cost savings are a priority over availability.  How many Node Pools can I have?â€‹ SleakOps base plan, allows you to have three extra node pool besides the build ones. If you need more, contact us.  Can I convert a spot node pool into an on demand and viceversa?â€‹ You can't directly convert a Spot node pool into an On-Demand node pool or vice versa, but you can achieve the desired outcome through a series of steps in SleakOps. Hereâ€™s how you can transition between node pools types: Create a Node Pool of the new desired type.Updade your workloads and projects to run into the new Node Pool.Delete the old node pool if it is not longer needed.  Can I convert a ARM node pool into an X86 and viceversa?â€‹ You can't change the architect type of a node pool, but you can achieve the desired outcome through a series of steps in SleakOps. Hereâ€™s how you can transition between node pools architectures: Create a Node Pool of the new desired architecture.Updade your workloads and projects to run into the new Node Pool.Delete the old node pool if it is not longer needed.  How do I create a Node Pool?â€‹ Follow Creating a Node Pool  How do I manage my a Node Pool?â€‹ Follow Managing a Node Pool ","version":"Next","tagName":"h2"},{"title":"Loki","type":0,"sectionRef":"#","url":"/cluster/addons/loki","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Loki","url":"/cluster/addons/loki#faqs","content":" Which dashboards allow me to read logs?â€‹ At this moment, Sleakops provide two dashboard to consult the logs that were recollected by Loki. Log Explorer: It's a simple dashboard that allows you to filter by Namespace, Pod, Container and Stream where you can choose between 'stdout' and 'stderr'. It also allows you to search expressions through the 'Search Query' field above.Container Log Dashboard: Similar to the previous but is more close to a Logs Dashboard that lets you analyze more complex cases that you might need. It's slower as it required more processing and for general querying it will not be needed.  Which is the best way to use Loki?â€‹ Minimizing the time-range that is being queried is the best way for fast and error-free logs revision as this parameter is the one with the most influence in the weight of the response. We recommend to first check for a big picture of when the problem occured and then check in Loki for logs in a more specific time-range as, generally, logs quantities could be really high. Bear in mind that Loki contains small processing units for reading, writing and as a controller (backend) so big queries might be slow if not having enough read replicas or processing capacity on them. This is modifiable through Sleakops but will also increase costs.  How can I modify the processing capacity of Loki?â€‹ SleakOps allows you to modify the processing capacity of the deployed Loki through the configuration of the Addon. One way of increasing its capacity is by modifying the quantity of replicas deployed.  How does Loki capture and store logs?â€‹ Loki collects logs from each Node of the cluster and therefore, from every container that it's running on it. In order to achieve this, SleakOps uses Promtail that is the default log Collector for Loki, for that reason, every Node of the cluster will have a Promtail instance deployed on it that is in charge of scrapping and pushing them to the Loki write instance that after a certain period of time pushes it to the S3 for long-term storage.  How is the log collection process?â€‹ The log collector, Promtail, collects and streams to Loki all the logs output through 'stdout' or 'stderr' from each running container in the cluster ","version":"Next","tagName":"h2"},{"title":"Managing a Node Pool","type":0,"sectionRef":"#","url":"/cluster/nodepools/managing-nodepool","content":"","keywords":"","version":"Next"},{"title":"1. Access your clusterâ€™s settings to access the Node Pools sectionâ€‹","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#1-access-your-clusters-settings-to-access-the-node-pools-section","content":" From the Cluster Listing, select a node pool and access the Setting option. Then, click on the Node Pools box.  ","version":"Next","tagName":"h3"},{"title":"2. Select the Node Poolâ€‹","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#2-select-the-node-pool","content":" Once you have selected your node pool, youâ€™ll find the access to update and delete them. Each node pool is displayed with CPU and Memory bars that show you how much capacity is left. The full bar represents the total capacity of the node pool, while the colored portion indicates the combined capacity used by all associated projects/workloads.  ","version":"Next","tagName":"h3"},{"title":"Changing node pool settingâ€‹","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#changing-node-pool-setting","content":" ","version":"Next","tagName":"h2"},{"title":"1. Click the setting button at the top right of the node pool cardâ€‹","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#1-click-the-setting-button-at-the-top-right-of-the-node-pool-card","content":" Update the parameters into the modal and click on Save.    ","version":"Next","tagName":"h3"},{"title":"Deleting a node poolâ€‹","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#deleting-a-node-pool","content":" ","version":"Next","tagName":"h2"},{"title":"1. Click the bin button at the top right of the node pool cardâ€‹","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#1-click-the-bin-button-at-the-top-right-of-the-node-pool-card","content":" Click on Delete to confirm and trigger the action on SleakOps. ","version":"Next","tagName":"h3"},{"title":"Creating a Node Pool","type":0,"sectionRef":"#","url":"/cluster/nodepools/creating-nodepool","content":"","keywords":"","version":"Next"},{"title":"1. Access your cluster's settings to access the Node Pools sectionâ€‹","type":1,"pageTitle":"Creating a Node Pool","url":"/cluster/nodepools/creating-nodepool#1-access-your-clusters-settings-to-access-the-node-pools-section","content":" From the Cluster Listing, select one and access the Setting option. Then, click on the Node Pools box.    ","version":"Next","tagName":"h3"},{"title":"2. Click on Createâ€‹","type":1,"pageTitle":"Creating a Node Pool","url":"/cluster/nodepools/creating-nodepool#2-click-on-create","content":" Into the Node Pool section, if you have permission, you'll find the Create option at the top right corner. Click on it.  Notice, that the quantity of Node Pools per Cluster might be limited by your plan.    ","version":"Next","tagName":"h3"},{"title":"3. Set up you node poolâ€‹","type":1,"pageTitle":"Creating a Node Pool","url":"/cluster/nodepools/creating-nodepool#3-set-up-you-node-pool","content":" In the create Node PoolÂ modal enter:  Setting\tDescriptionName\tEnter the name of your choice for your node pool. It cannot be repeated within a cluster. Instance Type\tSelect between Spot and On Demand based on your needs. See What are the different kind of node pools? Architecture Type\tSelect the architecture type to be used during the creation for your instances: (64-Bit) ARM or (64-Bit) X86, based on your performance and compatibility needs. Then youâ€™ll be able of creating new instances using a different architecture. Max Memory\tThis sets the maximum memory the cluster can use as services scale. The autoscaler provisions instances based on demand, but this doesn't mean the cluster always uses the maximum memory; it just defines the upper limit for the autoscaler. Max CPU\tThis sets the maximum CPU the cluster can use as services scale. The autoscaler provisions instances based on demand, but this doesn't mean the cluster always uses the maximum CPU; it just defines the upper limit for the autoscaler. Storage\tSet by default in 20GB, you can modify it based on your need.  Once youâ€™ve completed the form, click on Create in order to trigger the node pool creation into the selected cluster. ","version":"Next","tagName":"h3"},{"title":"Shutdown Cluster","type":0,"sectionRef":"#","url":"/cluster/shutdown-cluster","content":"","keywords":"","version":"Next"},{"title":"How can I activate this feature?â€‹","type":1,"pageTitle":"Shutdown Cluster","url":"/cluster/shutdown-cluster#how-can-i-activate-this-feature","content":" Cluster Shutdown should be manually activated in Cluster Settings through the &quot;Scheduled Shutdown&quot; card.      When you set the Cluster Shutdown, you're creating two cronjobs: one for turning ON the cluster at the scheduled time and another for turning it OFF. The configuration fields are the following:  Attribute\tDescriptionDays\tThe days that the Shutdown crons will run. Auto Downtime (Local Time)\tThe hour at which the OFF action runs. It is shown in your local time. Auto Uptime (Local Time)\tThe hour at which the ON action runs. It is shown in your local time.  Below you will see a box that shows 'Generated Cron Expressions (UTC)', which displays the OFF and ON cron expressions for these two actions in UTC time.  ","version":"Next","tagName":"h3"},{"title":"How can I shutdown my cluster at any time?â€‹","type":1,"pageTitle":"Shutdown Cluster","url":"/cluster/shutdown-cluster#how-can-i-shutdown-my-cluster-at-any-time","content":" In order to shutdown a cluster, just click on the Stop button and confirm the action.  This action can be performed just in Active clusters.  warning Shutdown requires that no dependencies are being updated, and no build or workload processes are active.    ","version":"Next","tagName":"h2"},{"title":"Turning On clusterâ€‹","type":1,"pageTitle":"Shutdown Cluster","url":"/cluster/shutdown-cluster#turning-on-cluster","content":" To turn on a cluster, click on the Play button and confirm the action.  This action is available for clusters in Scheduled Shutdown and Off status    info The cron actions that turn the cluster ON or OFF based on the scheduled time will still run even if you have manually turned it ON or OFF. ","version":"Next","tagName":"h3"},{"title":"Environment","type":0,"sectionRef":"#","url":"/environment","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Environment","url":"/environment#faqs","content":" How can I design my environments?â€‹ Environments can be tailored based on an application's lifecycle or the needs of different teams. For example, creating environments for development (dev), quality assurance (QA), staging (stg), and production (prod) allows each to have custom settings suitable for their specific roles. Before creating an environment, read Designing your Infra: Single Schema Vs. Multi Schema   Can I edit an environment?â€‹ No. You must delete it and create a new one.  How do I delete an environment?â€‹ Access the Environment List, on the Action column, click on the bin icon. Then confirm the action.  How can I delegate a domain?â€‹ Follow: Delegate Domains  warning Your DNS service must be delegated to the Primary Route53 of SleakOps manually. Follow the steps described on thisÂ link .  ","version":"Next","tagName":"h2"},{"title":"Set up your Environmentâ€‹","type":1,"pageTitle":"Environment","url":"/environment#set-up-your-environment","content":" 1. Navigate to the Environment sectionâ€‹  Into the Left Pane, access the Environments option and then, at the top right corner, click on the Create button.    2. Configure your Environmentâ€‹  With your Account selected, you will access the following form:    Setting\tDescriptionName\tDefine a name for your environment using down case letters and middle dashes. Cluster\tSelect one of the available clusters to host the new environment. Domain\tSpecify the domain for your environment.  Once youâ€™ve completed the form, click on Create in order to trigger the environment creation into the selected cluster. ","version":"Next","tagName":"h2"},{"title":"Connect your Git Account","type":0,"sectionRef":"#","url":"/connect_to_git","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Connect your Git Account","url":"/connect_to_git#faqs","content":" Can I connect more than one Git Account?â€‹ Not allowed yet.  How do I Connect my account?â€‹ Access the Setting &gt;&gt; Autorizations section in SleakOps. Select your provider and grant access to SleakOps. See steps below.  Can I change my Git Account?â€‹ Yes, you can do it by deleting de existing one and connecting the new one. Be sure the new account has access to the projects using in SleakOps.  How do I connect a Self hosted Gitlab account?â€‹ Get into the URL:Â yourgitlab.com/-/profile/applicationsÂ and create a new app, call it Sleakops. When asked, use these values: Setting\tDescriptionURL callback\thttps://api.sleakops.com/api/integrations/self-gitlab/callback/ scope\tRead &amp; Write. Once the application is created, add theÂ Application IDÂ andÂ SecretÂ generated inside the configuration of yourÂ Sleakops account.  How do I disconnect an account?â€‹ By clicking in the X button next to the git provider. Consider that current projects will continue to function but won't be able to receive updates once this integration is removed. If you're using GitHub, you will also need to remove the Sleakops app from your GitHub account to complete the deletion process.  ","version":"Next","tagName":"h2"},{"title":"Set up your git Accountâ€‹","type":1,"pageTitle":"Connect your Git Account","url":"/connect_to_git#set-up-your-git-account","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to the git authorization sectionâ€‹","type":1,"pageTitle":"Connect your Git Account","url":"/connect_to_git#1-navigate-to-the-git-authorization-section","content":" Into the Left Pane, access the Settings option and then, click on Authorizations.    ","version":"Next","tagName":"h3"},{"title":"2. Select your Git provider and grant access to SleakOpsâ€‹","type":1,"pageTitle":"Connect your Git Account","url":"/connect_to_git#2-select-your-git-provider-and-grant-access-to-sleakops","content":" Click on your provider an follow the required steps for each one in order to grant access.    ","version":"Next","tagName":"h3"},{"title":"Integrationsâ€‹","type":1,"pageTitle":"Connect your Git Account","url":"/connect_to_git#integrations","content":" Githubâ€‹  Gitlabâ€‹  Bitbucketâ€‹ ","version":"Next","tagName":"h2"},{"title":"Network Resources","type":0,"sectionRef":"#","url":"/network-resources","content":"","keywords":"","version":"Next"},{"title":"1. Overview of the Architectureâ€‹","type":1,"pageTitle":"Network Resources","url":"/network-resources#1-overview-of-the-architecture","content":" The SleakOps network infrastructure is based on the following key components:  VPC (Virtual Private Cloud): Segregates networks by environment (Management, Production, Development).Subnets: Public: exposed to the Internet.Private: restricted access, Internet access via NAT Gateway.Persistence: for databases and storage. Internet Gateway: Enables communication between the VPC and the Internet.Route Tables: Define routing paths between subnets and to/from the Internet.Security Groups: Virtual firewalls that control inbound and outbound traffic for resources.Internal DNS: Allows internal resources to communicate using hostnames instead of IP addresses.External-DNS: Runs inside each Kubernetes (EKS) cluster and automatically manages public DNS records in Route53 for exposed services.  ","version":"Next","tagName":"h2"},{"title":"2. Typical Communication Flowâ€‹","type":1,"pageTitle":"Network Resources","url":"/network-resources#2-typical-communication-flow","content":" The following illustrates a typical flow of network traffic in SleakOps:  Access from the Internet: A user accesses a publicly exposed service (e.g., an API). Traffic reaches the Internet Gateway and is routed to the public subnet. Access Control: The Security Group associated with the resource evaluates whether the connection is allowed. Internal Communication: Internal services (in private or persistence subnets) communicate using internal DNS, under Security Group rules. Service Exposure: If a service within a Kubernetes cluster needs to be publicly accessible (e.g., an API), it is exposed via an Application Load Balancer, and External-DNS registers the public domain automatically in Route53.  This segmentation and control ensure that only necessary services are exposed while keeping sensitive data protected.    ","version":"Next","tagName":"h2"},{"title":"3. External-DNS and Route53â€‹","type":1,"pageTitle":"Network Resources","url":"/network-resources#3-external-dns-and-route53","content":" An automated solution is used to manage public DNS records for deployed services, integrating the infrastructure with external DNS providers like Route53.  External-DNS does not expose services directly. It automates DNS record management for resources that are already exposed (e.g., via an Application Load Balancer).This allows services to be securely and easily accessible from the Internet.  ","version":"Next","tagName":"h2"},{"title":"4. Cross-Environment Connectivity via VPC Peeringâ€‹","type":1,"pageTitle":"Network Resources","url":"/network-resources#4-cross-environment-connectivity-via-vpc-peering","content":" To enable controlled communication between environments (e.g., between Management and Production), SleakOps sets up VPC Peering connections between the different VPCs.  VPC Peering enables two VPCs to exchange internal traffic as if they were part of the same network.It does not require Internet, NAT Gateway, or VPN traffic routing.It is a direct connection between two networks.  ğŸ’¡ Besides Internet Gateway access, SleakOps also supports other connectivity options such as Pritunl VPN, NAT Gateway, and Transit Gateway, depending on use case and required isolation level. ","version":"Next","tagName":"h2"},{"title":"Delegate Domains","type":0,"sectionRef":"#","url":"/environment/delegate_domains","content":"","keywords":"","version":"Next"},{"title":"Set up your domainsâ€‹","type":1,"pageTitle":"Delegate Domains","url":"/environment/delegate_domains#set-up-your-domains","content":" ","version":"Next","tagName":"h2"},{"title":"1. Access to the domain or subdomain informationâ€‹","type":1,"pageTitle":"Delegate Domains","url":"/environment/delegate_domains#1-access-to-the-domain-or-subdomain-information","content":" SleakOps provides a detailed information regarding domains, subdomains and alias. That looks like:    To access it, there are are different ways, based on what you need to do:  If you want to delegate your main domainâ€‹  a. Access the Dashboard and look for the domain widget. b. Click on the desired domain to display the detail.    To delegate an environmentâ€™s subdomainâ€‹  a. Access the Environmentâ€™s list and select an Environment. b. Click on the Cloud icon to display the domain detail.  Create an Alias for a web service workload and delegate itâ€‹  a. Access the Workloadâ€™s list and then, the Web services section. b. Select and Workload and click on the Three Dots button. c. Choose the Detail option d. Create the Alias if it doesnâ€™t exist by clicking the Associate new Domain and completing the form. e. Click on the Alias domain to display the detail.      ","version":"Next","tagName":"h3"},{"title":"2. Update Name Servers with Domain Registrarâ€‹","type":1,"pageTitle":"Delegate Domains","url":"/environment/delegate_domains#2-update-name-servers-with-domain-registrar","content":" Log in to the account where your domain is registered (e.g., GoDaddy, Namecheap, etc.).Locate the DNS settings for your domain.Replace the existing records with the ones provided.  ","version":"Next","tagName":"h3"},{"title":"3. Update Name Servers with Domain Registrarâ€‹","type":1,"pageTitle":"Delegate Domains","url":"/environment/delegate_domains#3-update-name-servers-with-domain-registrar","content":" It may take some time for the DNS changes to propagate globally (usually within a few hours).SleakOps periodically checks it but, if you want to manually verify it, you can click the yellow button Check Delegation to trigger the process.   ","version":"Next","tagName":"h3"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/gettingstarted","content":"","keywords":"","version":"Next"},{"title":"Sign in with your emailâ€‹","type":1,"pageTitle":"Getting Started","url":"/gettingstarted#sign-in-with-your-email","content":" Sign in to ourÂ web app.    info In case you do not have an account with us, you need to subscribe using AWS. Follow How to subscribe to SleakOps using AWS.  ","version":"Next","tagName":"h2"},{"title":"Requirements to Joinâ€‹","type":1,"pageTitle":"Getting Started","url":"/gettingstarted#requirements-to-join","content":" You need to have a root user on AWS. It is the initial account created with full permissions to manage all resources and services, serving as the primary account for AWS Organizations. Go to AWS Organizations.You need access to your code repositories (GitLab, Bitbucket or GitHub).You need your services in Docker files.You need to be able to manage your domains. ","version":"Next","tagName":"h3"},{"title":"Build","type":0,"sectionRef":"#","url":"/project/build","content":"","keywords":"","version":"Next"},{"title":"Build creationâ€‹","type":1,"pageTitle":"Build","url":"/project/build#build-creation","content":" To create a Build you only need four parameters, only the Project field is required as the other three are, if not set, wait until this access is automatically enabled are chosen by default:  Project: Refers to what we call ProjectEnv, here you choose which ProjectEnv you want to build.Branch: Lets you choose any branch of the repository that you've chosen as Project. Defaults to Environment name.Commit hash: You can also choose the commit has to build a specific commit and not the last one as we do by default. Defaults to last commit.Tag: Just a tag to differentiate builds. Defaults to 'latest'.  ","version":"Next","tagName":"h3"},{"title":"Why do we need to Build a Docker image?â€‹","type":1,"pageTitle":"Build","url":"/project/build#why-do-we-need-to-build-a-docker-image","content":" As we use Helm charts we need the image because is what they use to deploy a Kubernetes Release.  info Remember that you need a Build to update the code that the Deployment runs inside the Kubernetes Cluster.  CI/CD integration with SleakOps SleakOps has its own CLI Tool that you can use to automate Builds and Deployments in your CI/CD. More info here. ","version":"Next","tagName":"h2"},{"title":"Configure your Dockerfile","type":0,"sectionRef":"#","url":"/project/configure_your_dockerfile","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Configure your Dockerfile","url":"/project/configure_your_dockerfile#faqs","content":" What do I need to specify when configuring the Dockerfile in SleakOps?â€‹ When configuring your Dockerfile in SleakOps, you need to: Set the Dockerfile Path: Provide the relative path of the Dockerfile within your repository.Provide Docker Arguments: If your Dockerfile requires specific build arguments (e.g., environment variables, configurations), you must provide these values during the Docker image build process.  How do I add the Dockerfile arguments?â€‹ Once you specify the Dockerfile path, SleakOps analyzes it to identify any build arguments that are required. If necessary, SleakOps will prompt you to provide values for these arguments. You can update these arguments at any time through the SleakOps interface.  What are Docker build arguments?â€‹ Docker build arguments are variables that are passed during the Docker build process to customize the build according to different environments or configurations. They are defined in the Dockerfile using the ARG keyword. SleakOps will identify these arguments and ask you to provide the required values. You can also update these arguments later if needed.  How do I update the Dockerfile path and arguments?â€‹ You can add the by following the steps below.  ","version":"Next","tagName":"h2"},{"title":"Set up your Dockerfileâ€‹","type":1,"pageTitle":"Configure your Dockerfile","url":"/project/configure_your_dockerfile#set-up-your-dockerfile","content":" ","version":"Next","tagName":"h2"},{"title":"1. Access to your project settingsâ€‹","type":1,"pageTitle":"Configure your Dockerfile","url":"/project/configure_your_dockerfile#1-access-to-your-project-settings","content":" Complete the Dockerfile Path: To enable SleakOps to search for the needed arguments, specify the Dockerfile Path and save the changes. SleakOps will then analyze your Dockerfile and render the required build arguments for you to provide.  Dockerfile Path\tThe Dockerfile is a critical component used to build your project into a container. The Dockerfile Path field requires the relative file path to your Dockerfile within the repository (e.g., /Dockerfile, /src/Dockerfile, or /app/Dockerfile). This file contains the instructions needed to create the container image, which SleakOps will build and later use for deployments.  Add Arguments Before Saving: If you already know the required arguments, you can enter them before saving. This allows you to provide necessary values upfront rather than waiting for SleakOps to analyze the Dockerfile.  tip If you choose to add the argument using the text option: Each argument should be added on a new line, separated by an equal sign (=), with no extra spaces. ARGUMENT_NAME = VALUE ARGUMENT_TWO = VALUE ARGUMENT_ONE = VALUE ","version":"Next","tagName":"h3"},{"title":"AWS Memcached","type":0,"sectionRef":"#","url":"/project/dependency/memcached-aws","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"AWS Memcached","url":"/project/dependency/memcached-aws#faqs","content":" What are the key use cases for Memcached?â€‹ Memcached is ideal for caching frequently accessed database queries, storing temporary user session data, and caching API responses to reduce database load.  When should I use Memcached?â€‹ Memcached is ideal for: Simple Caching Needs: If you need a basic, high-speed cache for frequently accessed data.Non-Persistent Data: When you donâ€™t need data to be persisted and can tolerate data loss upon node failure or restart.Horizontal Scalability: For applications that benefit from adding multiple caching nodes to distribute load efficiently.Cost-Sensitive Applications: Memcached is more cost-effective than Redis because it lacks advanced features like persistence and replication.  Why should I choose Memcached over Redis?â€‹ Memcached is a simpler and more cost-effective caching solution if you don't need data persistence, replication, or advanced data types. It's suitable for applications that prioritize fast, distributed caching.  How does Memcached scale in SleakOps?â€‹ Memcached scales horizontally by adding more nodes to your cluster, allowing you to distribute the caching load across multiple nodes.  Does Memcached offer data persistence?â€‹ No, Memcached does not support data persistence. All cached data is stored in memory and will be lost if the node is restarted or fails.  What happens to the cached data if a node fails?â€‹ Cached data in Memcached is volatile, meaning it will be lost if a node fails or is restarted. For critical applications, Redis (which supports data persistence) might be a better choice.  ","version":"Next","tagName":"h2"},{"title":"Set up your AWS Memcachedâ€‹","type":1,"pageTitle":"AWS Memcached","url":"/project/dependency/memcached-aws#set-up-your-aws-memcached","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add AWS Memcached as a Dependencyâ€‹","type":1,"pageTitle":"AWS Memcached","url":"/project/dependency/memcached-aws#1-add-aws-memcached-as-a-dependency","content":" To integrate Memcached with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;AWS Redis&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your Memcached database.â€‹","type":1,"pageTitle":"AWS Memcached","url":"/project/dependency/memcached-aws#2-set-up-your-memcached-database","content":" When adding Memcached as a dependency in SleakOps, you need to configure several key attributes:    Attribute\tDescriptionNode Type\tInstance class that determines the performance and memory capacity of the Redis instance. Examples: cache.t3.micro, cache.m5.large, cache.r6g.large Nodes Quantity\tDefines the number of Memcached nodes for horizontal scaling. Adding more nodes increases scalability. Example: 1 or more Port\tThe communication port used by Redis to interact with your application. Default: 11121 (can be customized)  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable names for your Memcached.â€‹","type":1,"pageTitle":"AWS Memcached","url":"/project/dependency/memcached-aws#3-customize-your-variable-names-for-your-memcached","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"Dependencies: Integrating Databases, Caching, and Messaging Services","type":0,"sectionRef":"#","url":"/project/dependency","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#faqs","content":" What types of dependencies are included in SleakOps?â€‹ Hereâ€™s the updated list of dependencies included in SleakOps: Databases Amazon RDS: Managed relational databases such as MySQL, PostgreSQL, and others. Caching Services Amazon ElastiCache for Redis: In-memory data store for caching frequently accessed data.Amazon ElastiCache for Memcached: In-memory caching service for improved performance and reduced database load. Object Storage Amazon S3: Scalable and secure object storage for storing and retrieving any amount of data. Search and Analytics Amazon OpenSearch: A powerful search and analytics engine for exploring and visualizing data, enabling real-time insights and decision-making. Message Queuing Amazon SQS: Fully managed message queuing service that enables you to decouple components and enhance application scalability and reliability.RabbitMQ: A widely used open-source message broker that facilitates reliable messaging and integration between application components. These dependencies integrate seamlessly with SleakOps, providing a comprehensive suite of AWS and open-source services to enhance your application's functionality, performance, and scalability.  Can I modify dependency configurations after initial setup?â€‹ Yes, you can update dependency configurations at any time. Make sure to save any changes in the SleakOps interface to apply the updates.  Can the same dependency be used for multiple Projects?â€‹ At the moment this is not possible, you need one dependency per each project.  How do I delete a Dependency?â€‹ By accessing the Dependency Listing and clicking the delete option.  What happens when I delete a dependency?â€‹ By deleting a dependency, SleakOps will remove all the information related to it and all what is related to it will stop working. To solve that SleakOps create a Deployment in PENDING_APPROVAL status, that must be run manually ASAP to stop the downtime. In case you delete a database, SleakOps will generate a final snapshot before its deletion.  ","version":"Next","tagName":"h2"},{"title":"Project","type":0,"sectionRef":"#","url":"/project","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Project","url":"/project#faqs","content":" What is a Project or Namespace?â€‹ A Project is the combination of a repository and an environment. It acts as a Kubernetes Namespace in the cluster and contains all the services and resources related to the project within that environment.  How do I create a Project in SleakOps?â€‹ The Project and the Project will be created at the same time. You need to connect your Git account (e.g., GitHub, GitLab, or Bitbucket) to SleakOps, select a repository, choose a branch, and specify the location of the Dockerfile. Once these steps are completed, a ProjectEnv is created, and the first image build is triggered. Follow the steps below.  What happens when I create a Project?â€‹ When a Project is created, the following resources are set up: AWS Elastic Container Registry (ECR) to store container images and Helm charts.A Kubernetes Namespace to manage services in isolation.A Service Account to handle permissions and secure connections with AWS resources.A Dockerfile analysis to verify its correctness and build a container image using Kaniko  How do I add the Dockerfile Args?â€‹ If any build-time Docker Args (arguments) are required, SleakOps will prompt you to enter them before running the initial build. These arguments can be modified for future builds.  What is the purpose of the Dockerfile in my project?â€‹ The Dockerfile defines how your application is built into a container image. During Project creation, SleakOps analyzes the Dockerfile to ensure it's correctly configured, and then builds the image using Kaniko.  Where are the Docker images stored?â€‹ Docker images are stored in the AWS ECR (Elastic Container Registry) associated with your project. The images are named after the Project, which combines the environment name and project name.  What is the role of the Service Account?â€‹ The Service Account manages permissions for resources inside the Kubernetes cluster. It allows services deployed in the Project to securely interact with AWS resources like S3, RDS, or any other service your application may require.  Can I update the repository, branch, or Dockerfile path after creating a Project?â€‹ Just the Branch and the Dockerfile Path can be updated. If you need to work with a different Environment, Repository or Name, you will need to create a new record.  How does SleakOps handle the initial image build?â€‹ SleakOps automates the first image build as part of the Project creation process. This initial build ensures faster deployment by utilizing the existing infrastructure. Afterward, future image builds are triggered when services are published in deployments or manually via the Build Form.  How do I control projectâ€™s expenses?â€‹ SleakOps allows you to see all your project expenses in one place, classified by account, resources, dates. Access Projects, select one and click the button:  How do I monitor my project?â€‹ You can monitor your project by accessing Projects, selecting one **and clicking into the button: (button image)  How do I create a Kubernetes Volume?â€‹ When editing a Project, you can enable and define Kubernetes Volumes by specifying the mount path and storage capacity. SleakOps uses the AWS EFS CSI Driver to manage these volumes as EFS file systems in the EKS cluster.  ğŸš©Â How do I manage future builds and deployments?â€‹ Future builds and deployments can be managed manually via the SleakOps interface or automated using the SleakOps CLI.  ","version":"Next","tagName":"h2"},{"title":"Lets create your first Project on SleakOpsâ€‹","type":1,"pageTitle":"Project","url":"/project#lets-create-your-first-project-on-sleakops","content":" warning You must have your Git Repository connected. See Connect your Git Account  ","version":"Next","tagName":"h2"},{"title":"1. Navigate to Create Project sectionâ€‹","type":1,"pageTitle":"Project","url":"/project#1-navigate-to-create-project-section","content":" Into the Left Pane, access Projects option and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Set up your Projectâ€‹","type":1,"pageTitle":"Project","url":"/project#2-set-up-your-project","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Setting\tDescriptionName\tIdentify your Project. Environment\tThe environment represents a specific stage or setup within your infrastructure where your project will be deployed (e.g., Development, Staging, Production). You are associating your projectâ€™s code with a particular environment in your Kubernetes cluster. Repository\tThe repository is the Git repository that holds the codebase for your project. SleakOps will access this repository to manage code updates, builds, and deployments. Ensure that you have connected your Git provider (e.g., GitHub, GitLab, Bitbucket) and that the selected repository contains all necessary files for your project. Nodepool\tIs the resource in charge of provision the server where your services will run. More info here. Branch\tThe branch represents a specific version or line of development within the repository. This allows you to deploy a particular version of your code (e.g., main, develop, or a feature-specific branch). The branch you select will determine the code that gets built and deployed within the associated environment. Dockerfile Path\tThe Dockerfile is a critical component used to build your project into a container. The Dockerfile Path field requires the relative file path to your Dockerfile within the repository (e.g., /Dockerfile, /src/Dockerfile, or /app/Dockerfile). This file contains the instructions needed to create the container image, which SleakOps will build and later use for deployments.  Once youâ€™ve completed the form, click on Submit in order to trigger the Dockerfile validation and then the build. ","version":"Next","tagName":"h3"},{"title":"Lets add a Dependency for your Projectâ€‹","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#lets-add-a-dependency-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Dependency sectionâ€‹","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#1-navigate-to-create-dependency-section","content":" Into the Left Pane, access Dependencies option and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select the kind of dependency you want to createâ€‹","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#2-select-the-kind-of-dependency-you-want-to-create","content":"   ","version":"Next","tagName":"h3"},{"title":"3. Select the kind of dependency you want to createâ€‹","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#3-select-the-kind-of-dependency-you-want-to-create","content":" In Sleakops all dependecies start with the same step. So, complete these attributes and click Next to continue.  Setting\tDescriptionName\tIdentify your Project. Project\tSelect between the existent projects.    ","version":"Next","tagName":"h3"},{"title":"4. Follow each dependency guideâ€‹","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#4-follow-each-dependency-guide","content":" To move forward choose between the following guides.  S3 Bucket.MySQL.PostgreSQL.Redis.Memcached.OpenSearch.SQS. ","version":"Next","tagName":"h3"},{"title":"AWS OpenSearch","type":0,"sectionRef":"#","url":"/project/dependency/opensearch-aws","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"AWS OpenSearch","url":"/project/dependency/opensearch-aws#faqs","content":" What are the use cases for OpenSearch?â€‹ OpenSearch is commonly used for full-text search, real-time analytics, monitoring and observability, and log analysis. It is also ideal for powering search functionalities on websites and applications.  What does &quot;Dedicated Master Enabled&quot; mean?â€‹ When enabled, SleakOps sets up dedicated master nodes that help with managing the OpenSearch domain. They provide enhanced stability by separating management tasks from data nodes. This is highly recommended for production workloads.  What is the recommended configuration for master nodes in production?â€‹ For production environments, SleakOps recommends using 3 dedicated master nodes to improve the stability and performance of your OpenSearch cluster.  ","version":"Next","tagName":"h2"},{"title":"Set up your OpenSearchâ€‹","type":1,"pageTitle":"AWS OpenSearch","url":"/project/dependency/opensearch-aws#set-up-your-opensearch","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add AWS OpenSearch as a Dependencyâ€‹","type":1,"pageTitle":"AWS OpenSearch","url":"/project/dependency/opensearch-aws#1-add-aws-opensearch-as-a-dependency","content":" To integrate OpenSearch with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;SQS&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your OpenSearch.â€‹","type":1,"pageTitle":"AWS OpenSearch","url":"/project/dependency/opensearch-aws#2-set-up-your-opensearch","content":" When adding OpenSearch as a dependency in SleakOps, you need to configure several key attributes:    Attribute\tDescriptionFIFO Queue\tSpecifies the type of SQS queue. Where Standard Queue (for most use cases) or FIFO Queue (if message ordering is required)` FIFO Deduplication\tOnly for FIFO Queues, in order to avoid duplicates. Message Retention Period\tSpecifies the amount of time a message will be retained in the queue if it hasn't been consumed. Maximum Message Size\tThe maximum size of a message that can be sent to the SQS queue. Delivery Delay in Seconds\tThe delay between the message being sent to SQS and it being visible in the queue. No delay by default. Receive Message Wait Time\tDetermines how long a ReceiveMessage call waits if no messages are available in the queue. Visibility Timeout\tThe duration that a message remains invisible after a receiving component reads it from the queue. Dead-Letter Queue (DLQ)\tAdd a queue where messages that fail to be processed multiple times are sent for additional analysis.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable names for your SQS.â€‹","type":1,"pageTitle":"AWS OpenSearch","url":"/project/dependency/opensearch-aws#3-customize-your-variable-names-for-your-sqs","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS MySQL","type":0,"sectionRef":"#","url":"/project/dependency/mysql-aws","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"AWS MySQL","url":"/project/dependency/mysql-aws#faqs","content":" How does SleakOps manage MySQL credentials?â€‹ When you create a MySQL dependency in SleakOps, it automatically generates a Vargroup for your database. This Variable Group securely stores the MySQL credentials and other important configuration details, such as the database endpoint and user access information. Youâ€™ll be able of manage them from Vargroups section.  What is Multi-AZ deployment and should I enable it?â€‹ Multi-AZ (Availability Zone) deployment ensures high availability and failover support by replicating your database in another availability zone. Itâ€™s recommended for production environments to prevent downtime. Keep in mind that it increases costs.  Can I change the MySQL version after the database is deployed?â€‹ No, the database engine version cannot be changed after deployment. You would need to create a new MySQL instance with the desired version and migrate your data. Or change it manually into the AWS Console.  What happens if I need more storage for my MySQL database?â€‹ You can adjust the storage size when configuring your database. If you need more storage after deployment, you can scale modifying the settings in AWS as at the moment SleakOps does not support it.  How do I create a MySQL database dump?â€‹ To create a dump of your MySQL database, use the following command: sh mysqldump -h MYSQL_ADDRESS -u MYSQL_USERNAME -p MYSQL_PASSWORD &gt; dump.sql Replace MYSQL_ADDRESS, MYSQL_USERNAME, and MYSQL_PASSWORD with the appropriate values. For additional information on creating a MySQL dump, refer to the official MySQL documentation. Another option is creating it directly from the AWS Console and then import it. See Restoring to a DB instance.  How do I import an existent dump using docker?â€‹ For more details: MySQL Dump Documentation To import a database dump into your MySQL RDS instance: Connect to the VPN: Ensure you are connected to the VPN of the AWS account where the RDS instance is located.Run Docker Container (Recommended): Install Docker on your local machine if not already installed.Run a MySQL Docker container with the following command: sh docker run -it --name mysql-container -v ./initial_data/:/tmp/data/ -e MYSQL_ROOT_PASSWORD=MYSQL_PASSWORD -d mysql bash Attach to the containerâ€™s terminal: sh docker exec -t -i mysql-container bash Import the dump file: sh mysql -h MYSQL_ADDRESS -u MYSQL_USERNAME -p &lt; /tmp/data/dump.sql Replace MYSQL_ADDRESS, MYSQL_USERNAME, and MYSQL_PASSWORD with your RDS instance details.  How do I import an existent dump to my local machine ?â€‹ Alternatively, you can use a MySQL client installed on your local machine to import the dump. mysql -h MYSQL_ADDRESS -u MYSQL_USERNAME -p MYSQL_PASSWORD &lt; /tmp/data/dump.sql   What should I do if I encounter connection issues with my MySQL database?â€‹ Ensure the database endpoint, username, and password are correct.Verify that your security groups and firewall rules allow access.Ensure the database is running and has enough resources (CPU, memory). Otherwise, contact us.  What is an RDS Read Replica?â€‹ An RDS Read Replica is a read-only copy of your primary database instance in Amazon RDS. It helps distribute read-heavy workloads and improves the performance and scalability of your database by offloading read operations from the primary database. RDS Read Replicas are ideal when you need to: Offload read-heavy operations from your primary instance.Scale your read operations as your application grows.Distribute database reads across multiple geographic locations.Have a backup solution that can quickly be promoted to a primary instance in case of failure. info Keep in mind that Read replicas have a delay performing updates.  How do I configure a Read Replica in SleakOps?â€‹ In SleakOps, when creating a read replica for your RDS database, you will need to provide the following information: Name of the replicaReplica Instance Class, which determines the instance type for the replica.Replica Publicly Accessible, to decide if the replica should have a public IP or be accessible only within your private network.  Can I delete a replica?â€‹ At the moment, the only way is to delete the dependency.  info AWS documentation: Amazon RDS MySQL Documentation  ","version":"Next","tagName":"h2"},{"title":"Set up your MySQLâ€‹","type":1,"pageTitle":"AWS MySQL","url":"/project/dependency/mysql-aws#set-up-your-mysql","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add MySQL as a Dependencyâ€‹","type":1,"pageTitle":"AWS MySQL","url":"/project/dependency/mysql-aws#1-add-mysql-as-a-dependency","content":" To integrate MySQL with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;MySQL&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your MySQL.â€‹","type":1,"pageTitle":"AWS MySQL","url":"/project/dependency/mysql-aws#2-set-up-your-mysql","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionDatabase Engine Version\tSelect the specific version of the MySQL database engine you wish to use. This ensures compatibility with your application requirements. Example: MySQL 8.0.2, MySQL 5.7.1 Database Instance Class\tDefine the instance class that specifies the hardware configuration for your MySQL database. This controls CPU, memory, and network performance. Example: db.m6g.large, db.t3.medium. See AWS detail. Database Storage\tSpecify the amount of storage allocated for the database. Example: 100 GB, 500 GB. Username\tProvide the master username for the MySQL database. This is the main user with administrative privileges. Example: admin, root. Password\tPassword for the master user to access the database. Multi-Availability Zone\tEnable or disable Multi-AZ deployment. This ensures high availability and failover support by replicating the database across multiple availability zones. Recommended for production environments. Automated Backup\tConfigure automated backups for the MySQL database. This ensures data protection by enabling daily snapshots and transaction log backups. Set up the Backup Retention Period and the Backup Window. Recommended for production environments. Backup Retention Period\tSet the number of days to retain automated backups. Backup Window\tPeriod of time while the backup will be done.  warning SleakOps allow the creation of replicas only during the creation of the dependency.  After that basic data, you need to decide if a replica will be created. To do that:  Into the form, look for the section Definition of RDS Read Replicas and click on + Add Item.Complete the following data:  Setting\tDescriptionName\tA name for the replica Replica Instance Class\tDefine the instance class that specifies the hardware configuration for your MySQL database. This controls CPU, memory, and network performance. Example: db.m6g.large, db.t3.medium. Replica Publicly Accessible\tDecide if the replica should have a public IP or be accessible only within your private network.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable's name for your MySQL data base.â€‹","type":1,"pageTitle":"AWS MySQL","url":"/project/dependency/mysql-aws#3-customize-your-variables-name-for-your-mysql-data-base","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS Redis","type":0,"sectionRef":"#","url":"/project/dependency/redis-aws","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"AWS Redis","url":"/project/dependency/redis-aws#faqs","content":" What features make Redis a good choice for my application?â€‹ Redis offers several advanced features that make it suitable for a wide range of applications: Data Persistence: Redis can save data to disk, ensuring that information is not lost in the event of a restart.Advanced Data Structures: Redis supports more complex data structures than simple key-value stores, such as lists, sets, hashes, sorted sets, and more.High Availability: Through replication and automatic failover, Redis ensures that your application remains operational even if a node fails.Scalability: Redis can be scaled both vertically (with larger instances) and horizontally (using sharding and clusters).Pub/Sub Messaging: Redis offers native support for publish/subscribe messaging patterns, useful for building real-time applications.  What are the common use cases for Redis?â€‹ Redis is versatile and can be used in a variety of scenarios, including: Session Management: Redis is commonly used for storing user session data due to its low-latency data access and persistence features.Caching: Redis is ideal for caching frequently accessed data, reducing load on primary databases and improving response times.Real-Time Analytics: Redis's fast in-memory processing capabilities make it perfect for real-time analytics, leaderboards, and counters.Message Queues: With Redisâ€™s pub/sub functionality, you can use it for messaging systems and event streaming.Job Queues: Redis is used for managing background job queues in large-scale applications.  How does Redis differ from Memcached?â€‹ Redis is more feature-rich than Memcached. Redis supports a variety of data structures like lists, sets, and hashes, while Memcached is limited to simple key-value pairs. Redis also supports data persistence and replication, making it suitable for applications where durability and high availability are critical. However, Memcached is typically more lightweight and faster for basic caching scenarios.  ","version":"Next","tagName":"h2"},{"title":"Set up your AWS Redisâ€‹","type":1,"pageTitle":"AWS Redis","url":"/project/dependency/redis-aws#set-up-your-aws-redis","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add AWS Redis as a Dependencyâ€‹","type":1,"pageTitle":"AWS Redis","url":"/project/dependency/redis-aws#1-add-aws-redis-as-a-dependency","content":" To integrate Redis with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;AWS Redis&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your Redis database.â€‹","type":1,"pageTitle":"AWS Redis","url":"/project/dependency/redis-aws#2-set-up-your-redis-database","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionNode Type\tInstance class that determines the performance and memory capacity of the Redis instance. Examples: cache.t3.micro, cache.m5.large, cache.r6g.large Port\tThe communication port used by Redis to interact with your application. Default: 6379 (can be customized)  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable names for your Redis.â€‹","type":1,"pageTitle":"AWS Redis","url":"/project/dependency/redis-aws#3-customize-your-variable-names-for-your-redis","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS SQS","type":0,"sectionRef":"#","url":"/project/dependency/sqs-aws","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"AWS SQS","url":"/project/dependency/sqs-aws#faqs","content":" What is the difference between a Standard Queue and a FIFO Queue?â€‹ A Standard Queue supports high throughput with at-least-once message delivery. Message ordering is not guaranteed but it's ideal for scenarios where message order isn't critical.A FIFO Queue ensures message ordering and exactly-once delivery. It's suitable for applications where message order is crucial.  What is Multi-AZ deployment and should I enable it?â€‹ Multi-AZ (Availability Zone) deployment ensures high availability and failover support by replicating your database in another availability zone. Itâ€™s recommended for production environments to prevent downtime. Keep in mind that it increases costs.  What is a Dead-Letter Queue (DLQ), and when should I use it?â€‹ A DLQ is a secondary queue where messages that can't be processed successfully after a certain number of attempts are sent. You should configure a DLQ to help with error handling and to prevent message loss. See AWS SQS DLQ .  What is Deduplication in a FIFO Queue?â€‹ In an SQS FIFO Queue, deduplication ensures that duplicate messages are automatically removed, preserving strict message ordering.  ","version":"Next","tagName":"h2"},{"title":"Set up your SQSâ€‹","type":1,"pageTitle":"AWS SQS","url":"/project/dependency/sqs-aws#set-up-your-sqs","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add SQS as a Dependencyâ€‹","type":1,"pageTitle":"AWS SQS","url":"/project/dependency/sqs-aws#1-add-sqs-as-a-dependency","content":" To integrate SQS with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;SQS&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your SQS database.â€‹","type":1,"pageTitle":"AWS SQS","url":"/project/dependency/sqs-aws#2-set-up-your-sqs-database","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionFifo Queue\tSelect if the queue is standard or fifo. Read more on FAQs Fifo Deduplication\tSelect if deduplication is activated. Read more on FAQs Message Delay Seconds\tSeconds of delivery delay to messages on queue. Message Max Size\tBytes maximum limit per message Message Retention Seconds\tSeconds that a message is retained by SQS Receive WaitTime Seconds\tThe time for which a ReceiveMessage call will wait for a message to arrive (long polling) before returning Visibility Timeout Seconds\tSeconds that a message is retained by SQS Dead Letter Queue\tSelect if dead letter queue is enabled. Read more on FAQs  By activating the Dead Letter Queue feature, you need to complete the same information for the second queue.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable names for your SQS.â€‹","type":1,"pageTitle":"AWS SQS","url":"/project/dependency/sqs-aws#3-customize-your-variable-names-for-your-sqs","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS S3 Bucket","type":0,"sectionRef":"#","url":"/project/dependency/s3bucket-aws","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"AWS S3 Bucket","url":"/project/dependency/s3bucket-aws#faqs","content":" When should I use Amazon S3?â€‹ You should use Amazon S3 when you need: Scalable storage for files, media, backups, or static assets.Highly durable and available storage for critical data.Data archiving solutions or disaster recovery.A cost-effective solution for storing large amounts of unstructured data.  How do I create a S3 dump?â€‹ To create a backup of your S3 bucket data, follow these steps: Ensure Admin Access: Verify that you have admin access to the AWS account where the S3 bucket is located.Install AWS CLI: Ensure AWS CLI is installed on your local machine. For installation instructions, refer to the official AWS CLI documentation .Configure AWS CLI: Ensure that your Access and Secret keys are configured in the default profile. If they are under a different profile, use the --profile PROFILE_NAME option with the commands.Run Backup Commands: ```sh aws sts assume-role --role-arn arn:aws:iam::ACCOUNT_ID:role/SleakopsAdminRole aws s3 sync s3://BUCKET_NAME /path/to/local/directory``` Replace ACCOUNT_ID, BUCKET_NAME, and /path/to/local/directory with your specific details.  Does SleakOps create Variable Groups for S3 dependencies?â€‹ Yes, when you configure an S3 bucket in SleakOps, it will automatically create a Variable Group. This securely stores the access keys and other sensitive information needed to manage and interact with your S3 bucket.  What is an S3 Access Control List (ACL)?â€‹ An S3 Access Control List (ACL) defines the permissions for who can access your S3 bucket and its contents. It controls the level of access granted to users, groups, or predefined AWS entities. You can choose an ACL that defines who can access the bucket and what level of permission they have. The available options are: private: Only the bucket owner has full access. (Default)public-read: Anyone can read the objects in the bucket.public-read-write: Anyone can read and write to the bucket.aws-exec-read: Grants read access to AWS services like CloudFront.authenticated-read: Grants read access to authenticated AWS users.log-delivery-write: Grants write access to the bucket for logging purposes. For further details, you can refer to AWS S3 ACL documentation .  What is Amazon CloudFront?â€‹ Amazon CloudFront is a Content Delivery Network (CDN) that speeds up the delivery of your static and dynamic content (like HTML, CSS, images, and videos) by caching it at edge locations closer to your users around the world.  Can I use a custom domain name with CloudFront?â€‹ Yes, SleakOps allows you to set a custom alias (subdomain) for your CloudFront distribution. For example, you could use cdn.mydomain.com as the URL for your CloudFront distribution instead of the default CloudFront URL.  What is a Custom Header in CloudFront?â€‹ Custom Headers allow you to include additional information in every request that CloudFront makes to your S3 bucket. This feature provides more control over how your S3 content is accessed and delivered by attaching specific metadata to the requests. SleakOps allows you to define custom headers for CloudFront, which will be included in all requests sent to your S3 bucket. This is useful for adding security measures, managing permissions, or tracking requests. custom_headers: - key: &quot;X-Custom-Header&quot; value: &quot;MyCustomValue&quot;   ","version":"Next","tagName":"h2"},{"title":"Set up your S3 Bucketâ€‹","type":1,"pageTitle":"AWS S3 Bucket","url":"/project/dependency/s3bucket-aws#set-up-your-s3-bucket","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add S3 Bucket as a Dependencyâ€‹","type":1,"pageTitle":"AWS S3 Bucket","url":"/project/dependency/s3bucket-aws#1-add-s3-bucket-as-a-dependency","content":" To integrate S3 Bucket with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;S3 Bucket&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your S3 Bucket.â€‹","type":1,"pageTitle":"AWS S3 Bucket","url":"/project/dependency/s3bucket-aws#2-set-up-your-s3-bucket","content":" When adding an S3 bucket as a dependency in SleakOps, you will be required to configure the following attributes:  Attribute\tDescriptionS3 Access Control List (ACL)\tSpecifies the level of access for the S3 bucket and its contents. Options: private, public-read, public-read-write, aws-exec-read, authenticated-read, log-delivery-write Enable CloudFront\tAllows you to enable Amazon CloudFront, a CDN for delivering S3 bucket content globally. Alias\tThe alias for the S3 bucket or CloudFront distribution (e.g., cdn.mydomain.com). Price Class\tDefines the set of global CloudFront edge locations used to serve content. Options: Use all edge locations (best performance), Use North America, Europe, Asia, Middle East, and Africa, Use only North America and Europe. Check AWS Price Class . Custom Headers\tCustom headers that CloudFront includes in all requests sent to the S3 bucket. - Key: The header key, such as X-Custom-Header. - Value: The value you wish to assign to the header, like MyCustomValue. Override\tSpecify whether CloudFront should override existing headers in requests from the origin.    After that basic data, if you activated the Cloudfront, you will be able of customize its headers. To do that:  Into the form, look for the field CloudFront custom headers and click on + Add Item.Complete Key and ValueYou can add as many as you need.    ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable names for your S3.â€‹","type":1,"pageTitle":"AWS S3 Bucket","url":"/project/dependency/s3bucket-aws#3-customize-your-variable-names-for-your-s3","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes.    In this step you can change the name of the attributes in case it is needed but SleakOps completes the values automatically. After this step, your dependency is created. ","version":"Next","tagName":"h3"},{"title":"AWS PosgreSQL","type":0,"sectionRef":"#","url":"/project/dependency/postgresql-aws","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"AWS PosgreSQL","url":"/project/dependency/postgresql-aws#faqs","content":" How does SleakOps manage PostgreSQL credentials?â€‹ When you create a PostgreSQL dependency in SleakOps, it automatically generates a Vargroup for your database. This Variable Group securely stores the PostgreSQL credentials and other important configuration details, such as the database endpoint and user access information. Youâ€™ll be able of manage them from Vargroups section.  What is Multi-AZ deployment and should I enable it?â€‹ Multi-AZ (Availability Zone) deployment ensures high availability and failover support by replicating your database in another availability zone. Itâ€™s recommended for production environments to prevent downtime. Keep in mind that it increases costs.  Can I change the PostgreSQL version after the database is deployed?â€‹ No, the database engine version cannot be changed after deployment. You would need to create a new PostgreSQL instance with the desired version and migrate your data. Or change it manually into the AWS Console.  What happens if I need more storage for my PostgreSQL database?â€‹ You can adjust the storage size when configuring your database. If you need more storage after deployment, SleakOps allows you to scale the storage size without downtime.  How do I create a PostgreSQL database dump?â€‹ To create a dump of your PostgreSQL database: Run the pg_dump Command: sh pg_dump -h POSTGRESQL_ADDRESS -U POSTGRESQL_USERNAME -W &gt; dump.sql Replace POSTGRESQL_ADDRESS, POSTGRESQL_USERNAME, and dump.sql with the appropriate values. 2. Consult Documentation: For more information on how to create a dump, refer to the official PostgreSQL documentation..  How do I import an existent dump using docker?â€‹ To import a database dump into your PostgreSQL RDS instance: Connect to the VPN: Ensure you are connected to the VPN of the AWS account where the RDS instance is located.Run Docker Container (Recommended): Install Docker on your local machine if not already installed.Run a PostgreSQL Docker container with the following command: sh docker run -it --name postgresql-container -v ./initial_data/:/tmp/data/ -e POSTGRESQL_ROOT_PASSWORD=POSTGRESQL_PASSWORD -d postgres bash Attach to the containerâ€™s terminal: sh docker exec -t -i postgresql-container bash Import the dump file: pg_dump -h POSTGRESQL_ADDRESS -U POSTGRESQL_USERNAME -W &lt; /tmp/data/dump.sql Replace POSTGRESQL_ADDRESS, POSTGRESQL_USERNAME, and dump.sql with your specific details.  How do I import an existent dump to my local machine ?â€‹ Alternatively, you can use a PostgreSQL client installed on your local machine to import the dump. sh psql -h POSTGRESQL_ADDRESS -U POSTGRESQL_USERNAME -W -f /tmp/data/dump.sql   What should I do if I encounter connection issues with my PostgreSQL database?â€‹ Check the following: Ensure the database endpoint, username, and password are correct.Verify that your firewall rules allow access.Ensure the database is running and has enough resources (CPU, memory). Otherwise, contact us.  What is an RDS Read Replica?â€‹ An RDS Read Replica is a read-only copy of your primary database instance in Amazon RDS. It helps distribute read-heavy workloads and improves the performance and scalability of your database by offloading read operations from the primary database. RDS Read Replicas are ideal when you need to: Offload read-heavy operations from your primary instance.Scale your read operations as your application grows.Distribute database reads across multiple geographic locations.Have a backup solution that can quickly be promoted to a primary instance in case of failure. info Keep in mind that Read replicas have a delay performing updates.  How do I configure a Read Replica in SleakOps?â€‹ In SleakOps, when creating a read replica for your RDS database, you will need to provide the following information: Name of the replicaReplica Instance Class, which determines the instance type for the replica.Replica Publicly Accessible, to decide if the replica should have a public IP or be accessible only within your private network.  Can I delete a replica?â€‹ At the moment, the only way is to delete the dependency.  info AWS documentation: Amazon RDS PostgreSQL Documentation  ","version":"Next","tagName":"h2"},{"title":"Set up your PostgreSQLâ€‹","type":1,"pageTitle":"AWS PosgreSQL","url":"/project/dependency/postgresql-aws#set-up-your-postgresql","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add PostgreSQL as a Dependencyâ€‹","type":1,"pageTitle":"AWS PosgreSQL","url":"/project/dependency/postgresql-aws#1-add-postgresql-as-a-dependency","content":" To integrate PostgreSQL with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;PostgreSQL&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your PostgreSQL database.â€‹","type":1,"pageTitle":"AWS PosgreSQL","url":"/project/dependency/postgresql-aws#2-set-up-your-postgresql-database","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionDatabase Engine Version\tSelect the specific version of the MySQPostgreSQLL database engine you wish to use. This ensures compatibility with your application requirements. Example: PostgreSQL 14.9, PostgreSQL 16.5 Database Instance Class\tDefine the instance class that specifies the hardware configuration for your PostgreSQL database. This controls CPU, memory, and network performance. Example: db.m6g.large, db.t3.medium. See AWS detail. Database Storage\tSpecify the amount of storage allocated for the database. Example: 100 GB, 500 GB. Username\tProvide the master username for the PostgreSQL database. This is the main user with administrative privileges. Example: admin, root. Password\tPassword for the master user to access the database. Multi-Availability Zone\tEnable or disable Multi-AZ deployment. This ensures high availability and failover support by replicating the database across multiple availability zones. Recommended for production environments. Automated Backup\tConfigure automated backups for the PostgreSQL database. This ensures data protection by enabling daily snapshots and transaction log backups. Set up the Backup Retention Period and the Backup Window. Recommended for production environments. Backup Retention Period\tSet the number of days to retain automated backups. Backup Window\tPeriod of time while the backup will be done.  warning SleakOps allow the creation of replicas only during the creation of the dependency.  After that basic data, you need to decide if a replica will be created. To do that:  Into the form, look for the section Definition of RDS Read Replicas and click on + Add Item.Complete the following data:  Setting\tDescriptionName\tA name for the replica Replica Instance Class\tDefine the instance class that specifies the hardware configuration for your PostgreSQL database. This controls CPU, memory, and network performance. Example: db.m6g.large, db.t3.medium. Replica Publicly Accessible\tDecide if the replica should have a public IP or be accessible only within your private network.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variableâ€™s name for your PostgreSQL data base.â€‹","type":1,"pageTitle":"AWS PosgreSQL","url":"/project/dependency/postgresql-aws#3-customize-your-variables-name-for-your-postgresql-data-base","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS Oracle","type":0,"sectionRef":"#","url":"/project/dependency/oracle-aws","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"AWS Oracle","url":"/project/dependency/oracle-aws#faqs","content":" Licenseâ€‹ When creating an Oracle DB using Sleakops License Included (LI). Currently Bring Your Own License (BYOL) is not supported, however, contact support for more information.  How does SleakOps manage Oracle credentials?â€‹ When you create an Oracle dependency in SleakOps, it automatically generates a Vargroup for your database. This Variable Group securely stores the Oracle credentials and other important configuration details, such as the database endpoint and user access information. You'll be able of manage them from Vargroups section.  What is Multi-AZ deployment and should I enable it?â€‹ Multi-AZ (Availability Zone) deployment ensures high availability and failover support by replicating your database in another availability zone. It's recommended for production environments to prevent downtime. Keep in mind that it increases costs.  Can I change the Oracle version after the database is deployed?â€‹ No, the database engine version cannot be changed after deployment. You would need to create a new Oracle instance with the desired version and migrate your data. Or change it manually into the AWS Console.  What happens if I need more storage for my Oracle database?â€‹ You can adjust the storage size when configuring your database. If you need more storage after deployment, you can scale modifying the settings in AWS as at the moment SleakOps does not support it.  How do I create a Oracle database dump?â€‹ warning The client is only available for x86-64 Linux distributions. tip Follow this link to install the client To create a dump of your Oracle database, use the following command: exp ${ORACLE_USERNAME}/${ORACLE_PASSWORD}@${ORACLE_ENDPOINT}/${ORACLE_NAME} FILE=exp_file.dmp LOG=exp_file.log Replace ORACLE_USERNAME, ORACLE_ENDPOINT, ORACLE_NAME and ORACLE_PASSWORD with the appropriate values. For additional information on creating an Oracle dump, refer to the official Oracle documentation . Another option is creating it directly from the AWS Console and then import it. See Restoring to a DB instance.  How do I import an existent dump ?â€‹ warning The client is only available for x86-64 Linux distributions. tip Follow this link to install the client You can use a Oracle client installed on your local machine to import the dump. imp ${ORACLE_USERNAME}/${ORACLE_PASSWORD}@${ORACLE_ENDPOINT}/${ORACLE_NAME} FROMUSER=cust_schema TOUSER=cust_schema FILE=exp_file.dmp LOG=imp_file.log Replace ORACLE_USERNAME, ORACLE_ENDPOINT, ORACLE_NAME and ORACLE_PASSWORD with the appropriate values. For additional information on importing an Oracle dump, refer to the official Oracle documentation . Another option is creating it directly from the AWS Console and then import it. See Restoring to a DB instance.  What should I do if I encounter connection issues with my Oracle database?â€‹ Ensure the database endpoint, username, and password are correct.Verify that your security groups and firewall rules allow access.Ensure the database is running and has enough resources (CPU, memory). Otherwise, contact us.  info AWS documentation: Amazon RDS Oracle Documentation  ","version":"Next","tagName":"h2"},{"title":"Set up your Oracleâ€‹","type":1,"pageTitle":"AWS Oracle","url":"/project/dependency/oracle-aws#set-up-your-oracle","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add Oracle as a Dependencyâ€‹","type":1,"pageTitle":"AWS Oracle","url":"/project/dependency/oracle-aws#1-add-oracle-as-a-dependency","content":" To integrate Oracle with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;Oracle&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your Oracle.â€‹","type":1,"pageTitle":"AWS Oracle","url":"/project/dependency/oracle-aws#2-set-up-your-oracle","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionDatabase Engine Version\tSelect the specific version of the Oracle database engine you wish to use. This ensures compatibility with your application requirements. Example: 19.0.0.0.ru-2024-01.rur-2024-01.r1 Database Instance Class\tDefine the instance class that specifies the hardware configuration for your Oracle database. This controls CPU, memory, and network performance. Example: db.m6g.large, db.t3.medium. See AWS detail. Database Storage\tSpecify the amount of storage allocated for the database. Example: 100 GB, 500 GB. Username\tProvide the master username for the Oracle database. This is the main user with administrative privileges. Example: admin, root. Password\tPassword for the master user to access the database. Multi-Availability Zone\tEnable or disable Multi-AZ deployment. This ensures high availability and failover support by replicating the database across multiple availability zones. Recommended for production environments. Automated Backup\tConfigure automated backups for the Oracle database. This ensures data protection by enabling daily snapshots and transaction log backups. Set up the Backup Retention Period and the Backup Window. Recommended for production environments. Backup Retention Period\tSet the number of days to retain automated backups. Backup Window\tPeriod of time while the backup will be done.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable's name for your Oracle data base.â€‹","type":1,"pageTitle":"AWS Oracle","url":"/project/dependency/oracle-aws#3-customize-your-variables-name-for-your-oracle-data-base","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/project/deployment","content":"","keywords":"","version":"Next"},{"title":"Deploying your ProjectEnvâ€‹","type":1,"pageTitle":"Deployment","url":"/project/deployment#deploying-your-projectenv","content":" Upon creating a Deployment, we fetch the image corresponding to the Build to be deployed. Before this, we execute the Helm Release in the appropriate cluster namespace. This Helm release includes necessary Kubernetes services, ingresses, workers, and other services.  Various methods exist to generate a Deployment. These are outlined below. Note that we force certain Deployments. For details, refer to More on Deployment.  Workloads: Provides a switcher allowing you to decide whether to run a new Deployment.VariableGroup: Operates similarly to Workloads but doesn't create a new Release. Instead, it only updates the values of the Deployment.Dependency: Triggers a Deployment automatically. Delve deeper into this at More on Deployment.    ","version":"Next","tagName":"h2"},{"title":"Manual Deploymentâ€‹","type":1,"pageTitle":"Deployment","url":"/project/deployment#manual-deployment","content":" If you skip deploying your changes immediately, or if your modification doesn't enforces a deployment, you have three methods to execute a Deployment:  Build section: Utilizing the Deploy button, you can determine which Build to deploy.    Unpublished bannerâ€‹    Unpublished Changes Banner: This banner is shown when there's content pending that was not yet deployed on the cluster. Through this banner, you have the choice to deploy only the VariableGroups or if you want to deploy everything, including the Services in the 'draft' state.    Via the CLI . ","version":"Next","tagName":"h3"},{"title":"More on Deployments","type":0,"sectionRef":"#","url":"/project/deployment/more_on_deployment","content":"","keywords":"","version":"Next"},{"title":"How SleakOps Handles Deploymentsâ€‹","type":1,"pageTitle":"More on Deployments","url":"/project/deployment/more_on_deployment#how-sleakops-handles-deployments","content":" To execute a deployment, SleakOps utilizes the Build images stored in your project's image repository (AWS ECR), which are created either with the ProjectEnv entity during the Initial Build or with the creation of a Build entity that pushes to the ECR. Whenever a Deployment is initiated, we fetch the image corresponding to the designated Build.  The next phase involves constructing and deploying the Helm chart. This is accomplished using generally purpose-built templates. Once constructed, we upload the Helm chart to the same ECR utilized for the Build images and proceed to deploy a Helm Release into the Kubernetes cluster, specifically within the ProjectEnv namespace.  info All these resources reside in your own AWS Accounts. Sleakops does not exclusively store any data.  ","version":"Next","tagName":"h2"},{"title":"Forced Deploymentsâ€‹","type":1,"pageTitle":"More on Deployments","url":"/project/deployment/more_on_deployment#forced-deployments","content":" Forced Deployment Hace in mind that under certain circumstances, SleakOps forces a Deploy.  While multiple methods for generating a Deployment were highlighted in the primary Deployment documentation, it's crucial to understand that SleakOps sometimes enforces Deployments. The rationale behind this is to optimize uptime, safeguard the current state of the deployed infrastructure, and mitigate potential service downtimes on the Cluster. This imperative arises because Helm templates should always synchronize with the Kubernetes Secrets present in the namespace to avert deployment failures.  As you may already know, if it's not a 'forced' deployment, you'll be presented with an option (switcher) to determine if you wish to deploy your modifications. Deployments are forced in the following scenarios.  Workloads Alias Configuration Changes: A Deployment is forced if any alterations are made to the 'alias' configuration.Dependency: Always forces a Deployment to synchronize its associated VariableGroup state with the templates of the Helm Chart ensuring that Services operation is not affected.VariableGroup Deletion: Same case as Dependency deletion. ","version":"Next","tagName":"h3"},{"title":"Release","type":0,"sectionRef":"#","url":"/project/deployment/release","content":"","keywords":"","version":"Next"},{"title":"What is a Release?â€‹","type":1,"pageTitle":"Release","url":"/project/deployment/release#what-is-a-release","content":" In Sleakops, a release represents a deployable state of all the Workloads (web services, workers, cron jobs, hooks) of a project in an environment.  ","version":"Next","tagName":"h2"},{"title":"Release Creationâ€‹","type":1,"pageTitle":"Release","url":"/project/deployment/release#release-creation","content":" Sleakops administers releases for you. Every time you modify, delete, or add a web service, worker, hook, or cron job, Sleakops gives you the option to publish the changes. Each time you publish those changes, Sleakops creates a new release with auto-incremented versions.  ","version":"Next","tagName":"h2"},{"title":"Helm Chart Resourcesâ€‹","type":1,"pageTitle":"Release","url":"/project/deployment/release#helm-chart-resources","content":" Web Service:â€‹  A Kubernetes deploymentA Kubernetes serviceA Kubernetes HPA (Horizontal Pod Autoscaler)A Kubernetes ingress  The ingress generates its hosts using &lt;service_name&gt;.&lt;environment_name&gt;.&lt;organization_name&gt;.&lt;yourdomain.com&gt;  Worker:â€‹  A Kubernetes deploymentA Kubernetes HPA  Hook:â€‹  A Kubernetes job  This job uses Kubernetes hooks to start.  Cron Job:â€‹  A Kubernetes cron job ","version":"Next","tagName":"h3"},{"title":"Volumes","type":0,"sectionRef":"#","url":"/project/volumes","content":"Volumes In the context of Sleakops, volumes refer to AWS/Kubernetes storage resources attached to a cluster. They serve as general storage for specified containers. Pods can only interact with these volumes if they are explicitly attached to them. You can define the volumes in the Project form: Whenever a volume is needed, Sleakops deploys an EFS CSI Driver within the cluster. This allows every Project to have its own unique volume mount, storing folders based on the paths you specify. A practical use-case for Volumes is when you want all your pods to access the same files, such as a shared folder that requires its content to be persistent. For more details on how EFS is used for volumes, refer to the EFS documentation.","keywords":"","version":"Next"},{"title":"Workloads","type":0,"sectionRef":"#","url":"/project/workload","content":"","keywords":"","version":"Next"},{"title":"Which workload type is right for me?â€‹","type":1,"pageTitle":"Workloads","url":"/project/workload#which-workload-type-is-right-for-me","content":"   Web Service: Choose this if you need your application or service to be available 24/7 to respond to HTTP requests.Worker: Use this for background processing tasks, such as message queues or data pipelines, with no direct HTTP interaction.CronJob: Ideal for recurring maintenance or reporting tasks scheduled at specific times.Job: Suitable for one-time or on-demand tasks (e.g., manual database migrations).Hook: Perfect if you want to automate certain actions (like database migrations or analytics) on every deployment. ","version":"Next","tagName":"h2"},{"title":"VariableGroups","type":0,"sectionRef":"#","url":"/project/vargroup","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"VariableGroups","url":"/project/vargroup#faqs","content":" Can I edit a VariableGroup created by SleakOps?â€‹ Yes, you can access and manage the Variable Group for your MySQL instance from the SleakOps console. You can modify values such as usernames, passwords, or any other environment-specific credentials.  How are Vargroups used by my applications?â€‹ Vargroups are securely injected into your application's environment when it is deployed. Your applications can access these credentials and other variables without exposing sensitive information in the code.  How does SleakOps ensure the security of Vargroups?â€‹ SleakOps securely stores vargroups as Kubernetes secrets inside your EKS cluster. Access is controlled via Kubernetes Service Accounts, ensuring that only authorized components can access the sensitive information.  Can I delete a Variable Group?â€‹ Yes, they can be deleted or updated as needed. However, be cautious when deleting them, as it may disrupt your application.  What is the difference between a Global and a Workload-Scoped Vargroup?â€‹ Global: Available to all workload within the namespace. It is created without selecting a specific workload. To create them select â€œglobalâ€Workload-Scoped Variable Group: Only applies to the selected workload within the project and environment. It overrides global var group values if they have the same key.  What happens if there are duplicate keys in different Vargroups?â€‹ If duplicate keys exist across different var groups: If the key exists in both a global and a workload-scoped var group, the workload-scoped value takes precedence.If two global vargroups have the same key, the most recently created one will be used.  ","version":"Next","tagName":"h2"},{"title":"Create a VarGroupâ€‹","type":1,"pageTitle":"VariableGroups","url":"/project/vargroup#create-a-vargroup","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Vargroup sectionâ€‹","type":1,"pageTitle":"VariableGroups","url":"/project/vargroup#1-navigate-to-create-vargroup-section","content":" Into the Left Pane, access Vargroups option under Projects and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and complete the needed attributesâ€‹","type":1,"pageTitle":"VariableGroups","url":"/project/vargroup#2-select-a-project-and-complete-the-needed-attributes","content":" Complete the following attributes to create a new vargroup:  Attribute\tDescriptionProject\tThe specific application or workload within SleakOps. Determines the scope of the variable group. Workload\tA microservice or component within the project. If selected, the vargroup is limited to it; otherwise, by selecting global it'll be accessible into the namespace. Name\tA unique identifier for the var group, used to differentiate it within the project. Should be descriptive of the group's purpose. Deploy\tEnable this option if you want SleakOps to automatically publish and deploy your workloads into the project.  info If you choose to add the argument using the text option: Each argument should be added on a new line, separated by an equal sign (=), with no extra spaces. ARGUMENT_NAME = VALUE ARGUMENT_TWO = VALUE ARGUMENT_ONE = VALUE    Submit to create and Deploy your vargroup. ","version":"Next","tagName":"h3"},{"title":"Job","type":0,"sectionRef":"#","url":"/project/workload/job","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Job","url":"/project/workload/job#faqs","content":" How can I configure memory and CPU settings for my Job?â€‹ You can configure the CPU Min and CPU Max values to set the minimum and maximum CPU resources each instance in your cluster can use. Similarly, you set Memory Min and Memory Max for memory allocation per instance.  ","version":"Next","tagName":"h2"},{"title":"Lets add a Job for your Projectâ€‹","type":1,"pageTitle":"Job","url":"/project/workload/job#lets-add-a-job-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Job sectionâ€‹","type":1,"pageTitle":"Job","url":"/project/workload/job#1-navigate-to-create-job-section","content":" Into the Left Pane, access Workloads. Then select the Job tab and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and a Name for the jobâ€‹","type":1,"pageTitle":"Job","url":"/project/workload/job#2-select-a-project-and-a-name-for-the-job","content":" Start with the basic information, complete these attributes and click Next to continue.  Attribute\tDescriptionName\tIdentify your job. Project\tSelect between the existent projects. Command\tThe command that runs the job. Image\tBy default the job usage the image of your project, but you can override that with another Image tag\tYou can specify the tag of image.  Once those attributes are completed, click the Next button to move forward.    ","version":"Next","tagName":"h3"},{"title":"3. Finish the set upâ€‹","type":1,"pageTitle":"Job","url":"/project/workload/job#3-finish-the-set-up","content":" This step outlines the key attributes for configuring the resources of a Job in SleakOps, allowing for flexible management of CPU, memory, and scaling behaviors.  Attribute\tDescriptionCPU Min\tThe minimum amount of CPU resources allocated for each instance in the cluster. This ensures that each instance always has this amount of CPU available. CPU Max\tThe maximum CPU resources that each instance in the cluster can use. This cap helps prevent any single instance from consuming too much CPU. Memory Min\tThe minimum amount of memory allocated for each instance in the cluster. This guarantees that the instance has enough memory to operate efficiently. Memory Max\tThe maximum amount of memory each instance in the cluster can utilize. It limits the memory usage to prevent any single instance from overconsuming resources.    Submit to create and Deploy your job. ","version":"Next","tagName":"h3"},{"title":"Hooks","type":0,"sectionRef":"#","url":"/project/workload/hook","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#faqs","content":" How can I configure memory and CPU settings for my Hook?â€‹ You can configure the CPU Min and CPU Max values to set the minimum and maximum CPU resources each instance in your cluster can use. Similarly, you set Memory Min and Memory Max for memory allocation per instance.  ","version":"Next","tagName":"h2"},{"title":"Lets add a Hook for your Projectâ€‹","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#lets-add-a-hook-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Hook sectionâ€‹","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#1-navigate-to-create-hook-section","content":" Into the Left Pane, access Workloads. Then select the Hook tab and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and a Name for the hookâ€‹","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#2-select-a-project-and-a-name-for-the-hook","content":" Start with the basic information, complete these attributes and click Next to continue.  Attribute\tDescriptionName\tIdentify your web service. Project\tSelect between the existent projects. Command\tThe command that the workload runs.  Once those attributes are completed, click the Next button to move forward.    ","version":"Next","tagName":"h3"},{"title":"3. Define the deploy eventâ€‹","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#3-define-the-deploy-event","content":" Select when your hook will execute and click Next.  Attribute\tDescriptionEvent\tDefine when execute the hook. Check available events    ","version":"Next","tagName":"h3"},{"title":"5. Finish the set upâ€‹","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#5-finish-the-set-up","content":" This step outlines the key attributes for configuring the resources of a Hook in SleakOps, allowing for flexible management of CPU, memory, and scaling behaviors.  Attribute\tDescriptionCPU Min\tThe minimum amount of CPU resources allocated for each instance in the cluster. This ensures that each instance always has this amount of CPU available. CPU Max\tThe maximum CPU resources that each instance in the cluster can use. This cap helps prevent any single instance from consuming too much CPU. Memory Min\tThe minimum amount of memory allocated for each instance in the cluster. This guarantees that the instance has enough memory to operate efficiently. Memory Max\tThe maximum amount of memory each instance in the cluster can utilize. It limits the memory usage to prevent any single instance from overconsuming resources.    Submit to create and Deploy your hook. ","version":"Next","tagName":"h3"},{"title":"Web Service","type":0,"sectionRef":"#","url":"/project/workload/webservice","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#faqs","content":" What is the difference between Public, Private, and Internal service schemas?â€‹ Public: Accessible over the internet and is open to anyone.Private: Restricted access, available only when connected to the VPN.Internal: Only accessible within the same Kubernetes cluster and is used for internal communication between services.  How do I set up auto-scaling for my Web Service?â€‹ To enable auto-scaling, you can set the Autoscaling option to enabled and define the Memory Target and CPU Target. These targets determine the resource usage thresholds that trigger auto-scaling. You must also specify the minimum and maximum number of replicas to be maintained when auto-scaling is enabled.  What are the default success codes for a Web Service, and can I change them?â€‹ The default success code is 200, indicating the service is healthy. You can change this code based on your applicationâ€™s requirements, as some services might return different success codes based on specific actions.  What happens if my health check fails repeatedly?â€‹ If the health check fails consecutively and reaches the Failure Threshold (default is 60), the service is marked as unhealthy, and Kubernetes might restart or terminate the service instance to attempt a recovery.  How can I configure memory and CPU settings for my Web Service?â€‹ You can configure the CPU Min and CPU Max values to set the minimum and maximum CPU resources each instance in your cluster can use. Similarly, you set Memory Min and Memory Max for memory allocation per instance.  What are some best practices when configuring a Web Service in SleakOps?â€‹ Always set a minimum of 2 replicas to avoid downtime.Ensure your health check paths and success codes are correctly configured to reflect the true health of your service.Use auto-scaling where possible to optimize resources dynamically based on demand.Review and set memory and CPU usage targets appropriately to prevent overloading your infrastructure.  What should I do if my service shows response times longer than 10 seconds?â€‹ Long response times may indicate issues such as resource constraints, application inefficiencies, or network problems. You should check your service logs, ensure your resources (CPU, memory) are adequately allocated, and review your application code for potential optimizations.  How can I deploy my static web service?â€‹ At the moment, Sleakops doesnâ€™t natively support static sites, but you can still deploy them using the same flow as other sites, by containerizing them with a web server like Nginx. Below is a simple example of a Dockerfile and the corresponding nginx.conf to serve your static content. FROM node:20.11.0-alpine AS base WORKDIR /app FROM base AS build ARG BACKEND_URL WORKDIR /app COPY package.json package-lock.json ./ RUN npm install COPY . ./ RUN npm run build FROM nginx:1.25.3-alpine AS production COPY --from=build /app/config/nginx.conf /etc/nginx/conf.d/default.conf COPY --from=build /app/dist /usr/share/nginx/html EXPOSE 80 CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] And here is a sample config/nginx.conf: server { listen 80; location = /health { access_log off; add_header 'Content-Type' 'application/json'; return 200 '{&quot;status&quot;:&quot;OK&quot;}'; } location / { root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html =404; add_header Last-Modified $date_gmt; add_header Cache-Control 'no-store, no-cache'; if_modified_since off; expires off; etag off; } } Using this Docker-based approach, you can serve your static site with Nginx, all within a container.  ","version":"Next","tagName":"h2"},{"title":"Lets add a Web Service for your Projectâ€‹","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#lets-add-a-web-service-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Web Service sectionâ€‹","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#1-navigate-to-create-web-service-section","content":" Into the Left Pane, access Workloads. Then select the Web Services tab and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and a Name for the web serviceâ€‹","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#2-select-a-project-and-a-name-for-the-web-service","content":" Start with the basic information, complete these attributes and click Next to continue.  Attribute\tDescriptionName\tIdentify your web service. Project\tSelect between the existent projects. Command\tThe command that runs the service. Port\tThe port number where the service runs. Default: 8000  Once those attributes are completed, click the Next button to move forward.    ","version":"Next","tagName":"h3"},{"title":"3. Define the connectionâ€‹","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#3-define-the-connection","content":" Select how will be your connection and click on Next.  Attribute\tDescriptionService Schema\tDefines the accessibility of the service: public, private, or internal. URL\tThe URL assigned to the service based on the environment and project settings. Format: name.myenv.sleakops.com.    ","version":"Next","tagName":"h3"},{"title":"4. Specify your workload settingsâ€‹","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#4-specify-your-workload-settings","content":" Youâ€™ll see the following for to specify the conditions.    Attribute\tDescriptionPath\tThe path where Kubernetes checks if the web service is operational. Default: / Success Code\tThe HTTP success code indicating the serviceâ€™s health. Default: 200. Initial Delay Seconds\tNumber of seconds after startup before health checks begin. Default: 10. Timeout Seconds\tNumber of seconds after startup before health checks begin. Default: 1. Period Seconds\tInterval (in seconds) between each health check probe. Default: 5. Success Threshold\tMinimum number of consecutive successes required for the probe to be considered successful after it has failed. Default: 1. Failure Threshold\tNumber of consecutive failures before the probe is considered to have failed. Default: 60.  Once those attributes are completed, click the Next button to move to the next step.  ","version":"Next","tagName":"h3"},{"title":"5. Finish the set upâ€‹","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#5-finish-the-set-up","content":" This step outlines the key attributes for configuring the resources of a Web Service in SleakOps, allowing for flexible management of CPU, memory, and scaling behaviors.  Attribute\tDescriptionCPU Min\tThe minimum amount of CPU resources allocated for each instance in the cluster. This ensures that each instance always has this amount of CPU available. CPU Max\tThe maximum CPU resources that each instance in the cluster can use. This cap helps prevent any single instance from consuming too much CPU. Memory Min\tThe minimum amount of memory allocated for each instance in the cluster. This guarantees that the instance has enough memory to operate efficiently. Memory Max\tThe maximum amount of memory each instance in the cluster can utilize. It limits the memory usage to prevent any single instance from overconsuming resources. Autoscaling\tToggle to enable or disable auto-scaling. When enabled, it allows the service to adjust the number of replicas based on demand and resource usage. CPU Target\tThe CPU usage percentage target that initiates auto-scaling. If usage exceeds this target, additional replicas may be deployed to balance the load. Memory Target\tThe memory usage percentage target that triggers auto-scaling adjustments. When instances exceed this target, the system scales up to accommodate demand. Replicas Min\tThe minimum number of replicas to maintain when auto-scaling is active. A minimum of 2 replicas ensures high availability and prevents downtime. Replicas Max\tThe maximum number of replicas that can be deployed when auto-scaling is enabled. It sets an upper limit on the number of instances to avoid over-provisioning.    Submit to create and Deploy your web service. ","version":"Next","tagName":"h3"},{"title":"Providers","type":0,"sectionRef":"#","url":"/provider","content":"","keywords":"","version":"Next"},{"title":"Let's create your provider on SleakOpsâ€‹","type":1,"pageTitle":"Providers","url":"/provider#lets-create-your-provider-on-sleakops","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to the providers sectionâ€‹","type":1,"pageTitle":"Providers","url":"/provider#1-navigate-to-the-providers-section","content":" Into the Left Pane, access the Setting option and then Providers and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Set up basic Informationâ€‹","type":1,"pageTitle":"Providers","url":"/provider#2-set-up-basic-information","content":"   These are the settings you must define:  Setting\tDescriptionName\tSelect a name for the Organizative Unit into AWS under the needed accounts will be created. Region\tAWS region to use. If you want to know more about them, you can visit this documentationÂ https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html. Domain\tHere you must provide the domain you own in which the different environments will be deployed. It must be delegated to the Primary Route53 of SleakOps manually. Follow the steps described on thisÂ https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/MigratingDNS.html. Email\tBy default SleakOps uses the email from the root account provided, If you want to use other email to register your SleakOps accounts in AWS, fill in this field.  Once you've completed the form, click on Next to move forward.  ","version":"Next","tagName":"h3"},{"title":"3. Connect to your AWS Root Accountâ€‹","type":1,"pageTitle":"Providers","url":"/provider#3-connect-to-your-aws-root-account","content":" warning You must be logged into your AWS Root Account.  To start installing your application, we need to connect to your AWS Root Account. Here's how to do it:  By clicking the Next button, youâ€™ll be redirected to AWS to create an IAM role in your main account called &quot;SleakopsIntegrationRole&quot;.  This role lets us access necessary resources, making installation quick and smooth.After installation, we will remove this role to keep your account secure.    ","version":"Next","tagName":"h3"},{"title":"4. Ongoing Organizative Unit process startedâ€‹","type":1,"pageTitle":"Providers","url":"/provider#4-ongoing-organizative-unit-process-started","content":" note Creating your Organizative Unit doesn't generate any cost on your AWS account ğŸ˜ƒ  Once the connection is established role is created, SleakOps will automatically start the Organizative Unit creation.  This process will take a few minutes.    ","version":"Next","tagName":"h3"},{"title":"5. Learn about the infrastructure architecture created by SleakOps for you.â€‹","type":1,"pageTitle":"Providers","url":"/provider#5-learn-about-the-infrastructure-architecture-created-by-sleakops-for-you","content":" In order to understand what was created on your AWS, please see Accounts. ","version":"Next","tagName":"h3"},{"title":"Worker","type":0,"sectionRef":"#","url":"/project/workload/worker","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Worker","url":"/project/workload/worker#faqs","content":" How do I set up auto-scaling for my Worker?â€‹ To enable auto-scaling, you can set the Autoscaling option to enabled and define the Memory Target and CPU Target. These targets determine the resource usage thresholds that trigger auto-scaling. You must also specify the minimum and maximum number of replicas to be maintained when auto-scaling is enabled.  How can I configure memory and CPU settings for my Worker?â€‹ You can configure the CPU Min and CPU Max values to set the minimum and maximum CPU resources each instance in your cluster can use. Similarly, you set Memory Min and Memory Max for memory allocation per instance.  ","version":"Next","tagName":"h2"},{"title":"Lets add a Worker for your Projectâ€‹","type":1,"pageTitle":"Worker","url":"/project/workload/worker#lets-add-a-worker-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Worker sectionâ€‹","type":1,"pageTitle":"Worker","url":"/project/workload/worker#1-navigate-to-create-worker-section","content":" Into the Left Pane, access Workloads. Then select the Worker tab and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and a Name for the workerâ€‹","type":1,"pageTitle":"Worker","url":"/project/workload/worker#2-select-a-project-and-a-name-for-the-worker","content":" Start with the basic information, complete these attributes and click Next to continue.  Attribute\tDescriptionName\tIdentify your worker. Project\tSelect between the existent projects. Command\tThe command that runs the worker.  Once those attributes are completed, click the Next button to move forward.    ","version":"Next","tagName":"h3"},{"title":"3. Finish the set upâ€‹","type":1,"pageTitle":"Worker","url":"/project/workload/worker#3-finish-the-set-up","content":" This step outlines the key attributes for configuring the resources of a Worker in SleakOps, allowing for flexible management of CPU, memory, and scaling behaviors.  Attribute\tDescriptionCPU Min\tThe minimum amount of CPU resources allocated for each instance in the cluster. This ensures that each instance always has this amount of CPU available. CPU Max\tThe maximum CPU resources that each instance in the cluster can use. This cap helps prevent any single instance from consuming too much CPU. Memory Min\tThe minimum amount of memory allocated for each instance in the cluster. This guarantees that the instance has enough memory to operate efficiently. Memory Max\tThe maximum amount of memory each instance in the cluster can utilize. It limits the memory usage to prevent any single instance from overconsuming resources. Autoscaling\tToggle to enable or disable auto-scaling. When enabled, it allows the workload to adjust the number of replicas based on demand and resource usage. CPU Target\tThe CPU usage percentage target that initiates auto-scaling. If usage exceeds this target, additional replicas may be deployed to balance the load. Memory Target\tThe memory usage percentage target that triggers auto-scaling adjustments. When instances exceed this target, the system scales up to accommodate demand. Replicas Min\tThe minimum number of replicas to maintain when auto-scaling is active. A minimum of 2 replicas ensures high availability and prevents downtime. Replicas Max\tThe maximum number of replicas that can be deployed when auto-scaling is enabled. It sets an upper limit on the number of instances to avoid over-provisioning.    Submit to create and Deploy your worker. ","version":"Next","tagName":"h3"},{"title":"Cronjobs","type":0,"sectionRef":"#","url":"/project/workload/cronjob","content":"","keywords":"","version":"Next"},{"title":"FAQsâ€‹","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#faqs","content":" How can I configure memory and CPU settings for my Cronjob?â€‹ You can configure the CPU Min and CPU Max values to set the minimum and maximum CPU resources each instance in your cluster can use. Similarly, you set Memory Min and Memory Max for memory allocation per instance.  ","version":"Next","tagName":"h2"},{"title":"Lets add a Cronjob for your Projectâ€‹","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#lets-add-a-cronjob-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Cronjob sectionâ€‹","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#1-navigate-to-create-cronjob-section","content":" Into the Left Pane, access Workloads. Then select the Cronjob tab and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and a Name for the cronjobâ€‹","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#2-select-a-project-and-a-name-for-the-cronjob","content":" Start with the basic information, complete these attributes and click Next to continue.  Attribute\tDescriptionName\tIdentify your cronjob. Project\tSelect between the existent projects. Command\tThe command that runs the cronjob.  Once those attributes are completed, click the Next button to move forward.    ","version":"Next","tagName":"h3"},{"title":"3. Define the periodicityâ€‹","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#3-define-the-periodicity","content":" Select how will be your connection and click on Next.  Attribute\tDescriptionCrontab\tCron expresion to determine the schedule to execute the cronjob    ","version":"Next","tagName":"h3"},{"title":"5. Finish the set upâ€‹","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#5-finish-the-set-up","content":" This step outlines the key attributes for configuring the resources of a Cronjob in SleakOps, allowing for flexible management of CPU, memory, and scaling behaviors.  Attribute\tDescriptionCPU Min\tThe minimum amount of CPU resources allocated for each instance in the cluster. This ensures that each instance always has this amount of CPU available. CPU Max\tThe maximum CPU resources that each instance in the cluster can use. This cap helps prevent any single instance from consuming too much CPU. Memory Min\tThe minimum amount of memory allocated for each instance in the cluster. This guarantees that the instance has enough memory to operate efficiently. Memory Max\tThe maximum amount of memory each instance in the cluster can utilize. It limits the memory usage to prevent any single instance from overconsuming resources.    Submit to create and Deploy your cronjob. ","version":"Next","tagName":"h3"},{"title":"Common errors on Providers","type":0,"sectionRef":"#","url":"/provider/common-errors","content":"","keywords":"","version":"Next"},{"title":"1. The Account ID set does not have root accessâ€‹","type":1,"pageTitle":"Common errors on Providers","url":"/provider/common-errors#1-the-account-id-set-does-not-have-root-access","content":" In this case, by clicking the Fix button, youâ€™ll be redirected to AWS again.  Be sure to be logged as a Root user.    ","version":"Next","tagName":"h3"},{"title":"2. Maximum Number of AWS Accounts Reachedâ€‹","type":1,"pageTitle":"Common errors on Providers","url":"/provider/common-errors#2-maximum-number-of-aws-accounts-reached","content":" AWS has an account limit that can prevent new ones.  Before retrying the process, increase that limit. Otherwise, the process will fail again.    ","version":"Next","tagName":"h3"},{"title":"3. Other errorsâ€‹","type":1,"pageTitle":"Common errors on Providers","url":"/provider/common-errors#3-other-errors","content":" Other issues might happen and usually, they'll be solved by running the Connection to AWS again.  If the error remains and you've tried deleting the provider and creating a new one. Please, do not hesitate and report us an issue.   ","version":"Next","tagName":"h3"},{"title":"Deleting a Provider","type":0,"sectionRef":"#","url":"/provider/deleting-a-provider","content":"","keywords":"","version":"Next"},{"title":"How to delete a Providerâ€‹","type":1,"pageTitle":"Deleting a Provider","url":"/provider/deleting-a-provider#how-to-delete-a-provider","content":" ","version":"Next","tagName":"h2"},{"title":"1. Select the provider to be deletedâ€‹","type":1,"pageTitle":"Deleting a Provider","url":"/provider/deleting-a-provider#1-select-the-provider-to-be-deleted","content":" Once you are in the Providers section, choose a provider and click on the Three Dots **button to display the Delete option. Click on it.    ","version":"Next","tagName":"h3"},{"title":"2. Confirm the procedureâ€‹","type":1,"pageTitle":"Deleting a Provider","url":"/provider/deleting-a-provider#2-confirm-the-procedure","content":" You'll see a modal to confirm the action. Remember this action will remove all the infrastructure created on AWS under this Provider.    ","version":"Next","tagName":"h3"},{"title":"3. Manually remove the Organization and its Accountsâ€‹","type":1,"pageTitle":"Deleting a Provider","url":"/provider/deleting-a-provider#3-manually-remove-the-organization-and-its-accounts","content":" As it was mentioned before the created Organization and its accounts (management, development, production, and security) will not be automatically deleted.  Access your AWS Root Account to manually remove them by going to AWS Organizations.   ","version":"Next","tagName":"h3"},{"title":"Accounts","type":0,"sectionRef":"#","url":"/provider/accounts","content":"","keywords":"","version":"Next"},{"title":"Provider's Accountsâ€‹","type":1,"pageTitle":"Accounts","url":"/provider/accounts#providers-accounts","content":" Sleakops implements a well-defined infrastructure architecture designed to optimize operational excellence while ensuring a secure and scalable environment for users. The architecture consists of four accounts, each serving distinct purposes and isolated from one another.  Each account has a VPN instance generated upon the creation of the first cluster.  Once the Accounts are up, we set to each one of them what we call Network Module it contains a lot of different AWS services that are used to make the network connections inside accounts.    ","version":"Next","tagName":"h2"},{"title":"Security Accountâ€‹","type":1,"pageTitle":"Accounts","url":"/provider/accounts#security-account","content":" The Security Account serves as a centralized hub for managing IAM users and their access to the system. Learn how to switch between accounts in AWS Console Autentication.  ","version":"Next","tagName":"h3"},{"title":"Management Accountâ€‹","type":1,"pageTitle":"Accounts","url":"/provider/accounts#management-account","content":" Designed to maintain internal services used for application maintenance, regardless of whether they are shared across accounts. Example: Sentry.  Contains an EKS cluster with integrated CI/CD (GitHub and HashiCorp Vault).Vault manages credentials for CloudWatch, enhancing monitoring capabilities.VPC Peering enables private connections to other accounts.  ","version":"Next","tagName":"h3"},{"title":"Development Accountâ€‹","type":1,"pageTitle":"Accounts","url":"/provider/accounts#development-account","content":" For the different stages of your application before it goes into production.  Contains three environments: dev, QA, and staging.Replicas of the prod environment for code writing, testing, and pre-releases.Ensures isolated testing to prevent issues for external users.Similar architecture to prod but without RDS Slave for reduced high availability requirements.  ","version":"Next","tagName":"h3"},{"title":"Production Accountâ€‹","type":1,"pageTitle":"Accounts","url":"/provider/accounts#production-account","content":" This account is intended for your application to be installed in a production environment, isolated from the rest of your application's stages.  Supports external users and requires a fully functional database (RDS Master).Utilizes Private DB Subnet for RDS Master, RDS Slave, and ElastiCache, each on different Availability Zones (AZs) for high availability.Backend Deployment with replicas distributed across different AZs.Frontend Deployment with LoadBalancer for even distribution of network load.Route53 serves as DNS and performs health checks for the application.AWS CloudFront serves static frontend content from an S3 bucket.RDS Slave acts as a replica of RDS Master for failover scenarios, maximizing uptime.  ","version":"Next","tagName":"h3"},{"title":"Selecting an Account in SleakOpsâ€‹","type":1,"pageTitle":"Accounts","url":"/provider/accounts#selecting-an-account-in-sleakops","content":" To select an account and be able to work on it, select it from the left pane. The left icon refers to the Provider that groups the accounts. ","version":"Next","tagName":"h3"},{"title":"Shared Responsibility Model","type":0,"sectionRef":"#","url":"/responsability-model","content":"Shared Responsibility Model We transparently utilize all AWS services, which means the responsibility extends to the guidelines set by AWS. To safeguard your data and ensure it doesn't get lost due to potential service disruptions, it's advisable to have backup policies in place. Currently, we don't support backups for our dependencies (RDS, S3, RabbitMQ, etc.). To set this up, you can access via your AWS client and define your backup policies. [Learn more about AWS's Shared Responsibility Model] (https://aws.amazon.com/compliance/shared-responsibility-model/)","keywords":"","version":"Next"},{"title":"Designing your Infra","type":0,"sectionRef":"#","url":"/provider/schemas","content":"","keywords":"","version":"Next"},{"title":"Single Schema Vs. Multi Schemaâ€‹","type":1,"pageTitle":"Designing your Infra","url":"/provider/schemas#single-schema-vs-multi-schema","content":" SleakOps provides the flexibility and control necessary to build infrastructure tailored to your specific requirements.  While we recommend adopting a Multiple Schema configuration to align with best practices, we understand that different stages of your project may require alternative schema configurations.    Here a comparative with two options:  \tMulti Schema â­ï¸\tSingle SchemaDescription\tAligned with best practices. You'll set first the Development account.\tCentralizes your environments within a single cluster. Account to be used\tUse all accounts as described here\tOnly the Production account Pros\tIncreases Security by granting access per account Production remains isolated.\tReduces costs, as it is just one cluster. Cons\tMore expensive as each environment'll have it own cluster and VPN.\tLess secure, ass all environments shares the account.  These are just two options; you have the freedom to create the schema that best suits your needs. ","version":"Next","tagName":"h2"},{"title":"User","type":0,"sectionRef":"#","url":"/user","content":"","keywords":"","version":"Next"},{"title":"User Creationâ€‹","type":1,"pageTitle":"User","url":"/user#user-creation","content":" Sleakops has three fundamental fields of user permissions: Role: Defines what the user is allowed to do inside SleakOps, it's separated into three roles, 'Admin', 'Editor' and 'Read Only'.  Viewer: Read-only user. In AWS IAM it uses the ReadOnlyAccess Editor: It has PowerUser permissions which allows them to create infrastructure resources in Sleakops and AWS but not to manage users. You have to select to which accounts the User will have permissions. In AWS IAM it uses the PowerUserAccess Admin: Same as Editors but with the addition that they can manage other users permissions and they have complete access to every account. In AWS IAM it uses the AdministratorAccess   AWS Account Accesses: This field shows you every account, here you select to which accounts the user (Editor or Read-only) will have access. VPN Account Accesses: It's similar to the AWS account accesses field but here you set if a user it's also created on the VPN Server of the account you give. More information can be checked on VPN documentation    For access into the AWS accounts SleakOps initially sets a random password and sends it to the email of the created user. The user can login with that password but it will be obligated to change its password on the first login. For SleakOps platform access we use the password that was set on the User form.  After this user creation an AWS User will be created on the 'security' Account, this account is where we control acesses to all the SleakOps AWS accounts. We will also create, depending on the configuration, users on the VPN servers, read how to use them on the corresponding documentation and on the SleakOps user. ","version":"Next","tagName":"h2"},{"title":"AWS Console Authentication","type":0,"sectionRef":"#","url":"/user/aws_console_authentication","content":"AWS Console Authentication ::: tip[VIDEO] How to enter to any of your Accounts. ::: As described in the Architecture Overview. You'll have to enter the 'security' account, then, assume the role on the account you want. The easiest way to do this is by using the Sleakops Dashboard: First, use the AWS Login button: This will open the AWS login form. The Account ID field should be automatically filled with the 'security' account ID. If this doesn't happen, it might be because another service is attempting to fill the fields. Once logged into the 'security' account, it will appear as shown in the following image: Now, in the AWS console, you need to return to the SleakOps dashboard, select 'Get Access' and use the drawer, on it, select the account you want to log in to. This will prompt a new AWS tab to switch the role from your 'security' Account into the account you've selected, you'll leave the 'security account' and enter the selected one. info If you're in 'security' or another account you can directly use the account switchers, AWS understands that you are already inside the 'security' account. For more information about this process, you can read its AWS documentation .","keywords":"","version":"Next"},{"title":"Django + Celery","type":0,"sectionRef":"#","url":"/quickstart/django_celery","content":"","keywords":"","version":"Next"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#prerequisites","content":" Account in SleakopsA Cluster in this account. If you don't have it, here is the documentation on how to do it.A Environment configured. If you don't have it, here is the documentation on how to do itDjango project configured with celery (This project needs to have docker).  ","version":"Next","tagName":"h2"},{"title":"Let's Startâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#lets-start","content":" For this example, we are going to use this project . It is a Django project with Celery that already has Docker configured to run. We are also going to configure a Postgresql database, S3 bucket and Rabbitmq needed for this project.  ","version":"Next","tagName":"h2"},{"title":"Create a projectâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-a-project","content":" To start, we are going to create a new project. To do this, click the &quot;Projects&quot; button in the left panel:    Inside the Projects panel you will be able to see all the projects you have and manage them from here. We want to create a new one so let's click on the â€œcreateâ€ button at the top right:    In the project creation screen we have the following fields:  Setting\tDescriptionEnvironment\tWe have to select the previously created environment. Nodepool\tWe will leave the default one. Repositories\tWe will select our repository that we want to deploy. In our case example-django-celery. Project Name\tWe can define a project name. For the example we will leave the default. Branch\tIt has to coincide with the one we have in our project. In our case it is â€œMainâ€. Dockerfile path\tIt is the relative path to the dockerfile in your project.  Once configured all that we create the project with the â€œSubmitâ€ button at the bottom right:    With that, the project begins to be created. In the meantime we go to the workloads with the â€œWorkloadsâ€ button in the left panel:    ","version":"Next","tagName":"h3"},{"title":"Create a Web Serviceâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-a-web-service","content":" Here what we are going to do is to create a web service so we go to the web service section and create one:    In this page we are going to complete the first form with the following fields:  Setting\tDescriptionProject\tWe select the project we created previously, in our case â€œexample-django-celeryâ€. Name\tWe define a name for the web service. Command\tBy default this will take the value that is in the dockerfile, in our case this is fine. Port\tThe same as the command.  Then we continue by clicking on the â€œNextâ€ button up to step 3:    In step 3 we have to edit the path field and put the endpoint of healthcheck which in our case is â€œ/healthcheck/â€. Then click on the â€œNextâ€ button until the web service is created:    ","version":"Next","tagName":"h3"},{"title":"Deploy celery workerâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#deploy-celery-worker","content":" Well, with this we can see our web service deploying. Now we are going to deploy the celery. For this we have to go to the workers section inside the same workloads screen:    And click on the â€œCreateâ€ button to create a new one:    In the workers creation screen we will have to complete the following fields:  Setting\tDescriptionProject\tSelect the previously created project. In our case â€œexample-django-celeryâ€. Name\tWe define the name that we are going to give to the worker. In our case â€œceleryâ€. Command\tHere we set the command to run celery, in our case it is: bash celery -A core.celery_app worker -l INFO --concurrency 1 --max-tasks-per-child 1 --prefetch-multiplier 1 -n celery@%h --queues default,build,deployment,cluster,canvas,billing  With these fields filled in we will click on the â€œNextâ€ button at the bottom right and then â€œSubmitâ€ as we do not need to edit anything else:    With this we will see our celery published. Now we have to configure the hooks. For this we go to the hooks section:    ","version":"Next","tagName":"h3"},{"title":"Create a migration hookâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-a-migration-hook","content":" In the hook creation screen we will have the following fields:  Setting\tDescriptionProject\tSelect the previously created project. In our case â€œexample-django-celeryâ€. Name\tWe define the name that we are going to give to the worker. In our case â€œmigrationsâ€. Command\tHere we set the command to run celery, in our case it is: bash python manage.py migrate --no-input   With these fields filled in we will click on the â€œNextâ€ button at the bottom right and then â€œSubmitâ€ as we do not need to edit anything else:    ","version":"Next","tagName":"h3"},{"title":"Create a collect static hookâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-a-collect-static-hook","content":" Now we proceed to create another hook that we need for the statics:    In this form we are going to do the same as the previous one but modifying the command. We click next until we create the hook (without modifying anything else):    The command we use is as follows:  python manage.py collectstatic --no-input   ","version":"Next","tagName":"h3"},{"title":"Create a Postgresql Databaseâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-a-postgresql-database","content":" Once we have created the hooks we have to go to create our database. To do this we go to the â€œDependenciesâ€ section:    Inside this section we click on the â€œCreateâ€ button at the top right and then select â€œPostgresqlâ€:      In the 1st postgresql creation form we will have to select our previously created project and define a name for it, then click on the â€œNextâ€ button at the bottom right:    In the 2nd form we are going to have a lot of fields, the only ones that matter to us are the following:  Setting\tDescriptionDatabase Master Username\tHere we assign a root user name to our database. Database Master Password\tA password for this root user.  With these fields we are ready to move forward. We click on the â€œNextâ€ button at the bottom right and then on â€œSubmitâ€, with this we have already created our database:    ","version":"Next","tagName":"h3"},{"title":"Create S3 Bucketâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-s3-bucket","content":" In the same page of dependencies we have to create our s3 bucket, for it we are going to go to the â€œCreateâ€ button again:    And select S3 Bucket:    In the first form we have to select our previously created project and define a name for the bucket, we have to take into account that the name of the bucket is global so it has to be unique. Now click on the â€œNextâ€ button and go to step 3:    Here we are going to see some environment variables defined for the bucket. We are going to edit the one that says COLLECTSTATICEXAMPLEDJANGOCELERY_BUCKET_NAME and we are going to call it DJANGO_AWS_STORAGE_BUCKET_NAME. With this simple change we click on the â€œSubmitâ€ button at the bottom right to finish creating the bucket:    ","version":"Next","tagName":"h3"},{"title":"Create Rabbitmqâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-rabbitmq","content":" Now we need one more dependency. Our Rabbitmq to queue celery tasks, so let's get to it:    And select Rabbitmq:    In the first form we will have to select our project and define a name for it. Then click on the â€œNextâ€ button at the bottom right:    In the following form we have several fields but the only ones that we care for this example is the username and password, we can define what we want. For this example I chose admin as username and for the password I generated it randomly with the dice button. Then we click the â€œNextâ€ button to create the dependency:    ","version":"Next","tagName":"h3"},{"title":"Create yours environment variablesâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-yours-environment-variables","content":" Once the dependencies are deployed we have to configure our environment variables. But first we have to get some data from rabbitmq, for this we go to the rabbitmq detail:    And from here we need the following data:  RABBITMQ_BROKER_AUTH_URL    Save that url because now we are going to need it. Now we are going to go to the Vargroups section:    Here you will see all your environment variables that you created grouped in groups, for example you should have created one with the data for the database (which is the one you see in the image). Now we are going to create another one for our django environment variables, for this we click on the â€œCreateâ€ button at the top right:    In this form we have the following fields:  Project: we select the project we created previously.Workload: We select â€œglobalâ€ that makes reference to be used by all our workloads.Name: We define a name for this group of variables.Type: If we want to load it by file or by variable.Vars: Here we enable the textmode and copy the following environment variables:  CELERY_BROKER_URL=amqps://username:password@host:port CELERY_RESULT_BACKEND=django-db DJANGO_ADMIN_URL=admin/ DJANGO_DEBUG=False DJANGO_SECRET_KEY=secret_key DJANGO_SETTINGS_MODULE=core.settings.production DJANGO_STATIC_STORAGE=storages.backends.s3boto3.S3StaticStorage ENVIRONMENT=production LOGS_LEVEL=INFO PYTHONPATH=.   This is where you will paste your url in the variable â€œCELERY_BROKER_URLâ€ all the others you can leave the same. Finally click on the â€œSubmitâ€ button at the bottom right to create the variable group.    ","version":"Next","tagName":"h3"},{"title":"Deploymentsâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#deployments","content":" As last step we are going to see our project deployed, for this we go to the â€œDeploymentsâ€ section of the left panel:    Here we are going to see all the deploys that we do. In our case it is the first one and we can see that it has been created correctly, in case you see any error if you click on â€œerrorâ€ you can see a description of it. If we do not see any error then it means that the project is already deployed, we could begin to use it from the url that the web service provided us.    This concludes our project deployment process. We leave you an optional step which is to configure the ci with github.  ","version":"Next","tagName":"h3"},{"title":"Optionalâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#optional","content":" ","version":"Next","tagName":"h2"},{"title":"CI with Githubâ€‹","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#ci-with-github","content":" Every time you make a change in your code and want to deploy it you will have to do a build and a deploy, this eventually becomes tedious. That's why to avoid this we have to implement ci on github.  For this we are going to go to â€œProjectsâ€ in the left panel:    Let's locate our project and click on the gear to access the project configuration:    In the project configuration we locate the one that says â€œGit pipelinesâ€ and click on it:    Here we are going to find what we need to do this. Basically we need to set up a file in the root of our project .github/workflows/ called ci_sleakops_demo.yml and in that file we are going to paste the content that appears in this page.    This needs to have an environment variable SLEAKOPS_KEY, if you don't have it you have to go to the link that appears there Settings -&gt; CLI, get it and save it as an environment variable.  With this configured and deployed every time you do a push to your â€œmainâ€ branch a new version of your application will be launched automatically. ","version":"Next","tagName":"h3"},{"title":"Connect into the VPN","type":0,"sectionRef":"#","url":"/user/vpn","content":"Connect into the VPN To handle VPN connections we use Pritunl. You can download the client here. Once you've created a Provider , go to SleakOps dashboard and select the account for which you want to get VPN access. Remember, to do this, you need to have access to the VPN of that account, but the VPN must also be created. We create the VPN of a specific account when the first cluster of that account is created. This will prompt you with what is called the URI profile. It has a validation period of 24 hours, and you must load it into the Pritunl client. Copy it and import it into the Pritunl client, and you'll be able to connect to it:","keywords":"","version":"Next"},{"title":"Welcome to SleakOps!","type":0,"sectionRef":"#","url":"/","content":"","keywords":"","version":"Next"},{"title":"Guiding principlesâ€‹","type":1,"pageTitle":"Welcome to SleakOps!","url":"/#guiding-principles","content":" Respect the AWSÂ well architected bases.Keeping always an eye on costs.You have full control, itâ€™s your repo and your cloud.  ","version":"Next","tagName":"h2"},{"title":"Main Featuresâ€‹","type":1,"pageTitle":"Welcome to SleakOps!","url":"/#main-features","content":" GitHub, Bitbucket and Gitlab integrationBased on your repo and DockerfilesManage multiple environments using our proposed structure (dev, staging and production) or customize your own.Configure your CI/CD pipeline.Complete Observability stack for logging, monitoring and tracing.Secrets and env vars managment.Secure connections by TLS.Automated configuration for your services behind a load balancer and secureÂ ingress.Easy add AddOns to your cluster.Lot of dependencies ready to go (RDS, S3, Redis, SQL, rRabbit, etc.).Users access to services management.Automated VPNs.  Providers A cloud provider account. Getting Started Clusters A set of worker machines, called nodes, that run containerized applications. Getting Started Environments Abstraction that let us isolate the different resources. Getting Started Projects Represents a codebase and it is managed by a git repository. Getting Started Dependencies Pieces of underlying infrastructure your apps need to run in the cloud, such as relational databases, storage services or caches. Getting Started Workloads An abstract way to expose an application running on a set of Pods as a network service. Getting Started Deployments ... Getting Started Build Represents a deployable state of all the services. Getting Started Var Group Dictionary that provides configuration for services. Getting Started ","version":"Next","tagName":"h2"}],"options":{"languages":["en","es"],"indexBaseUrl":true,"id":"default"}}