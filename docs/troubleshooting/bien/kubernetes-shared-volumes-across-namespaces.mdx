---
sidebar_position: 3
title: "Kubernetes Shared Volumes Across Namespaces"
description: "Solution for sharing volumes between pods in different namespaces and alternative approaches"
date: "2025-01-30"
category: "cluster"
tags: ["kubernetes", "volumes", "namespaces", "storage", "s3"]
---

import TroubleshootingItem from "@site/src/components/HomepageFeatures/troubleshootingitem";

# Kubernetes Shared Volumes Across Namespaces

**Date:** January 30, 2025  
**Category:** Cluster  
**Tags:** Kubernetes, Volumes, Namespaces, Storage, S3

## Problem Description

**Context:** User needs to share data between a cronjob that generates files and an nginx service that serves those files, but they are running in different namespaces in a Kubernetes cluster.

**Observed Symptoms:**

- Cannot mount the same volume across different namespaces
- Need to share generated files between cronjob and web server
- Current setup works on EC2 with shared volume mount
- Looking for Kubernetes equivalent of shared volume access

**Relevant Configuration:**

- Platform: SleakOps on AWS EKS
- Use case: Cronjob generates files, Nginx serves them
- Current setup: EC2 with shared volume between containers
- Target: Kubernetes pods in different namespaces

**Error Conditions:**

- Kubernetes limitation: same volume cannot be used across namespaces
- Need alternative solution for file sharing
- Performance requirements for large file generation (5GB+)

## Detailed Solution

<TroubleshootingItem id="kubernetes-volume-limitation" summary="Understanding Kubernetes volume limitations">

Kubernetes has a fundamental limitation: **the same PersistentVolume cannot be mounted by pods in different namespaces**. This is by design for security and isolation purposes.

This means your current EC2 approach of sharing a volume between containers won't work directly in Kubernetes when pods are in different namespaces.

</TroubleshootingItem>

<TroubleshootingItem id="s3-solution" summary="Recommended solution: Use S3 as shared storage">

The recommended approach is to use **Amazon S3** as intermediate storage:

### Architecture:

1. **Cronjob**: Generates files locally → Uploads to S3
2. **Nginx service**: Downloads files from S3 → Serves them

### Benefits:

- Works across namespaces
- Scalable and reliable
- Cost-effective for large files
- Built-in authentication via SleakOps service accounts

### Configuration example:

```yaml
# Cronjob configuration
apiVersion: batch/v1
kind: CronJob
metadata:
  name: file-generator
  namespace: jobs
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: generator
              image: your-app:latest
              env:
                - name: S3_BUCKET
                  value: "your-bucket-name"
                - name: AWS_REGION
                  value: "us-east-1"
              volumeMounts:
                - name: temp-storage
                  mountPath: /tmp/files
          volumes:
            - name: temp-storage
              emptyDir:
                sizeLimit: 60Gi
```

</TroubleshootingItem>

<TroubleshootingItem id="nodepool-configuration" summary="Configure nodepool for adequate storage">

For large file generation, configure your nodepool with sufficient EBS storage:

### In SleakOps:

1. Go to **Cluster Configuration**
2. Select your **Nodepool**
3. Modify **Node Configuration**:
   - **EBS Volume Size**: 50-60 GB
   - **Volume Type**: gp3 (faster and cheaper)

### Configuration example:

```yaml
nodepool_config:
  instance_type: "t3.medium"
  disk_size: 60 # GB
  disk_type: "gp3"
  min_nodes: 1
  max_nodes: 5
```

</TroubleshootingItem>

<TroubleshootingItem id="s3-authentication" summary="S3 authentication in SleakOps">

SleakOps automatically configures S3 authentication via **service accounts**. You don't need to manage AWS credentials manually.

### Java example for S3 access:

```java
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.model.*;

public class S3FileUploader {
    public static void main(String[] args) {
        // SleakOps handles authentication automatically
        S3Client s3 = S3Client.builder()
                               .region(Region.US_EAST_1)
                               .build();

        // Upload file to S3
        PutObjectRequest putRequest = PutObjectRequest.builder()
                .bucket("your-bucket-name")
                .key("generated-files/data.zip")
                .build();

        s3.putObject(putRequest,
                    RequestBody.fromFile(new File("/tmp/files/data.zip")));
    }
}
```

### Python example:

```python
import boto3
import os

# SleakOps handles authentication via service account
s3_client = boto3.client('s3')
bucket_name = os.environ['S3_BUCKET']

# Upload generated file
s3_client.upload_file(
    '/tmp/files/generated_data.zip',
    bucket_name,
    'generated-files/generated_data.zip'
)
```

</TroubleshootingItem>

<TroubleshootingItem id="nginx-s3-integration" summary="Configure Nginx to serve files from S3">

For serving files from S3 through Nginx, you have several options:

### Option 1: Nginx with S3 proxy

```nginx
server {
    listen 80;
    server_name your-domain.com;

    location /files/ {
        proxy_pass https://your-bucket.s3.amazonaws.com/;
        proxy_set_header Host your-bucket.s3.amazonaws.com;
        proxy_hide_header x-amz-id-2;
        proxy_hide_header x-amz-request-id;
    }
}
```

### Option 2: Download and serve locally

```bash
#!/bin/bash
# Init script for nginx container
aws s3 sync s3://your-bucket/generated-files/ /usr/share/nginx/html/files/
nginx -g "daemon off;"
```

### Option 3: Use S3 static website hosting

Enable static website hosting on your S3 bucket and point your domain directly to S3.

</TroubleshootingItem>

<TroubleshootingItem id="alternative-solutions" summary="Alternative solutions within Kubernetes">

If you must keep everything within Kubernetes:

### Option 1: Same namespace

Move both cronjob and nginx to the same namespace to share volumes.

### Option 2: NFS or EFS

Use Amazon EFS (Elastic File System) which can be mounted across namespaces:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-pv
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-12345678.efs.us-west-2.amazonaws.com
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-pvc-cronjob
  namespace: jobs
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 100Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-pvc-nginx
  namespace: web
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 100Gi
```

**Pros:**

- True shared filesystem
- POSIX-compliant file operations
- Good performance for concurrent access

**Cons:**

- Additional AWS cost for EFS
- More complex setup
- Network dependency

### Option 3: Kubernetes Volumes with Copy Pattern

Use init containers to copy data between pods:

```yaml
# Nginx deployment with init container
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-with-data
  namespace: web
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      initContainers:
        - name: data-fetcher
          image: amazon/aws-cli:latest
          command:
            - sh
            - -c
            - |
              aws s3 sync s3://your-bucket/generated-files/ /shared-data/
          volumeMounts:
            - name: shared-volume
              mountPath: /shared-data
          env:
            - name: AWS_DEFAULT_REGION
              value: "us-west-2"
      containers:
        - name: nginx
          image: nginx:latest
          ports:
            - containerPort: 80
          volumeMounts:
            - name: shared-volume
              mountPath: /usr/share/nginx/html
              readOnly: true
      volumes:
        - name: shared-volume
          emptyDir: {}
```

</TroubleshootingItem>

<TroubleshootingItem id="performance-optimization" summary="Performance optimization for large files">

When dealing with large files (5GB+), consider these optimization strategies:

### 1. Streaming and Chunked Uploads

```python
import boto3
from botocore.config import Config
import os

# Optimized S3 client configuration
s3_config = Config(
    retries={'max_attempts': 10, 'mode': 'adaptive'},
    max_pool_connections=50
)

s3_client = boto3.client('s3', config=s3_config)

def upload_large_file(file_path, bucket, key):
    """Upload large file with multipart upload"""
    try:
        # Use multipart upload for files > 100MB
        file_size = os.path.getsize(file_path)
        if file_size > 100 * 1024 * 1024:  # 100MB
            print(f"Using multipart upload for {file_size / (1024*1024):.2f}MB file")

        s3_client.upload_file(
            file_path,
            bucket,
            key,
            Config=boto3.s3.transfer.TransferConfig(
                multipart_threshold=1024 * 25,  # 25MB
                max_concurrency=10,
                multipart_chunksize=1024 * 25,
                use_threads=True
            )
        )
        print(f"Successfully uploaded {key}")
        return True
    except Exception as e:
        print(f"Upload failed: {e}")
        return False

def download_large_file(bucket, key, file_path):
    """Download large file with optimization"""
    try:
        s3_client.download_file(
            bucket,
            key,
            file_path,
            Config=boto3.s3.transfer.TransferConfig(
                multipart_threshold=1024 * 25,
                max_concurrency=10,
                multipart_chunksize=1024 * 25,
                use_threads=True
            )
        )
        print(f"Successfully downloaded {key}")
        return True
    except Exception as e:
        print(f"Download failed: {e}")
        return False
```

### 2. Compression and Optimization

```bash
#!/bin/bash
# Script for cronjob to optimize file transfer

# Generate your files
generate_files.sh

# Compress files before upload
tar -czf generated-files-$(date +%Y%m%d).tar.gz /path/to/generated/files/

# Upload compressed file
aws s3 cp generated-files-$(date +%Y%m%d).tar.gz s3://your-bucket/compressed/

# Upload individual files for nginx access
aws s3 sync /path/to/generated/files/ s3://your-bucket/files/ --delete

# Cleanup local files
rm -rf /path/to/generated/files/*
rm generated-files-$(date +%Y%m%d).tar.gz
```

### 3. Nginx Configuration for S3 Proxy

Instead of downloading files, configure Nginx to proxy S3 requests:

```nginx
# nginx.conf
server {
    listen 80;
    server_name your-domain.com;

    location /files/ {
        # Proxy to S3 bucket
        proxy_pass https://your-bucket.s3.amazonaws.com/;
        proxy_set_header Host your-bucket.s3.amazonaws.com;
        proxy_set_header Authorization "";

        # Cache settings
        proxy_cache_valid 200 1h;
        proxy_cache_key $uri;

        # Hide S3 headers
        proxy_hide_header x-amz-id-2;
        proxy_hide_header x-amz-request-id;
        proxy_hide_header Set-Cookie;

        # Add custom headers
        add_header Cache-Control "public, max-age=3600";
    }

    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
}
```

</TroubleshootingItem>

<TroubleshootingItem id="monitoring-and-alerting" summary="Monitoring file sharing and transfers">

### CloudWatch Monitoring

Set up monitoring for your file sharing system:

```yaml
# CloudWatch alarm for S3 upload failures
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-config
data:
  cloudwatch-config.json: |
    {
      "alarms": [
        {
          "name": "S3-Upload-Errors",
          "metric": "4xxErrors",
          "namespace": "AWS/S3",
          "threshold": 5,
          "period": 300,
          "dimensions": {
            "BucketName": "your-bucket"
          }
        }
      ]
    }
```

### Application-Level Monitoring

```python
import time
import logging
from prometheus_client import Counter, Histogram, start_http_server

# Metrics
upload_counter = Counter('file_uploads_total', 'Total file uploads', ['status'])
upload_duration = Histogram('file_upload_duration_seconds', 'File upload duration')
download_counter = Counter('file_downloads_total', 'Total file downloads', ['status'])

def monitored_upload(file_path, bucket, key):
    start_time = time.time()
    try:
        result = upload_large_file(file_path, bucket, key)
        status = 'success' if result else 'failure'
        upload_counter.labels(status=status).inc()

        duration = time.time() - start_time
        upload_duration.observe(duration)

        logging.info(f"Upload {status}: {key} in {duration:.2f}s")
        return result
    except Exception as e:
        upload_counter.labels(status='error').inc()
        logging.error(f"Upload error: {e}")
        return False

# Start metrics server
start_http_server(8000)
```

### Log Aggregation

```yaml
# Fluent Bit configuration for log collection
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
data:
  fluent-bit.conf: |
    [INPUT]
        Name tail
        Path /var/log/cronjob/*.log
        Parser json
        Tag cronjob.*
        
    [INPUT]
        Name tail
        Path /var/log/nginx/*.log
        Parser nginx
        Tag nginx.*
        
    [OUTPUT]
        Name cloudwatch_logs
        Match *
        region us-west-2
        log_group_name /k8s/file-sharing
        auto_create_group true
```

</TroubleshootingItem>

<TroubleshootingItem id="troubleshooting-common-issues" summary="Troubleshooting common file sharing issues">

### Common Issues and Solutions

1. **S3 Upload Timeouts:**

   ```python
   import boto3
   from botocore.config import Config

   # Increase timeout values
   config = Config(
       read_timeout=900,  # 15 minutes
       connect_timeout=60,
       retries={'max_attempts': 10}
   )
   s3_client = boto3.client('s3', config=config)
   ```

2. **Permission Issues:**

   ```bash
   # Check IAM permissions
   aws sts get-caller-identity
   aws s3 ls s3://your-bucket/ --recursive

   # Test upload permission
   echo "test" | aws s3 cp - s3://your-bucket/test.txt
   ```

3. **Network Connectivity:**

   ```bash
   # Test from pod
   kubectl exec -it <cronjob-pod> -- nslookup s3.amazonaws.com
   kubectl exec -it <cronjob-pod> -- curl -I https://s3.amazonaws.com
   ```

4. **File Synchronization Issues:**

   ```python
   import hashlib
   import boto3

   def verify_file_integrity(local_file, bucket, s3_key):
       # Calculate local file hash
       with open(local_file, 'rb') as f:
           local_hash = hashlib.md5(f.read()).hexdigest()

       # Get S3 object ETag (MD5 for single-part uploads)
       s3 = boto3.client('s3')
       response = s3.head_object(Bucket=bucket, Key=s3_key)
       s3_etag = response['ETag'].strip('"')

       if local_hash == s3_etag:
           print("File integrity verified")
           return True
       else:
           print(f"Hash mismatch: local={local_hash}, s3={s3_etag}")
           return False
   ```

5. **Performance Issues:**

   ```bash
   # Monitor disk I/O
   kubectl top pods
   kubectl exec -it <pod> -- iostat -x 1

   # Check network throughput
   kubectl exec -it <pod> -- iperf3 -c s3.amazonaws.com -p 80
   ```

### Emergency Recovery Procedures

```bash
#!/bin/bash
# Emergency recovery script

BUCKET="your-bucket"
BACKUP_BUCKET="your-backup-bucket"

# Check if primary upload failed
if ! aws s3 ls s3://$BUCKET/latest/ > /dev/null 2>&1; then
    echo "Primary upload failed, initiating recovery..."

    # Attempt to restore from backup
    aws s3 sync s3://$BACKUP_BUCKET/latest/ s3://$BUCKET/latest/

    # Trigger nginx refresh
    kubectl rollout restart deployment/nginx -n web

    # Send alert
    curl -X POST "https://hooks.slack.com/..." \
         -H 'Content-type: application/json' \
         --data '{"text":"File sharing recovery completed"}'
fi
```

</TroubleshootingItem>

<TroubleshootingItem id="best-practices" summary="Best practices for cross-namespace file sharing">

### Security Best Practices

1. **Use least privilege IAM policies:**

   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Action": ["s3:GetObject", "s3:PutObject"],
         "Resource": "arn:aws:s3:::your-bucket/files/*"
       },
       {
         "Effect": "Allow",
         "Action": ["s3:ListBucket"],
         "Resource": "arn:aws:s3:::your-bucket",
         "Condition": {
           "StringLike": {
             "s3:prefix": ["files/*"]
           }
         }
       }
     ]
   }
   ```

2. **Enable S3 versioning and lifecycle policies:**

   ```json
   {
     "Rules": [
       {
         "ID": "FileRetention",
         "Status": "Enabled",
         "Filter": { "Prefix": "files/" },
         "Transitions": [
           {
             "Days": 30,
             "StorageClass": "STANDARD_IA"
           },
           {
             "Days": 90,
             "StorageClass": "GLACIER"
           }
         ],
         "Expiration": {
           "Days": 365
         }
       }
     ]
   }
   ```

3. **Implement proper error handling and retries:**

   ```python
   import time
   import random

   def retry_with_backoff(func, max_retries=5, base_delay=1):
       for attempt in range(max_retries):
           try:
               return func()
           except Exception as e:
               if attempt == max_retries - 1:
                   raise e

               delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
               print(f"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s")
               time.sleep(delay)
   ```

### Performance Best Practices

1. **Use appropriate instance types for file operations**
2. **Configure proper resource requests and limits**
3. **Implement caching strategies for frequently accessed files**
4. **Use S3 Transfer Acceleration for global deployments**
5. **Monitor and optimize network bandwidth usage**

### Operational Best Practices

1. **Implement comprehensive logging and monitoring**
2. **Set up automated backups and disaster recovery**
3. **Document file naming conventions and data retention policies**
4. **Regular testing of file sharing mechanisms**
5. **Maintain up-to-date documentation of the architecture**

</TroubleshootingItem>

---

_This FAQ was automatically generated on January 30, 2025 based on a real user query._
