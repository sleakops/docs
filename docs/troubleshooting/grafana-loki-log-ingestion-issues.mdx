---
sidebar_position: 3
title: "Grafana Loki Log Ingestion Issues"
description: "Troubleshooting missing logs and loki-write pod failures in Grafana"
date: "2024-12-19"
category: "dependency"
tags: ["grafana", "loki", "logging", "monitoring", "troubleshooting"]
---

import TroubleshootingItem from "@site/src/components/HomepageFeatures/troubleshootingitem";

# Grafana Loki Log Ingestion Issues

**Date:** December 19, 2024  
**Category:** Dependency  
**Tags:** Grafana, Loki, Logging, Monitoring, Troubleshooting

## Problem Description

**Context:** Users experience issues with Grafana's log visualization where the loki-write pod appears unable to write and store logs properly, resulting in missing log entries from application services.

**Observed Symptoms:**

- Missing initial logs from services in Grafana dashboard
- Log entries appear hours after the actual service start time
- loki-write pod experiencing write/storage failures
- Incomplete log timeline with gaps in log history
- Services monitored through Lens show logs that don't appear in Grafana

**Relevant Configuration:**

- Component: Grafana with Loki backend
- Affected pod: `loki-write`
- Log ingestion: Real-time application logs
- Monitoring setup: SleakOps integrated Grafana stack

**Error Conditions:**

- Logs missing from service startup period
- Delayed log appearance (hours after actual events)
- Inconsistent log ingestion across different services
- loki-write pod unable to persist log data

## Detailed Solution

<TroubleshootingItem id="initial-diagnosis" summary="Diagnosing Loki Write Issues">

To diagnose loki-write pod issues:

1. **Check pod status and logs:**

```bash
kubectl get pods -n monitoring | grep loki-write
kubectl logs -n monitoring loki-write-0 --tail=100
```

2. **Verify storage configuration:**

```bash
kubectl describe pvc -n monitoring | grep loki
```

3. **Check resource limits:**

```bash
kubectl describe pod -n monitoring loki-write-0
```

Common issues include:

- Insufficient storage space
- Memory/CPU resource constraints
- PVC mounting problems
- Network connectivity issues

</TroubleshootingItem>

<TroubleshootingItem id="storage-troubleshooting" summary="Resolving Storage Issues">

If storage is the root cause:

1. **Check available storage:**

```bash
kubectl exec -n monitoring loki-write-0 -- df -h
```

2. **Verify PVC status:**

```bash
kubectl get pvc -n monitoring
kubectl describe pvc loki-storage -n monitoring
```

3. **Increase storage if needed:**

```yaml
# In your Loki configuration
persistence:
  enabled: true
  size: 50Gi # Increase from default
  storageClass: gp3
```

4. **Clean up old logs if storage is full:**

```bash
# Access loki-write pod
kubectl exec -it -n monitoring loki-write-0 -- /bin/sh
# Check and clean old chunks
ls -la /loki/chunks/
```

</TroubleshootingItem>

<TroubleshootingItem id="resource-optimization" summary="Optimizing Loki Resources">

To prevent resource-related log ingestion issues:

1. **Increase memory limits:**

```yaml
# Loki write component resources
write:
  resources:
    requests:
      memory: 512Mi
      cpu: 100m
    limits:
      memory: 2Gi
      cpu: 500m
```

2. **Configure proper retention:**

```yaml
limits_config:
  retention_period: 168h # 7 days
  ingestion_rate_mb: 10
  ingestion_burst_size_mb: 20
```

3. **Optimize chunk configuration:**

```yaml
chunk_store_config:
  max_look_back_period: 168h
schema_config:
  configs:
    - from: 2023-01-01
      store: boltdb-shipper
      object_store: s3
      schema: v11
      index:
        prefix: loki_index_
        period: 24h
```

</TroubleshootingItem>

<TroubleshootingItem id="log-ingestion-verification" summary="Verifying Log Ingestion Pipeline">

To ensure logs are being properly ingested:

1. **Check Promtail configuration:**

```bash
kubectl logs -n monitoring promtail-daemonset-xxx
```

2. **Verify log shipping:**

```bash
# Check if logs are being sent to Loki
kubectl exec -n monitoring promtail-xxx -- wget -qO- http://localhost:3101/metrics | grep promtail_sent
```

3. **Test Loki API directly:**

```bash
# Query Loki for recent logs
kubectl port-forward -n monitoring svc/loki 3100:3100
curl -G -s "http://localhost:3100/loki/api/v1/query" --data-urlencode 'query={job="your-service"}' --data-urlencode 'start=1h'
```

4. **Check service discovery:**

```bash
# Verify Promtail is discovering your pods
kubectl exec -n monitoring promtail-xxx -- wget -qO- http://localhost:3101/targets
```

</TroubleshootingItem>

<TroubleshootingItem id="grafana-configuration" summary="Grafana Datasource Configuration">

Ensure Grafana is properly configured to query Loki:

1. **Verify Loki datasource:**

   - Go to Grafana → Configuration → Data Sources
   - Check Loki URL: `http://loki:3100`
   - Test connection

2. **Configure proper time ranges:**

   - In Grafana dashboards, ensure time range covers the expected period
   - Check timezone settings

3. **Optimize query performance:**

```logql
# Use efficient LogQL queries
{namespace="your-namespace", pod=~"your-service-.*"} |= "your-search-term"
```

4. **Set appropriate refresh intervals:**
   - For real-time monitoring: 5-10 seconds
   - For historical analysis: 1-5 minutes

</TroubleshootingItem>

<TroubleshootingItem id="monitoring-setup" summary="Setting Up Proper Monitoring">

To prevent future log ingestion issues:

1. **Monitor Loki metrics:**

```yaml
# Add alerts for Loki health
- alert: LokiWriteErrors
  expr: increase(loki_ingester_chunks_flushed_total{status="failed"}[5m]) > 0
  for: 2m
  labels:
    severity: warning
  annotations:
    summary: "Loki write errors detected"
```

2. **Set up storage monitoring:**

```yaml
- alert: LokiStorageFull
  expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim=~".*loki.*"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~".*loki.*"}) * 100 < 15
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "Loki storage is running low"
    description: "Loki storage has less than 15% space available"

- alert: LokiIngestionRate
  expr: rate(loki_distributor_lines_received_total[5m]) > 1000
  for: 2m
  labels:
    severity: info
  annotations:
    summary: "High log ingestion rate detected"
    description: "Loki is receiving logs at {{ $value }} lines/second"
```

3. **Monitor log pipeline health:**

```bash
# Create a dashboard to monitor:
# - Log ingestion rate
# - Storage usage
# - Write/read latency
# - Error rates
# - Memory/CPU usage
```

</TroubleshootingItem>

<TroubleshootingItem id="log-retention-management" summary="Managing Log Retention and Cleanup">

Configure proper log retention to prevent storage issues:

1. **Set up automatic log cleanup:**

```yaml
# Configure retention policies
compactor:
  working_directory: /loki/compactor
  shared_store: s3
  compaction_interval: 5m
  retention_enabled: true
  retention_delete_delay: 2h

limits_config:
  retention_period: 336h # 14 days
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
```

2. **Monitor retention effectiveness:**

```bash
# Check compactor logs
kubectl logs -n monitoring loki-compactor-0

# Verify retention is working
kubectl exec -n monitoring loki-read-0 -- ls -la /loki/chunks/
```

3. **Manual cleanup if needed:**

```bash
# Emergency cleanup procedure (use with caution)
kubectl exec -it -n monitoring loki-write-0 -- /bin/sh
# Remove old chunks manually if automatic cleanup fails
find /loki/chunks -type f -mtime +7 -delete
```

</TroubleshootingItem>

<TroubleshootingItem id="performance-optimization" summary="Performance optimization and scaling">

For high-volume log environments:

1. **Scale Loki components:**

```yaml
# Scale write components
write:
  replicas: 3
  resources:
    requests:
      memory: 1Gi
      cpu: 200m
    limits:
      memory: 4Gi
      cpu: 1000m

# Scale read components
read:
  replicas: 2
  resources:
    requests:
      memory: 512Mi
      cpu: 100m
    limits:
      memory: 2Gi
      cpu: 500m
```

2. **Optimize ingestion performance:**

```yaml
# Tune ingestion settings
ingester:
  chunk_block_size: 262144
  chunk_target_size: 1572864
  max_chunk_age: 2h
  chunk_encoding: snappy

  lifecycler:
    ring:
      kvstore:
        store: etcd
        etcd:
          endpoints:
            - etcd:2379
```

3. **Configure load balancing:**

```yaml
# Distribute load across ingesters
distributor:
  ring:
    kvstore:
      store: etcd
      etcd:
        endpoints:
          - etcd:2379
```

</TroubleshootingItem>

<TroubleshootingItem id="troubleshooting-checklist" summary="Complete troubleshooting checklist">

Use this checklist when logs are missing or delayed:

**Step 1: Basic Health Check**

```bash
□ Check all Loki pods are running
□ Verify no pods are in CrashLoopBackOff
□ Check pod resource usage (memory, CPU)
□ Verify storage availability
```

**Step 2: Configuration Verification**

```bash
□ Validate Loki configuration syntax
□ Check Promtail configuration for target discovery
□ Verify Grafana datasource configuration
□ Confirm log shipping endpoints
```

**Step 3: Network and Connectivity**

```bash
□ Test connectivity between Promtail and Loki
□ Verify service discovery is working
□ Check firewall/network policies
□ Test Loki API endpoints
```

**Step 4: Data Pipeline**

```bash
□ Verify logs are being generated by applications
□ Check Promtail is discovering log files/streams
□ Confirm logs are reaching Loki (check metrics)
□ Test querying logs directly via API
```

**Step 5: Resource and Performance**

```bash
□ Monitor ingestion rate vs capacity
□ Check for storage space issues
□ Verify retention policies are working
□ Review alert configurations
```

**Emergency Recovery Procedures:**

1. Restart loki-write pods if stuck
2. Clear storage space if full
3. Reset Promtail if not shipping logs
4. Recreate Grafana datasource if queries fail
5. Scale up resources if performance issues persist

</TroubleshootingItem>

---

_This FAQ was automatically generated on December 19, 2024 based on a real user query._
