---
sidebar_position: 3
title: "Grafana Loki Log Ingestion Issues"
description: "Troubleshooting missing logs and loki-write pod failures in Grafana"
date: "2024-12-19"
category: "dependency"
tags: ["grafana", "loki", "logging", "monitoring", "troubleshooting"]
---

import TroubleshootingItem from "@site/src/components/HomepageFeatures/troubleshootingitem";

# Grafana Loki Log Ingestion Issues

**Date:** December 19, 2024  
**Category:** Dependency  
**Tags:** Grafana, Loki, Logging, Monitoring, Troubleshooting

## Problem Description

**Context:** Users experience issues with Grafana's log visualization where the loki-write pod appears unable to write and store logs properly, resulting in missing log entries from application services.

**Observed Symptoms:**

- Missing initial logs from services in Grafana dashboard
- Log entries appear hours after the actual service start time
- loki-write pod experiencing write/storage failures
- Incomplete log timeline with gaps in log history
- Services monitored through Lens show logs that don't appear in Grafana

**Relevant Configuration:**

- Component: Grafana with Loki backend
- Affected pod: `loki-write`
- Log ingestion: Real-time application logs
- Monitoring setup: SleakOps integrated Grafana stack

**Error Conditions:**

- Logs missing from service startup period
- Delayed log appearance (hours after actual events)
- Inconsistent log ingestion across different services
- loki-write pod unable to persist log data

## Detailed Solution

<TroubleshootingItem id="initial-diagnosis" summary="Diagnosing Loki Write Issues">

To diagnose loki-write pod issues:

1. **Check pod status and logs:**

```bash
kubectl get pods -n monitoring | grep loki-write
kubectl logs -n monitoring loki-write-0 --tail=100
```

2. **Verify storage configuration:**

```bash
kubectl describe pvc -n monitoring | grep loki
```

3. **Check resource limits:**

```bash
kubectl describe pod -n monitoring loki-write-0
```

Common issues include:

- Insufficient storage space
- Memory/CPU resource constraints
- PVC mounting problems
- Network connectivity issues

</TroubleshootingItem>

<TroubleshootingItem id="storage-troubleshooting" summary="Resolving Storage Issues">

If storage is the root cause:

1. **Check available storage:**

```bash
kubectl exec -n monitoring loki-write-0 -- df -h
```

2. **Verify PVC status:**

```bash
kubectl get pvc -n monitoring
kubectl describe pvc loki-storage -n monitoring
```

3. **Increase storage if needed:**

```yaml
# In your Loki configuration
persistence:
  enabled: true
  size: 50Gi # Increase from default
  storageClass: gp3
```

4. **Clean up old logs if storage is full:**

```bash
# Access loki-write pod
kubectl exec -it -n monitoring loki-write-0 -- /bin/sh
# Check and clean old chunks
ls -la /loki/chunks/
```

</TroubleshootingItem>

<TroubleshootingItem id="resource-optimization" summary="Optimizing Loki Resources">

To prevent resource-related log ingestion issues:

1. **Increase memory limits:**

```yaml
# Loki write component resources
write:
  resources:
    requests:
      memory: 512Mi
      cpu: 100m
    limits:
      memory: 2Gi
      cpu: 500m
```

2. **Configure proper retention:**

```yaml
limits_config:
  retention_period: 168h # 7 days
  ingestion_rate_mb: 10
  ingestion_burst_size_mb: 20
```

3. **Optimize chunk configuration:**

```yaml
chunk_store_config:
  max_look_back_period: 168h
schema_config:
  configs:
    - from: 2023-01-01
      store: boltdb-shipper
      object_store: s3
      schema: v11
      index:
        prefix: loki_index_
        period: 24h
```

</TroubleshootingItem>

<TroubleshootingItem id="log-ingestion-verification" summary="Verifying Log Ingestion Pipeline">

To ensure logs are being properly ingested:

1. **Check Promtail configuration:**

```bash
kubectl logs -n monitoring promtail-daemonset-xxx
```

2. **Verify log shipping:**

```bash
# Check if logs are being sent to Loki
kubectl exec -n monitoring promtail-xxx -- wget -qO- http://localhost:3101/metrics | grep promtail_sent
```

3. **Test Loki API directly:**

```bash
# Query Loki for recent logs
kubectl port-forward -n monitoring svc/loki 3100:3100
curl -G -s "http://localhost:3100/loki/api/v1/query" --data-urlencode 'query={job="your-service"}' --data-urlencode 'start=1h'
```

4. **Check service discovery:**

```bash
# Verify Promtail is discovering your pods
kubectl exec -n monitoring promtail-xxx -- wget -qO- http://localhost:3101/targets
```

</TroubleshootingItem>

<TroubleshootingItem id="grafana-configuration" summary="Grafana Datasource Configuration">

Ensure Grafana is properly configured to query Loki:

1. **Verify Loki datasource:**

   - Go to Grafana → Configuration → Data Sources
   - Check Loki URL: `http://loki:3100`
   - Test connection

2. **Configure proper time ranges:**

   - In Grafana dashboards, ensure time range covers the expected period
   - Check timezone settings

3. **Optimize query performance:**

```logql
# Use efficient LogQL queries
{namespace="your-namespace", pod=~"your-service-.*"} |= "your-search-term"
```

4. **Set appropriate refresh intervals:**
   - For real-time monitoring: 5-10 seconds
   - For historical analysis: 1-5 minutes

</TroubleshootingItem>

<TroubleshootingItem id="monitoring-setup" summary="Setting Up Proper Monitoring">

To prevent future log ingestion issues:

1. **Monitor Loki metrics:**

```yaml
# Add alerts for Loki health
- alert: LokiWriteErrors
  expr: increase(loki_ingester_chunks_flushed_total{status="failed"}[5m]) > 0
  for: 2m
  labels:
    severity: warning
  annotations:
    summary: "Loki write errors detected"
```

2. **Set up storage monitoring:**

```yaml
- alert: LokiStorageFull
  expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim=~".*loki.*"} / kubelet_volume_stats_capacity_bytes{
```
