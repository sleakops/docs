---
sidebar_position: 3
title: "Doppler Integration Issues in SleakOps"
description: "Troubleshooting Doppler configuration and environment variable synchronization issues"
date: "2024-12-19"
category: "dependency"
tags:
  ["doppler", "environment-variables", "secrets", "configuration", "deployment"]
---

import TroubleshootingItem from "@site/src/components/HomepageFeatures/troubleshootingitem";

# Doppler Integration Issues in SleakOps

**Date:** December 19, 2024  
**Category:** Dependency  
**Tags:** Doppler, Environment Variables, Secrets, Configuration, Deployment

## Problem Description

**Context:** Users integrating Doppler as an external service for environment variable management in SleakOps deployments experience issues where variables are not being updated during deployments, despite successful configuration.

**Observed Symptoms:**

- Environment variables not updating during recent deployments
- Doppler service configured but variables remain stale
- Deployments complete successfully but use outdated configuration
- Rolling update strategy shows temporary replica count increase

**Relevant Configuration:**

- External service: Doppler for environment variable management
- Deployment strategy: Rolling Update
- Docker args: `DOPPLER_CONFIG = staging`
- Project secrets and Doppler references configured in SleakOps

**Error Conditions:**

- Variables not refreshing on new deployments
- Occurs specifically with Doppler-managed environment variables
- Problem persists across multiple deployment attempts
- Local secrets may still work while Doppler integration fails

## Detailed Solution

<TroubleshootingItem id="doppler-config-verification" summary="Verify Doppler Configuration in SleakOps">

First, verify your Doppler configuration is correctly set up in SleakOps:

1. **Check Docker Arguments:**

   ```yaml
   # In your SleakOps project configuration
   docker_args:
     DOPPLER_CONFIG: "staging" # or your environment name
     DOPPLER_TOKEN: "${DOPPLER_TOKEN}" # should reference secret
   ```

2. **Verify Doppler Token Secret:**

   - Go to **Project Settings** → **Secrets**
   - Ensure `DOPPLER_TOKEN` is properly configured
   - Token should have read access to the specified config

3. **Check Doppler Config Reference:**
   - Verify the config name matches exactly in Doppler dashboard
   - Common configs: `dev`, `staging`, `production`

</TroubleshootingItem>

<TroubleshootingItem id="doppler-token-validation" summary="Validate Doppler Token and Permissions">

Ensure your Doppler token has the correct permissions:

1. **Test Token Locally:**

   ```bash
   # Test if token can access the config
   curl -H "Authorization: Bearer YOUR_DOPPLER_TOKEN" \
        "https://api.doppler.com/v3/configs/config/secrets" \
        -G -d project=YOUR_PROJECT -d config=staging
   ```

2. **Check Token Scope:**

   - Token must have `read` access to the specific config
   - Verify the token hasn't expired
   - Ensure it's a **Service Token**, not a **Personal Token**

3. **Regenerate Token if Needed:**
   - Go to Doppler Dashboard → **Access** → **Service Tokens**
   - Create new token with appropriate scope
   - Update the secret in SleakOps

</TroubleshootingItem>

<TroubleshootingItem id="deployment-refresh-strategy" summary="Force Environment Variable Refresh">

To ensure environment variables are refreshed during deployment:

1. **Trigger Complete Redeployment:**

   ```bash
   # Force restart all pods to pick up new variables
   kubectl rollout restart deployment/your-app-name
   ```

2. **Check Pod Environment Variables:**

   ```bash
   # Verify variables are loaded correctly
   kubectl exec -it pod/your-pod-name -- env | grep YOUR_VAR
   ```

3. **Use Deployment Annotations:**
   Add a timestamp annotation to force pod recreation:
   ```yaml
   # This forces Kubernetes to recreate pods
   spec:
     template:
       metadata:
         annotations:
           deployment.kubernetes.io/revision: "$(date +%s)"
   ```

</TroubleshootingItem>

<TroubleshootingItem id="doppler-sync-troubleshooting" summary="Troubleshoot Doppler Synchronization">

If variables still aren't updating:

1. **Check Doppler Logs:**

   ```bash
   # Check if Doppler CLI is working in your container
   kubectl logs deployment/your-app -c your-container | grep -i doppler
   ```

2. **Verify Doppler CLI Installation:**

   ```dockerfile
   # Ensure Doppler CLI is installed in your Docker image
   RUN curl -Ls https://cli.doppler.com/install.sh | sh

   # Use Doppler to run your application
   CMD ["doppler", "run", "--", "your-app-command"]
   ```

3. **Test Manual Sync:**

   ```bash
   # Inside your container, test manual sync
   doppler secrets download --no-file --format env
   ```

4. **Check Network Connectivity:**
   - Ensure your cluster can reach `api.doppler.com`
   - Verify no firewall rules block the connection
   - Test DNS resolution: `nslookup api.doppler.com`

</TroubleshootingItem>

<TroubleshootingItem id="alternative-approaches" summary="Alternative Configuration Approaches">

If Doppler integration continues to fail:

1. **Use Kubernetes Secrets as Backup:**

   ```yaml
   # Create a Kubernetes secret with critical variables
   apiVersion: v1
   kind: Secret
   metadata:
     name: app-secrets
   data:
     DATABASE_URL: <base64-encoded-value>
   ```

2. **Implement Doppler Webhook:**

   - Set up Doppler webhooks to trigger redeployments
   - Automatically update secrets when Doppler config changes

3. **Use Init Container Pattern:**

   ```yaml
   # Fetch secrets before main container starts
   initContainers:
     - name: doppler-sync
       image: dopplerhq/cli:latest
       command:
         ["doppler", "secrets", "download", "--format", "env", "--no-file"]
       volumeMounts:
         - name: secrets-volume
           mountPath: /secrets
   ```

4. **Hybrid Approach:**
   - Use SleakOps secrets for critical variables
   - Use Doppler for non-critical configuration
   - Implement fallback mechanisms in your application

</TroubleshootingItem>

<TroubleshootingItem id="rolling-update-explanation" summary="Understanding Rolling Update Behavior">

The temporary increase in replica count is normal during deployments:

1. **Rolling Update Process:**

   - Kubernetes creates new pods with updated configuration
   - Keeps old pods running until new ones are ready
   - Gradually shifts traffic to new pods
   - Terminates old pods once new ones are healthy

2. **Expected Behavior:**

   - Temporary replica count: `desired + maxSurge`
   - For 2 replicas with default settings: up to 3 pods temporarily
   - Returns to desired count (2) after deployment completes

3. **Monitor Deployment Progress:**
   ```bash
   kubectl rollout status deployment
   ```
