---
sidebar_position: 3
title: "Deployment Issue - Old Pods Not Terminating"
description: "Solution for when new deployments create pods but old pods remain active and receive traffic"
date: "2024-06-10"
category: "workload"
tags: ["deployment", "kubernetes", "pods", "traffic-routing", "troubleshooting"]
---

import TroubleshootingItem from "@site/src/components/HomepageFeatures/troubleshootingitem";

# Deployment Issue - Old Pods Not Terminating

**Date:** June 10, 2024  
**Category:** Workload  
**Tags:** Deployment, Kubernetes, Pods, Traffic Routing, Troubleshooting

## Problem Description

**Context:** User experiences issues when deploying new builds where new pods are created successfully but old pods from previous deployments (created 2 days ago) remain active and continue receiving traffic instead of the new pods.

**Observed Symptoms:**

- New pod is created with the latest image tag
- Old pods (2+ days old) are not being terminated
- All traffic continues to be routed to old pods instead of new ones
- New pod works correctly when accessed directly via port-forward
- Health checks on new pod are passing
- Attempting to manually delete old pods results in them restarting

**Relevant Configuration:**

- Project: `rattlesnake`
- Environment: `development`
- Image Tag: `7edb7463a22198fc4d79bac76cfcb2c0f94b3755`
- Platform: Kubernetes (viewed through Lens)

**Error Conditions:**

- Issue occurs during deployment process
- Old pods restart when manually deleted
- Traffic routing does not switch to new pods
- Problem has occurred previously with other projects

## Detailed Solution

<TroubleshootingItem id="initial-diagnosis" summary="Understanding the root cause">

This issue typically occurs due to one of these Kubernetes deployment problems:

1. **Deployment strategy misconfiguration**: Rolling update settings may be preventing old pods from terminating
2. **Resource constraints**: Insufficient resources preventing new pods from becoming ready
3. **Service selector mismatch**: Service is not pointing to the new pods
4. **Readiness probe failures**: New pods may not be passing readiness checks
5. **Multiple deployment controllers**: Conflicting controllers managing the same pods

</TroubleshootingItem>

<TroubleshootingItem id="check-deployment-status" summary="Check deployment and pod status">

First, verify the current state of your deployment:

```bash
# Check deployment status
kubectl get deployments -n <namespace>
kubectl describe deployment <deployment-name> -n <namespace>

# Check pods and their ages
kubectl get pods -n <namespace> --show-labels
kubectl get pods -n <namespace> -o wide

# Check replica sets
kubectl get replicasets -n <namespace>
```

Look for:

- Multiple replica sets with pods
- Pod readiness status
- Deployment rollout status

</TroubleshootingItem>

<TroubleshootingItem id="verify-service-endpoints" summary="Verify service endpoints and traffic routing">

Check if the service is correctly pointing to the new pods:

```bash
# Check service endpoints
kubectl get endpoints <service-name> -n <namespace>
kubectl describe service <service-name> -n <namespace>

# Compare service selector with pod labels
kubectl get service <service-name> -n <namespace> -o yaml
kubectl get pods -n <namespace> --show-labels
```

The service selector should match the labels on your new pods, not the old ones.

</TroubleshootingItem>

<TroubleshootingItem id="check-readiness-probes" summary="Verify readiness and liveness probes">

Even though port-forward works, check if readiness probes are configured correctly:

```bash
# Check pod readiness status
kubectl describe pod <new-pod-name> -n <namespace>

# Look for readiness probe configuration
kubectl get pod <new-pod-name> -n <namespace> -o yaml | grep -A 10 readinessProbe
```

If readiness probes are failing, the service won't route traffic to the new pods.

</TroubleshootingItem>

<TroubleshootingItem id="force-deployment-rollout" summary="Force a proper deployment rollout">

If the deployment is stuck
