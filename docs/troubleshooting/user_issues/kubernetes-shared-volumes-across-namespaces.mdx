---
sidebar_position: 3
title: "Kubernetes Shared Volumes Across Namespaces"
description: "Solution for sharing volumes between pods in different namespaces and alternative approaches"
date: "2025-01-30"
category: "cluster"
tags: ["kubernetes", "volumes", "namespaces", "storage", "s3"]
---

import TroubleshootingItem from "@site/src/components/HomepageFeatures/troubleshootingitem";

# Kubernetes Shared Volumes Across Namespaces

**Date:** January 30, 2025  
**Category:** Cluster  
**Tags:** Kubernetes, Volumes, Namespaces, Storage, S3

## Problem Description

**Context:** User needs to share data between a cronjob that generates files and an nginx service that serves those files, but they are running in different namespaces in a Kubernetes cluster.

**Observed Symptoms:**

- Cannot mount the same volume across different namespaces
- Need to share generated files between cronjob and web server
- Current setup works on EC2 with shared volume mount
- Looking for Kubernetes equivalent of shared volume access

**Relevant Configuration:**

- Platform: SleakOps on AWS EKS
- Use case: Cronjob generates files, Nginx serves them
- Current setup: EC2 with shared volume between containers
- Target: Kubernetes pods in different namespaces

**Error Conditions:**

- Kubernetes limitation: same volume cannot be used across namespaces
- Need alternative solution for file sharing
- Performance requirements for large file generation (5GB+)

## Detailed Solution

<TroubleshootingItem id="kubernetes-volume-limitation" summary="Understanding Kubernetes volume limitations">

Kubernetes has a fundamental limitation: **the same PersistentVolume cannot be mounted by pods in different namespaces**. This is by design for security and isolation purposes.

This means your current EC2 approach of sharing a volume between containers won't work directly in Kubernetes when pods are in different namespaces.

</TroubleshootingItem>

<TroubleshootingItem id="s3-solution" summary="Recommended solution: Use S3 as shared storage">

The recommended approach is to use **Amazon S3** as intermediate storage:

### Architecture:

1. **Cronjob**: Generates files locally → Uploads to S3
2. **Nginx service**: Downloads files from S3 → Serves them

### Benefits:

- Works across namespaces
- Scalable and reliable
- Cost-effective for large files
- Built-in authentication via SleakOps service accounts

### Configuration example:

```yaml
# Cronjob configuration
apiVersion: batch/v1
kind: CronJob
metadata:
  name: file-generator
  namespace: jobs
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: generator
              image: your-app:latest
              env:
                - name: S3_BUCKET
                  value: "your-bucket-name"
                - name: AWS_REGION
                  value: "us-east-1"
              volumeMounts:
                - name: temp-storage
                  mountPath: /tmp/files
          volumes:
            - name: temp-storage
              emptyDir:
                sizeLimit: 60Gi
```

</TroubleshootingItem>

<TroubleshootingItem id="nodepool-configuration" summary="Configure nodepool for adequate storage">

For large file generation, configure your nodepool with sufficient EBS storage:

### In SleakOps:

1. Go to **Cluster Configuration**
2. Select your **Nodepool**
3. Modify **Node Configuration**:
   - **EBS Volume Size**: 50-60 GB
   - **Volume Type**: gp3 (faster and cheaper)

### Configuration example:

```yaml
nodepool_config:
  instance_type: "t3.medium"
  disk_size: 60 # GB
  disk_type: "gp3"
  min_nodes: 1
  max_nodes: 5
```

</TroubleshootingItem>

<TroubleshootingItem id="s3-authentication" summary="S3 authentication in SleakOps">

SleakOps automatically configures S3 authentication via **service accounts**. You don't need to manage AWS credentials manually.

### Java example for S3 access:

```java
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.model.*;

public class S3FileUploader {
    public static void main(String[] args) {
        // SleakOps handles authentication automatically
        S3Client s3 = S3Client.builder()
                               .region(Region.US_EAST_1)
                               .build();

        // Upload file to S3
        PutObjectRequest putRequest = PutObjectRequest.builder()
                .bucket("your-bucket-name")
                .key("generated-files/data.zip")
                .build();

        s3.putObject(putRequest,
                    RequestBody.fromFile(new File("/tmp/files/data.zip")));
    }
}
```

### Python example:

```python
import boto3
import os

# SleakOps handles authentication via service account
s3_client = boto3.client('s3')
bucket_name = os.environ['S3_BUCKET']

# Upload generated file
s3_client.upload_file(
    '/tmp/files/generated_data.zip',
    bucket_name,
    'generated-files/generated_data.zip'
)
```

</TroubleshootingItem>

<TroubleshootingItem id="nginx-s3-integration" summary="Configure Nginx to serve files from S3">

For serving files from S3 through Nginx, you have several options:

### Option 1: Nginx with S3 proxy

```nginx
server {
    listen 80;
    server_name your-domain.com;

    location /files/ {
        proxy_pass https://your-bucket.s3.amazonaws.com/;
        proxy_set_header Host your-bucket.s3.amazonaws.com;
        proxy_hide_header x-amz-id-2;
        proxy_hide_header x-amz-request-id;
    }
}
```

### Option 2: Download and serve locally

```bash
#!/bin/bash
# Init script for nginx container
aws s3 sync s3://your-bucket/generated-files/ /usr/share/nginx/html/files/
nginx -g "daemon off;"
```

### Option 3: Use S3 static website hosting

Enable static website hosting on your S3 bucket and point your domain directly to S3.

</TroubleshootingItem>

<TroubleshootingItem id="alternative-solutions" summary="Alternative solutions within Kubernetes">

If you must keep everything within Kubernetes:

### Option 1: Same namespace

Move both cronjob and nginx to the same namespace to share volumes.

### Option 2: NFS or EFS

Use Amazon EFS (Elastic File System) which can be mounted across namespaces:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-pv
spec:
  capacity:
    storage: 100Gi
  accessModes:
```
