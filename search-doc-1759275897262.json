{"searchDocs":[{"title":"Basic Concepts","type":0,"sectionRef":"#","url":"/basicconcepts","content":"","keywords":"","version":"Next"},{"title":"Provider‚Äã","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#provider","content":" As the first step to start an infrastructure is to decide which cloud provider to use (AWS, Azure or GCP, etc.), in SleakOps a Provider represents the selection of one of this cloud providers, the credentials granted to SleakOps to it and the set of accounts created in order to properly manage the infrastructure. It is composed by an Organizative Unit on AWS and its accounts.  ","version":"Next","tagName":"h3"},{"title":"Cluster‚Äã","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#cluster","content":" A Kubernetes cluster is a distributed system for managing containerized applications. It consists of nodes (physical or virtual machines) running pods (groups of one or more containers). A central control plane, composed of various software components, coordinates the activity of the nodes and manages the lifecycle of the pods.  ","version":"Next","tagName":"h3"},{"title":"Environment‚Äã","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#environment","content":" In computing, an environment or namespace typically refers to an isolated area where specific resources, applications, or Workloads operate independently. This isolation enhances organization, security, and resource management within larger systems or platforms.  ","version":"Next","tagName":"h3"},{"title":"Project‚Äã","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#project","content":" A project is a collection of files and code managed using Git, representing a codebase within a git repository.  ","version":"Next","tagName":"h3"},{"title":"Workload‚Äã","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#workload","content":" A workload is a fundamental unit of functionality that can be independently deployed and managed within an environment. Workloads perform specific tasks or processes, interacting with other services via defined interfaces. They are scalable and modular, forming the building blocks of architectures like microservices and service-oriented architectures (SOA), allowing for flexible and efficient system development.  ","version":"Next","tagName":"h3"},{"title":"Dependency‚Äã","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#dependency","content":" A dependency is an external resource or service that an application requires to function properly in a cloud environment. These dependencies include various infrastructure components such as relational databases, storage services, and caches. Each dependency is associated with provider-specific services, ensuring seamless integration and operation within the cloud platform.  ","version":"Next","tagName":"h3"},{"title":"Var Group‚Äã","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#var-group","content":" In Sleakops, a variable group (or var group) is a set of key-value pairs, similar to a dictionary, that provides configuration settings for workloads within a specific project and environment.  ","version":"Next","tagName":"h3"},{"title":"Build‚Äã","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#build","content":" A build is the process of creating a new version of your application's code as a container image from a Dockerfile, incorporating the compiled code and necessary dependencies.  ","version":"Next","tagName":"h3"},{"title":"Deploy‚Äã","type":1,"pageTitle":"Basic Concepts","url":"/basicconcepts#deploy","content":" In technology, to deploy means to release a software application, service, or update to a production environment, making it available for use by end-users. ","version":"Next","tagName":"h3"},{"title":"Access your Cluster","type":0,"sectionRef":"#","url":"/cluster/access-cluster","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#faqs","content":" What is Kubeconfig?‚Äã Kubeconfig is a configuration file used by the Kubernetes command-line tool kubectl to interact with Kubernetes clusters. It contains information about the clusters, users, contexts, and namespaces that kubectl uses to communicate with one or more Kubernetes clusters.  What is an IDE?‚Äã An IDE for Kubernetes is a software environment that provides tools and features specifically designed to help developers create, manage, and deploy applications on Kubernetes clusters, integrating Kubernetes commands, resource management, and YAML/Helm chart editing within the development workflow. Lens is an open-source IDE for Kubernetes, providing a user-friendly GUI to manage, monitor, and troubleshoot multiple clusters in real-time.  ","version":"Next","tagName":"h2"},{"title":"How to access your cluster?‚Äã","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#how-to-access-your-cluster","content":" ","version":"Next","tagName":"h2"},{"title":"1. Go to the Access Cluster setting‚Äã","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#1-go-to-the-access-cluster-setting","content":" Click on Clusters, select one and access its settings.  Go to the Access Cluster option.    ","version":"Next","tagName":"h3"},{"title":"2. Install the following Dependencies‚Äã","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#2-install-the-following-dependencies","content":" AWS Cli: AWS DocsKubeCtl: Kubernetes DocsLens: K8SLens DocsPritunl VPN Client: Pritunl Page  ","version":"Next","tagName":"h3"},{"title":"3. Set up a VPN‚Äã","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#3-set-up-a-vpn","content":" Open the Pritunl VPN ClientGenerate a VPN URi, copy it and set it up on the client.    ","version":"Next","tagName":"h3"},{"title":"4. Generate your AWS Keys and generate the kubeconfig file‚Äã","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#4-generate-your-aws-keys-and-generate-the-kubeconfig-file","content":" Login into AWS with your username.Then, go to AWS Access Key Wizard to generate the keys on AWS.Paste the keys in the form and generate the kubeconfig file.Copy the output.    ","version":"Next","tagName":"h3"},{"title":"5. Add it to Lens‚Äã","type":1,"pageTitle":"Access your Cluster","url":"/cluster/access-cluster#5-add-it-to-lens","content":" Open Lens, locate the 'Import Kubeconfig' option, and import the YAML file obtained from the Access Cluster section.   ","version":"Next","tagName":"h3"},{"title":"CLI","type":0,"sectionRef":"#","url":"/cli","content":"","keywords":"","version":"Next"},{"title":"Streamline CI/CD with SleakOps CLI‚Äã","type":1,"pageTitle":"CLI","url":"/cli#streamline-cicd-with-sleakops-cli","content":" SleakOps CLI is a Python package designed to simplify your CI/CD workflows. With just two straightforward subcommands, you can effortlessly create builds and deploy your applications, ensuring a smooth and efficient development process. To get started, simply install SleakOps using pip:  pip install sleakops   ","version":"Next","tagName":"h2"},{"title":"1. Authentication‚Äã","type":1,"pageTitle":"CLI","url":"/cli#1-authentication","content":" To authenticate with the SleakOps CLI, you need an API_KEY. You can obtain this key from the console by clicking on Generate API-Key. Each company is allowed to have only one active API_KEY at a time. If you request a new API_KEY, the old one will be automatically revoked. The page shows the company keys and who generated them.  Once you have your API_KEY, you can use it as an argument when running SleakOps commands or set it as an environment variable named SLEAKOPS_KEY.  ","version":"Next","tagName":"h3"},{"title":"2. Create a Build‚Äã","type":1,"pageTitle":"CLI","url":"/cli#2-create-a-build","content":" To create a build for your application, use the following command:  sleakops build [options]   This command initiates the build process, and SleakOps takes care of compiling your code, running tests, and packaging the application for deployment. You can specify additional options to tailor the build process to your specific needs.  There are two required arguments project and branch, which are used to know what to build. Besides, you might add a commit to build a previous commit, a tag for the image, and the provider if you need to specify it.  As previously mentioned the key might be an input here or a environment variable.  Also, you might mark if you want the process to wait the build to be finished or not.  ","version":"Next","tagName":"h3"},{"title":"3. Make a Deploy‚Äã","type":1,"pageTitle":"CLI","url":"/cli#3-make-a-deploy","content":" Once your build is ready, you can effortlessly deploy your application using the following command:  sleakops deploy [options]   SleakOps seamlessly handles the deployment process, ensuring that your application is up and running in no time. You can specify deployment options to fine-tune the process according to your requirements.  Here project and environment are the required arguments. User might add a build or tag image to specify an image. Here the wait and key options are present to, the usage is the same as in the build command.  ","version":"Next","tagName":"h3"},{"title":"CI/CD Examples‚Äã","type":1,"pageTitle":"CLI","url":"/cli#cicd-examples","content":" With SleakOps CLI, you can integrate your CI/CD pipelines, automate the build and deployment process, and focus on delivering exceptional applications without the hassle of manual intervention. Enjoy a seamless development experience with SleakOps, and make your custom CI/CD workflows.  GitHubGitLabBitBucket name: Deploy on: push: branches: - main jobs: build: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v2 - name: Install SleakOps CLI run: pip install sleakops - name: Run SleakOps build env: SLEAKOPS_KEY: ${{ secrets.SLEAKOPS_KEY }} run: sleakops build -p core -b main -w deploy: needs: [build] runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v2 - name: Install SleakOps CLI run: pip install sleakops - name: Run SleakOps deploy env: SLEAKOPS_KEY: ${{ secrets.SLEAKOPS_KEY }} run: sleakops deploy -p core -e main -w  ","version":"Next","tagName":"h2"},{"title":"Cluster","type":0,"sectionRef":"#","url":"/cluster","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Cluster","url":"/cluster#faqs","content":" What are the benefits of having a cluster?‚Äã Clusters allow you to deploy multiple instances of the same application effortlessly, ensuring that if one instance fails, others can instantly take over. Additionally, clusters offer auto-scaling capabilities, automatically adjusting the number of worker nodes as traffic fluctuates, which ensures high availability and optimal performance. They also provide an excellent way to isolate your production environment from your staging environment.  How SleakOps manage clusters?‚Äã SleakOps offers you the flexibility and control needed to create an EKS cluster that aligns precisely with your requirements. To define how to create your infra, you can see: Designing your Infra: Single Schema Vs. Multi Schema   What happens when a Cluster is created on SleakOps?‚Äã SleakOps creates a group of Node Pools that will allow you to run your applications using Kubernetes orchestration based on your needs. See Node Pools.  What is Karpenter?‚Äã In the process of creating your EKS cluster, node provisioning is seamlessly managed by advanced Karpenter technology. Karpenter automatically provisions nodes with just the required resources and scales the cluster based on application demands, removing concerns about deprovisioning. As your workload changes, Karpenter adjusts nodes and resources dynamically, optimizing performance and cost. See Karpenter Web  How do I choose the right instance types for my cluster?‚Äã Choosing the right instance types depends on your application requirements, cost optimization goals, and availability needs. SleakOps provides different node pools (Spot, On-Demand, Reserved) to help you optimize costs while maintaining performance. For detailed guidance on instance types and cluster sizing, see Instance Types and Node Management.  How does SleakOps handle cluster updates and upgrades?‚Äã We notify users about new version coming in approximatively 1 month in advance.Upgrade clusters for a group of selected customers.Upgrade all non-production flagged clusters.Upgrade all clusters.  How do I control cluster's expenses?‚Äã SleakOps allows you to see all your clusters expenses in one place, classified by account, resources, dates. Access Clusters, select one and click the button: $  How do I monitor my cluster?‚Äã You can monitor your cluster's activity by accessing Clusters, and clicking into the button:  ","version":"Next","tagName":"h2"},{"title":"Lets create your first Cluster on SleakOps‚Äã","type":1,"pageTitle":"Cluster","url":"/cluster#lets-create-your-first-cluster-on-sleakops","content":" warning Creating a cluster incurs costs on your AWS account, as the EKS service includes a fixed fee plus variable costs based on resource consumption.  ","version":"Next","tagName":"h2"},{"title":"1. Select the account under the cluster‚Äôll be created.‚Äã","type":1,"pageTitle":"Cluster","url":"/cluster#1-select-the-account-under-the-clusterll-be-created","content":" For more information regarding accounts, see Accounts.  On the left pane, you will see a selector with the account names, select the one you'll use based on how you decide to manage your clusters and environments. We suggest to move forward with a Multiple Schema configuration, that aligns with the best practices. To follow this schema select the development account to create the cluster for your staging environments, and the production account for the production cluster. Check Designing your Infra: Single Schema Vs. Multi Schema for more information.  ","version":"Next","tagName":"h3"},{"title":"2. Navigate to create Cluster section‚Äã","type":1,"pageTitle":"Cluster","url":"/cluster#2-navigate-to-create-cluster-section","content":" Into the Left Pane, access Clusters option and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"3. Set up your Cluster‚Äã","type":1,"pageTitle":"Cluster","url":"/cluster#3-set-up-your-cluster","content":" With your Account selected, you will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Setting\tDescriptionName\tIdentify your cluster Description\tOptional space to describe what is included into this cluster. Architecture Type\tSelect the architecture type to be used during the creation for your instances: (64-Bit) ARM or (64-Bit) X86, based on your performance and compatibility needs. Then you'll be able of creating new instances using a different architecture. Production\tHigh-Availability (HA) Configuration: When enabled, this setting transforms your cluster into a production-ready environment by distributing the EKS cluster across multiple Availability Zones (AZs). This provides redundancy and fault tolerance, ensuring uninterrupted operations even if one AZ experiences issues. Production clusters also benefit from enhanced monitoring, automated backups, and optimized resource allocation for critical workloads.  Once you've completed the form, click on Submit in order to trigger the cluster creation into the selected account on AWS.  This process will create a Cluster with five node pools by default. See Node Pools. ","version":"Next","tagName":"h3"},{"title":"Addons","type":0,"sectionRef":"#","url":"/cluster/addons","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Addons","url":"/cluster/addons#faqs","content":" Which are the essential add-ons?‚Äã By default SleakOps includes in your infra: Metric Server: SleakOps installs the Metric Server to collect cluster and node-level metrics, enabling performance monitoring and informed scaling decisions.External-DNS: SleakOps deploys External-DNS for automatic DNS record management, ensuring seamless connectivity with user-friendly domain names.Automatic Load Balancer: SleakOps provisions load balancers automatically, efficiently distributing traffic and maintaining high availability.Karpenter Deployment: SleakOps deploys Karpenter for intelligent node provisioning, scaling your cluster based on actual resource needs to optimize performance.  Which optional Add-ons are available?‚Äã Grafana: Visualize and analyze data with Grafana's dashboards, making it easier to monitor system performance and troubleshoot issues. Perfect for tracking application memory and CPU usage.LOKI: Use Loki for cost-effective log aggregation. It simplifies log management by labeling log streams without indexing content, making it ideal for browsing and monitoring application logs.Kubecost: Gain real-time insights into Kubernetes cloud costs with Kubecost. This add-on helps you monitor and reduce expenses across projects in your cluster.Prometheus: SleakOps deploys Prometheus for monitoring and alerting, providing detailed insights into cluster performance and resource utilization.OTEL: Use OpenTelemetry to collect and analyze distributed traces, enabling you to monitor and optimize application performance across your cluster.EFS Controller: The EFS Controller allows you to manage EFS volumes within your EKS cluster, providing scalable and shared storage for your applications. For more details, refer to the EFS documentation.EBS Controller: The EBS Controller allows you to manage EBS volumes within your EKS cluster, providing persistent block storage for your applications. For more details, refer to the EBS documentation.  How do I set up an Add-on?‚Äã To set up an add-on, follow these steps: Navigate to the Add-ons section in the Cluster sectionSelect the desired add-on from the list of available options.Configure the add-on settings as needed.Click &quot;Deploy&quot; to install the add-on in your EKS cluster. For more detailed instructions, refer to the Add-ons setup guide. ","version":"Next","tagName":"h2"},{"title":"EBS (Elastic Block Store)","type":0,"sectionRef":"#","url":"/cluster/addons/ebs","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"EBS (Elastic Block Store)","url":"/cluster/addons/ebs#faqs","content":" What is AWS EBS?‚Äã AWS EBS is a block storage service from AWS designed to provide persistent storage for EC2 instances. EBS volumes are automatically replicated within their Availability Zone, offering high availability and durability, and protecting against hardware failures.  How is EBS used in SleakOps?‚Äã In SleakOps, EBS is used to deliver persistent storage for applications running on EC2 instances. Each EBS volume can be attached to a single EC2 instance at a time, although multiple volumes can be attached to one instance. Within Kubernetes, EBS is used for persistent volumes, ensuring data consistency across pod restarts and rescheduling. SleakOps manages and configures EBS automatically to suit your application needs.  What are the benefits of using EBS?‚Äã EBS provides several key benefits for high-performance applications: High performance: EBS offers consistent and low-latency performance, ideal for applications requiring quick data access.Durability: EBS volumes are replicated within their Availability Zone to ensure high durability.Scalability: Volumes can be easily resized to accommodate growing application demands.  How do I configure volumes with EBS in SleakOps?‚Äã To set up and manage volumes using EBS within SleakOps, please refer to the Volumes documentation. This guide provides instructions on creating and managing EBS volumes to support your application‚Äôs storage requirements.  How do I use ebs volumes on my own charts?‚Äã To use EBS Volumes you must pass to the chart values the 'StorageClass' name 'default-sc'. You can check your current StorageClasses kubectl get storageclass --all-namespaces   When do I use EBS?‚Äã You should use EBS when I need a volume that will be mounted to only one pod, for example a database running in the Cluster without replicas. ","version":"Next","tagName":"h2"},{"title":"EFS (Elastic File System)","type":0,"sectionRef":"#","url":"/cluster/addons/efs","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"EFS (Elastic File System)","url":"/cluster/addons/efs#faqs","content":" What is AWS EFS?‚Äã AWS EFS is a cloud file storage service from AWS that provides shared and scalable storage for applications and services. It is ideal for workloads that require concurrent access to a common file system across different services.  How is EFS used in SleakOps?‚Äã In SleakOps, EFS is utilized for Project volumes. Each Project can have one or more volumes, which are implemented as EFS file systems within the EKS cluster, providing shared storage that can be accessed by different services and pods.  What are the benefits of using EFS?‚Äã EFS offers several advantages, making it a powerful option for shared storage in distributed applications: Scalability: Automatically scales as files are added or removed.High availability: Designed to be highly available and durable, with data replicated across multiple availability zones.Concurrent access: Multiple EC2 instances can mount the same EFS file system simultaneously, supporting workloads requiring concurrent access.  What is the EFS Retain Policy in SleakOps?‚Äã SleakOps enforces a retain policy for EFS volumes, which prevents the deletion of an EFS volume in AWS when a volume is removed from SleakOps. This ensures data persistence even if the volume is detached from the cluster.  How do I configure volumes with EFS in SleakOps?‚Äã To set up and manage volumes using EFS within SleakOps, follow the instructions in the Volumes documentation. This guide covers creating and managing volumes for your Projects and configuring EFS settings within your cluster.  How do I use EFS volumes on my own charts?‚Äã To use EFS Volumes you must pass to the chart values the 'StorageClass' name 'efs-sc-delete' or 'efs-sc-retain' depending of which retain policy is needed. You can check your current StorageClasses kubectl get storageclass --all-namespaces   When do I use EFS?‚Äã You should use EFS when you need a volume that will be mounted to more than one pod, for example an application running in the Cluster with two replicas or more. ","version":"Next","tagName":"h2"},{"title":"Grafana","type":0,"sectionRef":"#","url":"/cluster/addons/grafana","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Grafana","url":"/cluster/addons/grafana#faqs","content":" Accessing Grafana‚Äã To access Grafana, first you need to be connected to the VPN, then just click the Grafana button on the Sleakops console. tip When creating the grafana addon, Sleakops gives you the user and credential needed. You will be redirected to the Grafana login page. Once logged in, the Grafana interface will present a dashboard like this:  How it works‚Äã Grafana as a monitoring tool, let's you directly connect to datasources within your cluster without extensive configuration. When installing Grafana, Sleakops configures Prometheus datasource, providing quick access to essential metrics through a centralized interface. When installing Loki or Otel, those datasources are also connected.  Provisioned Dashboards‚Äã Grafana on Sleakops comes with a series of practical dashboards. They cover general system metrics, offering organized views to monitor various aspects of application performance and resource usage. These dashboards can be used out-of-the-box, offering consistent data visualizations that simplify ongoing system monitoring and management. Sleakops ships with these dashboards, to monitor Resource allocations: Kubernetes / Compute Resources / ClusterKubernetes / Compute Resources / Namespaces (Pods)Kubernetes / Compute Resources / Namespaces (Workloads)Kubernetes / Compute Resources / Nodes (Pods) These ones for networking: Kubernetes / Networking / ClusterKubernetes / Networking / Namespaces (Pods)Kubernetes / Networking / Namespaces (Workloads) And some others like CoreDNS which monitors this internal dns and service discovery system. All of them are useful to monitor the health of your cluster and the applications running on it. Specially to resize workloads, investigate if more replicas are needed or if the resources are well allocated. Dashboards are customizable, allowing you to adjust the layout and data displayed to suit your specific monitoring needs. Also you can create your own dashboards.  Viewing Deployment Resources‚Äã To observe the resources used by a deployment, navigate to the resource monitoring dashboard in Grafana. Once logged in, go to: Home -&gt; Dashboards -&gt; Kubernetes / Compute Resources / Namespace (Pods) This dashboard is set up to display real-time data on CPU, memory, and disk usage, allowing you to track and manage the resources allocated to each deployment within your cluster. ","version":"Next","tagName":"h2"},{"title":"Kubecost: Cluster Cost Monitoring","type":0,"sectionRef":"#","url":"/cluster/addons/kubecost","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Kubecost: Cluster Cost Monitoring","url":"/cluster/addons/kubecost#faqs","content":" What does the idle metric means?‚Äã The 'idle' shown in all Kubecost metrics is a value that shows how much of the capacity of all your filtered options is not being used. Be careful, this value should be prudently analyzed as many of this &quot;idle&quot; capacity will be part of a Node that is not yet fully allocated or should be available for your workloads.  Should I be worried if my idle value is too high?‚Äã No, you could optimize this value by reducing the CPU and Memory requests of the workloads deployed in your Project, but bear in mind that many of this capacity is allocated as a maximum limit for your resource utilization even if it is not being used, this gives space for the internal scaling of each workload in case it needs it. By the other side, many of these workloads are cluster-critical so they will have an &quot;idle&quot; capacity to let them scale freely.  Could I review a Namespace more deeply?‚Äã Kubecost has a good granularity of how much specific you could be when analyzing costs, for example, besides Namespace costs you can start digging and by clicking the Namespace you can analyze the costs of the pods, deployments and others it has allocated on it.  Can I analyze something else beside namespaces?‚Äã Yes, from the main dashboard you can analyze more specifically the costs of a specific Node as an entity. It also allows you to review the Storage costs that is being used. For example, for a specific node you could see this:  Does Kubecost has any feature to analyze Networking costs?‚Äã At this moment SleakOps is making available the capacity to allow 'NetworkCosts' which is a feature of Kubecost that estimates the cost of the network traffic that each workload has. This feature is a really good choice if you want to analyze cluster network traffic more deeply. It can be allowed in the Kubecost form: ","version":"Next","tagName":"h2"},{"title":"Prometheus: Monitoring System","type":0,"sectionRef":"#","url":"/cluster/addons/prometheus","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Prometheus: Monitoring System","url":"/cluster/addons/prometheus#faqs","content":" Does Prometheus store metrics?‚Äã Prometheus is not directly in charge of pushing metrics to S3, this is done by a related entity called Thanos.  How does SleakOps store Prometheus metrics?‚Äã Prometheus has two related storage units: It depends on the EBS CSI Driver Addon for short-term storage.Uses S3 for long-term storage, this S3 is created in your Account in parallel with Prometheus.  Can I use Prometheus alone?‚Äã It's main purpose is to collect metrics but it also includes a frontend that can be consumed portforwarding its Pod to make some specific queries to its data or to watch some metrics, but it is far easier and comfortable to watch them with Grafana. ","version":"Next","tagName":"h2"},{"title":"Changelog","type":0,"sectionRef":"#","url":"/changelog","content":"","keywords":"","version":"Next"},{"title":"Version 2.0.0‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-200","content":" üóìÔ∏è 01/10/2025  üöÄ New Features:  Redise√±o completo de la consola: Interfaz modernizada para una experiencia m√°s limpia, √°gil e intuitiva.Soporte de tema claro: Compatibilidad total con modo claro en la interfaz.Bot de soporte: Respuestas autom√°ticas para consultas frecuentes.Documentaci√≥n ampliada: Gu√≠as detalladas y cobertura completa de todas las funcionalidades.Actualizaci√≥n lambdas: Actualizaci√≥n de versiones de Python utilizadas en las Lambdas.Project Chart: Promovido a versi√≥n estable.Project Access: Promovido a versi√≥n estable.Dependency Aurora MySQL: Promovido a versi√≥n estable.Dependency Oracle: Promovido a versi√≥n estable.Dependency MariaDB: Promovido a versi√≥n estable.Dependency Aurora PostgreSQL: Promovido a versi√≥n estable.Edici√≥n de dependencias: Ahora es posible editar dependencias existentes.Dockertron (beta): Dockerizaci√≥n autom√°tica impulsada por IA.Cancelar builds pendientes: Opci√≥n para detener builds en cola.Nueva dependencia MSK: Soporte para Kafka con AWS MSK.Mejoras en Webservices: Configuraci√≥n de custom ingress annotations y healthcheck opcional.Nodepools avanzados: Nuevas opciones de fallback y mezcla de instancias (reservadas, spot y bajo demanda) para un mayor control de costos y rendimiento.  üêû Bug Fixes:  GitLab self-hosted: Validaci√≥n de URL corregida.Cluster deletion: Manejo mejorado de la eliminaci√≥n en cascada.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.16‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-1716","content":" üóìÔ∏è 21/07/2025  üöÄ New Features:  Projects with Public Repositories: You can now create and manage projects linked to public repositories.Exclude Builds from Metrics: Builds can be excluded from the Grafana metrics dashboard for more accurate reporting.  üêû Bug Fixes:  New Project Deployments: Fixed issues preventing successful deployment of newly created projects.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.15‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-1715","content":" üóìÔ∏è 07/07/2025  üöÄ New Features:  Nodegroup Spot Resilience: Spot nodegroups now prevent failures when no Spot instances are available.File-based VariableGroups: Added support for creating variablegroups of type file.Agent Bot (beta): Experimental agent bot released in beta.  üêû Bug Fixes:  Dependent domain configuration: Generate DNS records when parent domain already created.Cluster status with nightly shutdown: Fixed incorrect status display for clusters with nightly shutdown enabled.VariableGroups filters: Fiter by projects on variablegroups listDelete cluster: Fixed deletion cluster flow.Support ticket status: Fixed close support ticket status.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.14‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-1714","content":" üóìÔ∏è 26/06/2025  üöÄ New Features:  State Transition Improvements: Smoother state changes for cluster addons and forms.Support with Images: Users can now upload images in the support chat.Jobs from Cronjobs or Existing Jobs: Ability to launch a Job from an existing cronjob or Job.Infrastructure Errors: Improved parsing and display of infrastructure errors for easier troubleshooting.  üêû Bug Fixes:  Duplicate Volumes: Fixed issue when creating volumes with the same name.Duplicate Users: Prevented creation of users with the same email.Duplicate Dependencies: Blocked creation of dependencies with duplicate names.Dependency Monitoring: Fixed date range issues on the dependency monitoring screen.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.13‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-1713","content":" üóìÔ∏è 02/06/2025  üöÄ New Features:  Dependency Monitoring: Improved visualization and tracking of dependencies.Service Control: New toggle to turn webservices and workers on or off.Builds with or without cache: Option to run builds using cache or from scratch.S3 Bucket Import with Versioning: Added support for importing S3 Buckets with active versioning.Variable Groups: Enhanced interface for managing variable groups.Dockerfile Validation: New validations to ensure reliability of Dockerfiles.  üêû Bug Fixes:  Job Logs: Fixed broken log links for Jobs.Branch Names: Added support for branches with / in their names.GitLab Pipelines: Fixed issues affecting pipeline execution.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.12‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-1712","content":" üóìÔ∏è 14/05/2025  üöÄ New Features:  New Support Flow: Introduced a support chatbot and ticketing system to provide better traceability and faster response times.Subscription &amp; Plan Management: Enhanced tools for managing subscriptions and service plans.  üêû Bug Fixes:  Form Improvements: General enhancements to form usability and validation.Project Console: UI/UX improvements in the project console screen.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.11‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-1711","content":" üóìÔ∏è 24/04/2025  üöÄ New Features:  Kubernetes 1.31 &amp; Karpenter 1.3: SleakOps now provisions clusters on EKS 1.31 and upgrades the autoscaler to Karpenter 1.3.Stronger Secret Management : All secrets are now also stored encrypted in AWS Systems Manager Parameter Store, adding an extra layer of durability beyond the in-cluster copy.  üêû Bug Fixes:  Dev-Cluster Workers: Removed the PodDisruptionBudge improving worker reliability in development clusters when the cluster had the scheduler shutdown enabled.Builds: Builds are no longer triggered for every minor project edit.Deployments: Switched deployments jobs away from Fargate; build logs are now persisted for easier troubleshooting.Web Service Details: Refined the service detail page for clearer visibility of endpoints, status, and metrics.Kubecost Add-on: Stability improvements  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.10‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-1710","content":" üóìÔ∏è 01/04/2025  üöÄ New Features:  Enhanced Permission Control: Projects can now have additional associated permissions, whether they are AWS IAM Policies or custom permissions.Dependency Details: The configuration details of each dependency are now displayed within its detail view.Cluster Update Screen Improvements: EKS Insights analysis is now included directly in SleakOps to streamline cluster updates.Build &amp; Project Enhancements: Additional information during builds and improved project validation workflows.  üêû Bug Fixes:  Improved Text Input: Resolved issues affecting text inputs in forms.Cluster Access Data: Fixed a bug when retrieving cluster connection data under a different selected account.Domain List Filters: Added filters by account to the domain listing.Nodepool List Improvements: Refined visuals for the nodepool list view.Add-on Installation Updates: The list of add-ons now refreshes properly after installation.Variable Group Editing: Fixed an issue with editing variable groups.Subscription Attachment: Addressed a bug that prevented new subscriptions from attaching correctly.Cost Forecast: Fixed forecasting issues for better cost estimations.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.9‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-179","content":" üóìÔ∏è 17/02/2025  üöÄ New Features:  Cronjob Enhancements: Configure cronjob policies and easily filter between active and inactive cronjobs.Support Emails on Notifications: When SleakOps generates a notification, users now receive it via email.EKS Insights: During cluster upgrades, SleakOps checks EKS Insights to ensure everything is running smoothly.  üêû Bug Fixes:  Project Flow Improvements: Enhanced various settings, forms, and other elements for smoother project management.AWS Account Creation Flow: Now supports inactive AWS accounts, providing clear guidance on how to manually activate them before resuming the process in SleakOps.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.8‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-178","content":" üóìÔ∏è 10/02/2025  üöÄ New Features:  Kubernetes 1.30: Updated EKS support to version 1.30.  üêû Bug Fixes:  Minor UI Enhancements: Improved the visual design for project and workload screens.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.7‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-177","content":" üóìÔ∏è 05/02/2025  üöÄ New Features:  Import from External Buckets: Quickly copy files from an external S3 Bucket into SleakOps via the new Import Bucket feature.Project View Overhaul: See logs and key info in a single screen for better visibility.Executions Renamed to Workloads: Updated terminology to align with internal cluster notation.Cluster Deletion Optimization: Added extra validation for a more secure and stable deletion process.  üêû Bug Fixes:  Project Permissions for Jobs: Fixed an issue where Jobs used cluster node permissions instead of Project permissions.Docker Args Modification: Builds now correctly apply any Docker Args changed just before they run.VPN Profile Generation: Resolved an issue preventing third-party user profiles from being generated successfully.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.6‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-176","content":" üóìÔ∏è 06/01/2025  üöÄ New Features:  New Nodepool Configurations: You can now set additional parameters, such as minimum instance sizes and more.Job with Specific Images: When creating a job, you can specify the exact image and tag you want to run (e.g., postgres:16.4).(BETA) Chart Extension by Project: SleakOps can now extend the charts used to deploy project workloads, allowing you to add dependencies. For more information, see the Helm documentation.CI/CD Improvements: The file for configuring CI/CD has been simplified and optimized.  üêû Bug Fixes:  Internal Web Service URL: Fixed an issue that caused incorrect URLs for ‚Äúinternal‚Äù type web services.Volume Deletion: Resolved problems related to volume deletion under various retention policies.UX/UI Enhancements: Improvements in the interface for Projects, Volumes, and Variable Groups.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.5‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-175","content":" üóìÔ∏è 09/12/2024  üöÄ New Features:  AWS Integration Error Handling:: Implemented a mechanism to handle delays in AWS account activations created by SleakOps.Add-on Links in Builds: Added links for easily viewing logs and metrics during the build process.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.4‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-174","content":" üóìÔ∏è 05/12/2024  üöÄ New Features:  Add-on Accessibility: Added links in SleakOps for easy access to view logs, APM, or metrics for specific resources.OpenTelemetry (Beta): Introduced an add-on to enhance observability in applications deployed with SleakOps. With OpenTelemetry, you can have your own APM to monitor metrics like request rate, latency, and error rate of your application.Add-on Availability Configurations: Added various availability settings for each add-on.Documentation: Updated the add-on documentation and made it available in Spanish.  üêû Bug Fixes:  Kubecost Integration Review: Reviewed the Prometheus-Kubecost integration. Kubecost now correctly maps the names of deployed resources to their costs, greatly improving the accuracy of its estimates. It's now possible to enable approximate network traffic cost analysis within the cluster in Kubecost (Beta).  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.3‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-173","content":" üóìÔ∏è 14/11/2024  üöÄ New Features:  Oracle RDS Support (Beta): You can now manage Oracle RDS instances as dependencies within SleakOps.Aurora PostgreSQL Serverless Support (Beta): Added the ability to create and manage Aurora PostgreSQL Serverless databases.  üêû Bug Fixes:  Various minor bug fixes.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.2‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-172","content":" üóìÔ∏è 05/11/2024  üöÄ New Features:  S3 Bucket Deletion: Introduced the ability to delete S3 buckets containing a large number of files.VPN: Updated the Pritunl module to the latest version for enhanced security and performance.Subscription Management Improvements: Enhanced the management of subscriptions for a better user experience.User Registration: Enabled the registration of new users to the platform.  üêû Fixes:  Various minor bug fixes.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.1‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-171","content":" üóìÔ∏è 30/10/2024  üöÄ New Features:  Environment and Domain Creation: Improved the process for creating environments and domains. You can now use a different domain than the one configured globally without any limitations.Notifications: Added a notification system to inform users about pending manual actions and scheduled infrastructure updates.Documentation: Updated documentation on managing domains, projects, dependencies, and environment variables.  üêû Fixes:  Various minor bug fixes.  ","version":"Next","tagName":"h2"},{"title":"Version 1.7.0‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-170","content":" üóìÔ∏è 14/10/2024  üöÄ New Features:  Advanced Node Management: Introduced node pool management to provide greater control over the types of nodes where workloads are executed.Cluster Module Migration: All modules created with the cluster now run on Graviton instances, enhancing performance and reducing costs.Cluster Add-ons: All add-ons now run on Graviton instances, further improving performance and lowering costs.Isolated Build Nodes: Builds are now executed on dedicated nodes separate from the application nodes, improving the stability of the nodes running applications.  üêû Fixes:  Various minor bug fixes.  ","version":"Next","tagName":"h2"},{"title":"Version 1.6.3‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-163","content":" üóìÔ∏è 27/09/2024  üöÄ New Features:  Registration: Implemented a new registration flow.  üêû Fixes:  Various minor bug fixes and improvements.  ","version":"Next","tagName":"h2"},{"title":"Version 1.6.2‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-162","content":" üóìÔ∏è 09/19/2024  üöÄ New Features:  Upgrades: Updated Prometheus, Loki, and EBS CSI Driver to the latest versions as of August 2024.EBS CSI Driver Migration: SleakOps now uses the AWS-managed EKS Addon for the EBS CSI Driver, replacing the self-managed version.Prometheus and Loki with EBS: Prometheus now utilizes EBS volumes for data persistence, preventing data loss even if the pods crash.Loki with SimpleScalable: It adopts a SimpleScalable structure with TSDB storage for logs, enhancing performance.SQS Dead-letter Queues: Now supports the creation of SQS queues with associated dead-letter queues for improved error handling.  üêû Fixes:  Various minor bug fixes and improvements to the platform's workload flows.  ","version":"Next","tagName":"h2"},{"title":"Version 1.6.1‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-161","content":" üóìÔ∏è 08/22/2024  üöÄ New Features:  Dependency Version Updates: Updated versions of MQ, Elasticsearch, Memcache, and Redis dependencies.Authentication Improvements: Added support for storing authentication tokens via cookies instead of local storage.Added ACM validation record printing on the ACM detail screen, and ACM status is now included in the system.  üêû Fixes:  Issues with the provider flow have been resolved.  ","version":"Next","tagName":"h2"},{"title":"Version 1.6.0‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-160","content":" üóìÔ∏è 08/12/2024  üöÄ New Features:  Support for ARM Instances and Additional RDS Versions: Added ARM instances and extra versions in RDS.EKS Updated to Version 1.29: EKS has been updated to version 1.29. Changelogs for EKS updates are now displayed.Improvements in Provider Creation and Editing: Screens and fields for provider forms were updated, including changes in states and visual display.Improved Repository Search: Added support for asynchronous search in the repository selector and enhanced the search function for GitHub, GitLab, and Bitbucket.Healthcheck Parameterization: Healthcheck properties can now be parameterized with JSONSchema.New Dashboard: A new dashboard has been added to view consumption by namespace.  Fixes:  Fixed an error when regenerating certificates, as well as issues with builds not running properly.Frontend errors related to listing and API problems that caused filtering errors have been corrected.  ","version":"Next","tagName":"h2"},{"title":"Version 1.5.1‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-151","content":" üóìÔ∏è 06/24/2024  üöÄ New Features:  Advanced Resource Configuration: Advanced options for resource configuration in project environments have been implemented.Optimization of Data Collection Scripts: Improved the efficiency of data collection scripts for faster workload.  üêû Fixes:  Several interface errors affecting system usability have been resolved.  ","version":"Next","tagName":"h2"},{"title":"Version 1.5.0‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-150","content":" üóìÔ∏è 05/23/2024  üöÄ New Features:  Multiple Project Environments Creation: You can now create multiple project environments using the same repository and branch.Domain Validation for Aliases: Improved domain creation validation for aliases by using an existing usable ACM for ingress.Resource Configuration in Project Env: Added the ability to configure build and deploy resources per project environment.Deploy and Build Request Configuration: Added the option to configure deploy and build requests in a ProjectEnv.Grafana Dashboard: A Grafana dashboard was incorporated to visualize consumption by namespace.Loki Configuration: Logs can now be searched by namespace with the new Loki configuration.Data Collection: Improved the billing collection script to be idempotent and executable for specific dates.  üêû Fixes:  Fixed an error when creating S3 dependencies and solved a critical problem with vargroups during cluster shutdown updates.Fixed a critical error when inviting collaborators.  ","version":"Next","tagName":"h2"},{"title":"Version 1.4.3‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-143","content":" üóìÔ∏è 05/13/2024  üöÄ New Features:  Dashboard Management Improvements: Dashboard loading was improved, allowing it to be viewed even if no account is selected.Billing and Project Screen Improvements: Improvements to the billing screen were made, including a new &quot;others&quot; section to account for previously unconsidered costs. The project environment screen was also improved.Policy Updates: CloudFormation policy has been updated to enhance management and security.  üêû Fixes:  Fixed a critical error that prevented the creation of providers.Reviewed and resolved an issue related to NewRelic integration.Fixed a problem with the refresh token when requesting the VPN URI.ACM Validation Screen and Build Logs Errors: Corrections made to the ACM validation table and logs display for builds in creation state.  ","version":"Next","tagName":"h2"},{"title":"Version 1.4.2‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-142","content":" üóìÔ∏è 04/25/2024  üöÄ New Features:  New Metrics: Added new metrics for S3 buckets and RabbitMQ, improving service monitoring. An OpenSearch metrics monitoring system was also implemented.Monitoring Schema Reorganization: Monitoring schema structures were reorganized for better management and visualization. The Dependencies monitoring screen now supports different resource types, providing a more detailed view.  üêû Fixes:  A critical issue with vargroups was resolved, ensuring their proper functioning.  ","version":"Next","tagName":"h2"},{"title":"Version 1.4.1‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-141","content":" üóìÔ∏è 04/11/2024  üöÄ New Features:  Dependency and OpenSearch Monitoring: A new monitoring page was created for dependencies, facilitating the tracking of their status. OpenSearch was included.ECR Lifecycle Policy: A lifecycle policy was configured for ECR, improving image management.  üêû Fixes:  Fixed the issue of duplicate names between cluster and node in Redis.Resolved various frontend errors that affected the user experience.Fixed the problem where an error was displayed when attempting to publish a vargroup without an associated service.Issues with performing multiple deployments and releases in a row were fixed.  ","version":"Next","tagName":"h2"},{"title":"Version 1.4.0‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-140","content":" üóìÔ∏è 03/06/2024  üöÄ New Features:  Grafana Configuration: The database for the Grafana addon was configured, along with DataSources and Dashboards.Prometheus Metrics Persistence with Thanos: Added support for persisting Prometheus metrics using Thanos.New Volume API: Implemented support for the new volume API, displaying statuses and applying configuration for deployments.The update option in addons has been disabled.Now, when a dependency is deleted, a deploy with &quot;pending-approval&quot; will be created instead of an automatic one.  üêû Fixes:  Fixed an issue where pre-hooks and new volumes were added during deploys, preventing them from being generated.Subdomains are now correctly marked as delegated if the parent domains are already delegated.  ","version":"Next","tagName":"h2"},{"title":"Version 1.3.0‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-130","content":" üóìÔ∏è 01/03/2024  üöÄ New Features:  Project Details View: A detailed project view is now available in the new interface.RDS Metrics API: A new API for displaying RDS metrics has been added, improving resource visibility.Improved LogViewer: LogViewer loading is now faster and more efficient.Enhanced Onboarding: A new onboarding process has been implemented for easier setup.Redis Monitoring: Redis monitoring has been added, improving infrastructure supervision.RDS Replica Configuration: The option to configure replicas in the RDS Dependency has been added for more flexibility.Domain Deletion Status: Domain deletion now creates a deploy with pending-approval status, rather than an automatic deploy.Job Workload Improvements: Job workload has been improved, allowing automatic retries in case of an initial failure.  üêû Fixes:  Bitbucket integration issues have been resolved.Undefined value issues in Vargroups have been fixed.  ","version":"Next","tagName":"h2"},{"title":"Version 1.2.4‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-124","content":" üóìÔ∏è 15/02/2024  üöÄ New Features:  Cluster Switcher Optimization: Cluster selector behavior has been optimized.Login in AWS Subscription Flow: The AWS subscription flow now includes the ability to log in directly.  üêû Fixes:  Callback issues for Git integrations and Docker file path for GitLab have been resolved.Minor billing screen-related bugs have been fixed.  ","version":"Next","tagName":"h2"},{"title":"Version 1.2.3‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-123","content":" üóìÔ∏è 05/02/2024  üöÄ New Features:  Alias Decoupling in Web Services: The creation of aliases is now separated from the web services form.IAM Password Reset: It is now possible to reset the IAM password for a user.  üêû Fixes:  A minor issue with release tasks has been corrected.  ","version":"Next","tagName":"h2"},{"title":"Version 1.2.2‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-122","content":" üóìÔ∏è 25/01/2024  üöÄ New Features:  Domain Validation Button: A &quot;check validation&quot; button has been added to the domain drawer for easier domain management.Activity Log Table: An activity log table has been created.Access Key Encryption: Access keys for code version providers (GIT) are now encrypted.  üêû Fixes:  An issue where the API didn't correctly recreate the ACM module during regeneration has been fixed.  ","version":"Next","tagName":"h2"},{"title":"Version 1.2.1‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-121","content":" üóìÔ∏è 12/01/2024  üöÄ New Features:  Vargroup Form Optimization: Usability improvements have been made to the Vargroup forms.Provider and User Account Deletion: Deleting a provider now also deletes associated user accounts.  üêû Fixes:  A bug in ACM certificate regeneration has been fixed.A provider deletion issue has been corrected.  ","version":"Next","tagName":"h2"},{"title":"Version 1.2.0‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-120","content":" üóìÔ∏è 05/01/2024  üöÄ New Features:  Logs in Grafana: A data source has been configured in Grafana to display logs from S3.Cluster Update Button: A button has been added to allow cluster updates from the interface.User Activity Log: An activity log for user actions has been created.Domain Validation Deploy: You can now create a deploy that runs once domains are validated.Two-Factor Authentication: Two-factor authentication (2FA) has been added to the login for enhanced security.  üêû Fixes:  An issue with builds using the same branch as the default has been fixed.Log reading has been improved for faster processing.Various frontend optimizations, including styles, search, and pending resource visibility, have been made.  ","version":"Next","tagName":"h2"},{"title":"Version 1.1.1‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-111","content":" üóìÔ∏è 05/12/2023  üöÄ New Features:  Log Viewer in Jobs: Added a log viewer in the job list, similar to what already exists for deployments.Dashboard v2: Improvements in the second version of the Dashboard, with more options and better organization of information.Cluster Certificates: Cluster certificates are now automatically deleted and updated to prevent expiration issues.  ","version":"Next","tagName":"h2"},{"title":"Version 1.1.0‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-110","content":" üóìÔ∏è 06/11/2023  üöÄ New Features:  Vargroups Management: Added the option to show vargroups in the forms for services, workers, hooks, and cronjobs.Kubecost: Integrated Kubecost with Prometheus-stack.  üêû Fixes:  Solved the issue with Karpenter on spot instances.Fixed user roles and user editing.Corrected problems when deleting an environment and the incorrect deletion of domains.Fixed the error when trying to manually start the cluster.Resolved an error in generating hooks.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.5‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-105","content":" üóìÔ∏è 27/10/2023  üêû Fixes:  Solved deployment issues and fixed Karpenter with spot instances.Fixed issues in deleting entities and validating service URLs.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.4‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-104","content":" üóìÔ∏è 11/10/2023  üöÄ New Features:  Refactoring and Improvements: Refactored the dashboard and improved log visualization and the management of entity deletion.  üêû Fixes:  Fixed user editing issues.Corrected cluster state management.Solved problems with environment domains.Fixed error handling in S3 responses with CloudFront.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.3‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-103","content":" üóìÔ∏è 25/09/2023  üöÄ New Features:  Management Buttons and Form Improvements: Added buttons for resource management and improved variable mapping forms.Cronjobs and Domain Regeneration: You can now stop or activate cronjobs and regenerate domains.  üêû Fixes:  Solved the issue of obtaining the VPN URI in Pritunl.Fixed the account selection issue for viewer users.Improved the handling of health check information sent to the backend.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.2‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-102","content":" üóìÔ∏è 04/09/2023  üöÄ New Features:  Deployment Optimization: Simplified the deployment process and project environment (ProjectEnv) editing, facilitating configuration and deployment.Resource and Configuration Adjustments: You can now create custom aliases for buckets.Health Check Improvements: The readiness probe for services in the development account is now optional.  üêû Fixes:  Solved issues related to VPN and security parameter configuration.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.1‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-101","content":" üóìÔ∏è 29/08/2023  üöÄ New Features:  Subscription Management: Login and token updates are controlled based on the subscription status. Additionally, a new API was implemented to register users and companies, validating pending subscriptions, with a new model to better manage subscriptions, integrating AwsClient.Marketplace Onboarding: Simplified process for creating users who come from a marketplace.  ","version":"Next","tagName":"h2"},{"title":"Version 1.0.0‚Äã","type":1,"pageTitle":"Changelog","url":"/changelog#version-100","content":" üóìÔ∏è 23/08/2023  üöÄ New Features:  Volume Configuration: You can now configure volumes in project environments directly from the form.Nightly Shutdown with Timezone: Added support for selecting time zones in the nightly shutdown.Manual Cluster Startup: New button to manually start clusters.CloudFront Integration: Support for using CloudFront to improve content delivery.Automatic Backups: You can configure automatic backups for dependencies.Graviton Instances: Support for using Graviton instances on nodes.Encryption: Implemented encryption in StackSettings for added security.  üêû Fixes:  Resolved an issue in the billing API and cost estimation.Fixed errors when deleting Providers and VPNs.You can now delete ACM certificates used by a Load Balancer without problems. ","version":"Next","tagName":"h2"},{"title":"Loki","type":0,"sectionRef":"#","url":"/cluster/addons/loki","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Loki","url":"/cluster/addons/loki#faqs","content":" Which dashboards allow me to read logs?‚Äã At this moment, Sleakops provide two dashboard to consult the logs that were recollected by Loki. Log Explorer: It's a simple dashboard that allows you to filter by Namespace, Pod, Container and Stream where you can choose between 'stdout' and 'stderr'. It also allows you to search expressions through the 'Search Query' field above.Container Log Dashboard: Similar to the previous but is more close to a Logs Dashboard that lets you analyze more complex cases that you might need. It's slower as it required more processing and for general querying it will not be needed.  Which is the best way to use Loki?‚Äã Minimizing the time-range that is being queried is the best way for fast and error-free logs revision as this parameter is the one with the most influence in the weight of the response. We recommend to first check for a big picture of when the problem occured and then check in Loki for logs in a more specific time-range as, generally, logs quantities could be really high. Bear in mind that Loki contains small processing units for reading, writing and as a controller (backend) so big queries might be slow if not having enough read replicas or processing capacity on them. This is modifiable through Sleakops but will also increase costs.  How can I modify the processing capacity of Loki?‚Äã SleakOps allows you to modify the processing capacity of the deployed Loki through the configuration of the Addon. One way of increasing its capacity is by modifying the quantity of replicas deployed.  How does Loki capture and store logs?‚Äã Loki collects logs from each Node of the cluster and therefore, from every container that it's running on it. In order to achieve this, SleakOps uses Promtail that is the default log Collector for Loki, for that reason, every Node of the cluster will have a Promtail instance deployed on it that is in charge of scrapping and pushing them to the Loki write instance that after a certain period of time pushes it to the S3 for long-term storage.  How is the log collection process?‚Äã The log collector, Promtail, collects and streams to Loki all the logs output through 'stdout' or 'stderr' from each running container in the cluster ","version":"Next","tagName":"h2"},{"title":"Node Pools","type":0,"sectionRef":"#","url":"/cluster/nodepools","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Node Pools","url":"/cluster/nodepools#faqs","content":" What are the different kind of node pools?‚Äã Reserved: are instances that you commit to using for a specific term (1 or 3 years) in exchange for significant cost savings (up to 75% less than On-Demand pricing). They provide the best price with commitment and are ideal for: Predictable Workloads: Applications with steady, predictable usage patterns that can benefit from long-term commitment.Production Environments: Critical applications that require guaranteed capacity and cost optimization.Cost Optimization: Workloads where you can commit to usage for extended periods to maximize savings. Spot: are instances that take advantage of spare capacity in a cloud provider's data center, offering up to 90% cost savings compared to On-Demand instances. While they can be terminated if the provider needs capacity back, this is not a limitation but an opportunity for well-architected applications. Ideal for: Cloud-Native Applications: Applications designed with stateless architecture that can handle pod interruptions and restart quickly.Cost-Optimized Workloads: Perfect for applications where significant cost savings (up to 90%) outweigh the need for guaranteed capacity.Resilient Systems: Applications with proper health checks, graceful shutdowns, and automatic restart capabilities. On-Demand: are instances in a Kubernetes cluster that run with a fixed pricing model, providing reliable access to compute resources without the risk of interruption. Can be used: Critical Workloads: Applications that require consistent uptime, such as databases, financial systems, or other critical services.Long-Running Tasks: Tasks that cannot be interrupted without significant consequence. Priority Order: When you select multiple node types, the system will automatically prioritize them in the following order to optimize costs: Reserved (best price with commitment) ‚Üí Spot (best price without commitment) ‚Üí On-Demand (highest price but most flexible). For detailed guidance on choosing the right instance types and evaluating application compatibility, see Instance Types and Node Management.  How many Node Pools can I have?‚Äã SleakOps base plan, allows you to have three extra node pool besides the build ones. If you need more, contact us.  Can I convert a spot node pool into an on demand and viceversa?‚Äã You can't directly convert a Spot node pool into an On-Demand node pool or vice versa, but you can achieve the desired outcome through a series of steps in SleakOps. Here‚Äôs how you can transition between node pools types: Create a Node Pool of the new desired type.Updade your workloads and projects to run into the new Node Pool.Delete the old node pool if it is not longer needed.  Can I convert a ARM node pool into an X86 and viceversa?‚Äã You can't change the architect type of a node pool, but you can achieve the desired outcome through a series of steps in SleakOps. Here‚Äôs how you can transition between node pools architectures: Create a Node Pool of the new desired architecture.Update your workloads and projects to run into the new Node Pool.Delete the old node pool if it is not longer needed.  How do I create a Node Pool?‚Äã Follow Creating a Node Pool  How do I manage my a Node Pool?‚Äã Follow Managing a Node Pool ","version":"Next","tagName":"h2"},{"title":"Open Telemetry","type":0,"sectionRef":"#","url":"/cluster/addons/otel","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Open Telemetry","url":"/cluster/addons/otel#faqs","content":" How it works‚Äã In order to use OpenTelemetry, you need to have a project instrumented with OpenTelemetry. Sleakops will deploy the necessary resources to collect and store the data. Instrumentation is the process of adding code to your application to collect telemetry data. OpenTelemetry provides libraries to instrument your application in a variety of languages. Also Sleakops offers Autoinstrumentation for some languages, learn more about it in the section Autoinstrumentation.  Traces and Metrics‚Äã Telemetry data consist of three main components: traces, metrics and logs. For logs Sleakops offers Loki. Traces are the path of a request through the system, while metrics are the values of the system at a given time. The OpenTelemetry addon collects traces from the pods running your project and sends them to the OpenTelemetry collector. The collector stores the traces throw Tempo . Traces could be visualized in Grafana. Also the collector generates metrics via the SpanMetrics Connector and stores them in Prometheus. A dashboard is available in Grafana for every project that gets instrumented.  Using the Addon‚Äã Let's dive in with a view of the OpenTelemetry dashboard. First three metrics are Request rates, Error rates and Durations, or RED metrics. These metrics are the most important to monitor the health of your application. Then we see a table that list Top operations (endpoints) and their error rate as well. Tipically the dashboard gives a quick look to problematic endpoints, application performance bottlenecks, as well as the overall health of the application.  Autoinstrumentation‚Äã Sleakops offers autoinstrumentation for some languages. This means that Sleakops will automatically instrument your project with OpenTelemetry. This is done by deploying an init container alongside your project. The sidecar container will collect the telemetry data and send it to the OpenTelemetry collector.  Manual instrumentation‚Äã Manual instrumentation resolves the implementation through code of OpenTelemetry in your project. This is done by adding the OpenTelemetry libraries to your project and adding the necessary code to collect the telemetry data. Sleakops presents the endpoint where the telemetry data should be sent.  What does Sleakops install when installing OpenTelemetry‚Äã The stack deployed when the addon is installed is the following: OpenTelemetry Operator OpenTelemetry Collector Custom resource (CRD)OpenTelemetry Instrumentation Custom resource (CRD) for every autoinstrumentated projectTempo with a frontend, and caching enabledS3 Bucket as Tempo Backend  ","version":"Next","tagName":"h2"},{"title":"Start using OpenTelemetry‚Äã","type":1,"pageTitle":"Open Telemetry","url":"/cluster/addons/otel#start-using-opentelemetry","content":" To start using OpenTelemetry, you need to install the addon. Then go to the Project list page, activate the project Instrumentation using the small white icon at the left of the name of the project.    These are the options you can choose from:  Option\tDescriptionEnabled\tEnable or disable instrumentation on this proyect. Autoinstrumentation\tOpt for autoinstrumentation. Read more on Autoinstrumentation and Manual Instrumentation Language\tIf autoinstrumentation is enabled, this option marks the language of the project. Currently GO, Java, NodeJS, Python and DotNet are available. Sample Rate\tIf autoinstrumentation is enabled, this option marks the sampling rate, where 0 is none and 1 is all the traces.  Projects that are instrumented are visible in the Project list page. Marked with a green icon, as in the image:   ","version":"Next","tagName":"h2"},{"title":"Instance Types and Node Management","type":0,"sectionRef":"#","url":"/cluster/nodepools/instance-types","content":"","keywords":"","version":"Next"},{"title":"Available Instance Types‚Äã","type":1,"pageTitle":"Instance Types and Node Management","url":"/cluster/nodepools/instance-types#available-instance-types","content":" ","version":"Next","tagName":"h2"},{"title":"1. Spot Instances‚Äã","type":1,"pageTitle":"Instance Types and Node Management","url":"/cluster/nodepools/instance-types#1-spot-instances","content":" Spot instances take advantage of unused capacity in AWS data centers, offering significant discounts (up to 90% less than On-Demand) but with the risk of interruption.  Characteristics:  Cost: Up to 90% discount vs On-DemandAvailability: Variable, can be interrupted with 2 minutes noticeIdeal use: Fault-tolerant applications, batch processing, development environments  ","version":"Next","tagName":"h3"},{"title":"2. On-Demand Instances‚Äã","type":1,"pageTitle":"Instance Types and Node Management","url":"/cluster/nodepools/instance-types#2-on-demand-instances","content":" On-Demand instances provide immediate and reliable access to compute resources with fixed pricing per hour or second.  Characteristics:  Cost: Fixed price, higher than SpotAvailability: Guaranteed, no interruption riskIdeal use: Critical applications, databases, production services  ","version":"Next","tagName":"h3"},{"title":"3. Reserved Instances‚Äã","type":1,"pageTitle":"Instance Types and Node Management","url":"/cluster/nodepools/instance-types#3-reserved-instances","content":" Reserved instances offer significant discounts (up to 75%) in exchange for a usage commitment of 1 or 3 years.  Characteristics:  Cost: Up to 75% discount with commitmentAvailability: Guaranteed for the committed periodIdeal use: Predictable workloads, stable production environments  ","version":"Next","tagName":"h3"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Instance Types and Node Management","url":"/cluster/nodepools/instance-types#faqs","content":" How to evaluate if my application works with Spot instances?‚Äã To determine if your application is compatible with Spot instances, evaluate the following aspects: ‚úÖ IDEAL applications for Spot: Stateless: Don't maintain critical local stateFault-tolerant: Can recover from interruptionsBatch processing: Tasks that can be restartedDevelopment/Testing: Non-critical environmentsMicroservices: With circuit breakers and retry logicTested with FIS: Applications validated with AWS Fault Injection Simulator to test node interruptions ‚ùå Applications NOT recommended for Spot: Real-time applications: That require constant latencyLong-running processes: That cannot be easily restartedPayment systems: That require high availability  When should I use On-Demand instances?‚Äã Use On-Demand instances in the following scenarios: Critical Applications: Payment and financial transaction systemsHigh-availability APIs (99.9%+ SLA)Authentication and authorization services Specific Workloads: Processes that cannot be interruptedApplications with strict latency requirementsLegacy systems that are not fault-tolerantProduction environments without redundancy Cost Considerations: When downtime cost exceeds Spot savingsFor workloads with unpredictable usage patternsIn cases where guaranteed capacity is critical ","version":"Next","tagName":"h2"},{"title":"Managing a Node Pool","type":0,"sectionRef":"#","url":"/cluster/nodepools/managing-nodepool","content":"","keywords":"","version":"Next"},{"title":"1. Access your cluster‚Äôs settings to access the Node Pools section‚Äã","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#1-access-your-clusters-settings-to-access-the-node-pools-section","content":" From the Cluster Listing, select a node pool and access the Setting option. Then, click on the Node Pools box.  ","version":"Next","tagName":"h3"},{"title":"2. Select the Node Pool‚Äã","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#2-select-the-node-pool","content":" Once you have selected your node pool, you‚Äôll find the access to update and delete them. Each node pool is displayed with CPU and Memory bars that show you how much capacity is left. The full bar represents the total capacity of the node pool, while the colored portion indicates the combined capacity used by all associated projects/workloads.  ","version":"Next","tagName":"h3"},{"title":"Changing node pool setting‚Äã","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#changing-node-pool-setting","content":" ","version":"Next","tagName":"h2"},{"title":"1. Click the setting button at the top right of the node pool card‚Äã","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#1-click-the-setting-button-at-the-top-right-of-the-node-pool-card","content":" Update the parameters into the modal and click on Save.    ","version":"Next","tagName":"h3"},{"title":"Deleting a node pool‚Äã","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#deleting-a-node-pool","content":" ","version":"Next","tagName":"h2"},{"title":"1. Click the bin button at the top right of the node pool card‚Äã","type":1,"pageTitle":"Managing a Node Pool","url":"/cluster/nodepools/managing-nodepool#1-click-the-bin-button-at-the-top-right-of-the-node-pool-card","content":" Click on Delete to confirm and trigger the action on SleakOps. ","version":"Next","tagName":"h3"},{"title":"Shutdown Cluster","type":0,"sectionRef":"#","url":"/cluster/shutdown-cluster","content":"","keywords":"","version":"Next"},{"title":"How can I activate this feature?‚Äã","type":1,"pageTitle":"Shutdown Cluster","url":"/cluster/shutdown-cluster#how-can-i-activate-this-feature","content":" Cluster Shutdown should be manually activated in Cluster Settings through the &quot;Scheduled Shutdown&quot; card.      When you set the Cluster Shutdown, you're creating two cronjobs: one for turning ON the cluster at the scheduled time and another for turning it OFF. The configuration fields are the following:  Attribute\tDescriptionDays\tThe days that the Shutdown crons will run. Auto Downtime (Local Time)\tThe hour at which the OFF action runs. It is shown in your local time. Auto Uptime (Local Time)\tThe hour at which the ON action runs. It is shown in your local time.  Below you will see a box that shows 'Generated Cron Expressions (UTC)', which displays the OFF and ON cron expressions for these two actions in UTC time.  ","version":"Next","tagName":"h3"},{"title":"How can I shutdown my cluster at any time?‚Äã","type":1,"pageTitle":"Shutdown Cluster","url":"/cluster/shutdown-cluster#how-can-i-shutdown-my-cluster-at-any-time","content":" In order to shutdown a cluster, just click on the Stop button and confirm the action.  This action can be performed just in Active clusters.  warning Shutdown requires that no dependencies are being updated, and no build or workload processes are active.    ","version":"Next","tagName":"h2"},{"title":"Turning On cluster‚Äã","type":1,"pageTitle":"Shutdown Cluster","url":"/cluster/shutdown-cluster#turning-on-cluster","content":" To turn on a cluster, click on the Play button and confirm the action.  This action is available for clusters in Scheduled Shutdown and Off status    info The cron actions that turn the cluster ON or OFF based on the scheduled time will still run even if you have manually turned it ON or OFF. ","version":"Next","tagName":"h3"},{"title":"Creating a Node Pool","type":0,"sectionRef":"#","url":"/cluster/nodepools/creating-nodepool","content":"","keywords":"","version":"Next"},{"title":"1. Access your cluster's settings to access the Node Pools section‚Äã","type":1,"pageTitle":"Creating a Node Pool","url":"/cluster/nodepools/creating-nodepool#1-access-your-clusters-settings-to-access-the-node-pools-section","content":" From the Cluster Listing, select one and access the Setting option. Then, click on the Node Pools box.    ","version":"Next","tagName":"h3"},{"title":"2. Click on Create‚Äã","type":1,"pageTitle":"Creating a Node Pool","url":"/cluster/nodepools/creating-nodepool#2-click-on-create","content":" Into the Node Pool section, if you have permission, you'll find the Create option at the top right corner. Click on it.  Notice, that the quantity of Node Pools per Cluster might be limited by your plan.    ","version":"Next","tagName":"h3"},{"title":"3. Set up you node pool‚Äã","type":1,"pageTitle":"Creating a Node Pool","url":"/cluster/nodepools/creating-nodepool#3-set-up-you-node-pool","content":" In the create Node Pool¬†modal enter:  Setting\tDescriptionName\tEnter the name of your choice for your node pool. It cannot be repeated within a cluster. Instance Type\tSelect one or more instance types (e.g. t3.medium, m5.large, c5.xlarge) based on your compute requirements. You can choose multiple instance types to provide flexibility for the autoscaler to provision the most cost-effective available instance. Node Type\tSelect one or more billing models for your instances. You can choose multiple options, and the system will prioritize them in the following order: Reserved (best price with commitment) ‚Üí Spot (best price without commitment) ‚Üí On Demand (highest price but most flexible). See What are the different kind of node pools? Architecture Type\tSelect the architecture type to be used during the creation for your instances: (64-Bit) ARM or (64-Bit) X86, based on your performance and compatibility needs. Then you'll be able of creating new instances using a different architecture. Memory Limit\tThis sets the maximum memory the cluster can use as services scale. The autoscaler provisions instances based on demand, but this doesn't mean the cluster always uses the maximum memory; it just defines the upper limit for the autoscaler. CPU Limit\tThis sets the maximum CPU the cluster can use as services scale. The autoscaler provisions instances based on demand, but this doesn't mean the cluster always uses the maximum CPU; it just defines the upper limit for the autoscaler. Storage\tSet by default in 20GB, you can modify it based on your need. (Per Node) Minimum Memory\tDefines the minimum amount of memory that must be available on each node before the autoscaler considers the node as &quot;utilized&quot;. This setting helps prevent over-provisioning by ensuring nodes maintain a minimum memory buffer for system processes and unexpected workload spikes. (Per Node) Minimum CPU\tDefines the minimum amount of CPU that must be available on each node before the autoscaler considers the node as &quot;utilized&quot;. This setting helps prevent over-provisioning by ensuring nodes maintain a minimum CPU buffer for system processes and unexpected workload spikes.  Once you've completed the form, click on Create in order to trigger the node pool creation into the selected cluster. ","version":"Next","tagName":"h3"},{"title":"Connect your Git Account","type":0,"sectionRef":"#","url":"/connect_to_git","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Connect your Git Account","url":"/connect_to_git#faqs","content":" Can I connect more than one Git Account?‚Äã Not allowed yet.  How do I Connect my account?‚Äã Access the Setting &gt;&gt; Autorizations section in SleakOps. Select your provider and grant access to SleakOps. See steps below.  Can I change my Git Account?‚Äã Yes, you can do it by deleting de existing one and connecting the new one. Be sure the new account has access to the projects using in SleakOps.  How do I connect a Self hosted Gitlab account?‚Äã Get into the URL:¬†yourgitlab.com/-/profile/applications¬†and create a new app, call it Sleakops. When asked, use these values: Setting\tDescriptionURL callback\thttps://api.sleakops.com/api/integrations/self-gitlab/callback/ scope\tRead &amp; Write. Once the application is created, add the¬†Application ID¬†and¬†Secret¬†generated inside the configuration of your¬†Sleakops account.  How do I disconnect an account?‚Äã By clicking in the X button next to the git provider. Consider that current projects will continue to function but won't be able to receive updates once this integration is removed. If you're using GitHub, you will also need to remove the Sleakops app from your GitHub account to complete the deletion process.  ","version":"Next","tagName":"h2"},{"title":"Set up your git Account‚Äã","type":1,"pageTitle":"Connect your Git Account","url":"/connect_to_git#set-up-your-git-account","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to the git authorization section‚Äã","type":1,"pageTitle":"Connect your Git Account","url":"/connect_to_git#1-navigate-to-the-git-authorization-section","content":" Into the Left Pane, access the Settings option and then, click on Authorizations.    ","version":"Next","tagName":"h3"},{"title":"2. Select your Git provider and grant access to SleakOps‚Äã","type":1,"pageTitle":"Connect your Git Account","url":"/connect_to_git#2-select-your-git-provider-and-grant-access-to-sleakops","content":" Click on your provider an follow the required steps for each one in order to grant access.    ","version":"Next","tagName":"h3"},{"title":"Integrations‚Äã","type":1,"pageTitle":"Connect your Git Account","url":"/connect_to_git#integrations","content":" Github‚Äã  Gitlab‚Äã  Bitbucket‚Äã ","version":"Next","tagName":"h2"},{"title":"Environment","type":0,"sectionRef":"#","url":"/environment","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Environment","url":"/environment#faqs","content":" How can I design my environments?‚Äã Environments can be tailored based on an application's lifecycle or the needs of different teams. For example, creating environments for development (dev), quality assurance (QA), staging (stg), and production (prod) allows each to have custom settings suitable for their specific roles. Before creating an environment, read Designing your Infra: Single Schema Vs. Multi Schema   Can I edit an environment?‚Äã No. You must delete it and create a new one.  How do I delete an environment?‚Äã Access the Environment List, on the Action column, click on the bin icon. Then confirm the action.  How can I delegate a domain?‚Äã Follow: Delegate Domains  warning Your DNS service must be delegated to the Primary Route53 of SleakOps manually. Follow the steps described on this¬†link .  ","version":"Next","tagName":"h2"},{"title":"Set up your Environment‚Äã","type":1,"pageTitle":"Environment","url":"/environment#set-up-your-environment","content":" 1. Navigate to the Environment section‚Äã  Into the Left Pane, access the Environments option and then, at the top right corner, click on the Create button.    2. Configure your Environment‚Äã  With your Account selected, you will access the following form:    Setting\tDescriptionName\tDefine a name for your environment using down case letters and middle dashes. Cluster\tSelect one of the available clusters to host the new environment. Domain\tSpecify the domain for your environment.  Once you‚Äôve completed the form, click on Create in order to trigger the environment creation into the selected cluster. ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/domain/setup","content":"","keywords":"","version":"Next"},{"title":"Set up your domains‚Äã","type":1,"pageTitle":"Overview","url":"/domain/setup#set-up-your-domains","content":" ","version":"Next","tagName":"h2"},{"title":"1. Access to the domain or subdomain information‚Äã","type":1,"pageTitle":"Overview","url":"/domain/setup#1-access-to-the-domain-or-subdomain-information","content":" SleakOps provides a detailed information regarding domains, subdomains and alias. That looks like:    To access it, there are are different ways, based on what you need to do:  If you want to delegate your main domain‚Äã  a. Access the Dashboard and look for the domain widget. b. Click on the desired domain to display the detail.    To delegate an environment‚Äôs subdomain‚Äã  a. Access the Environment‚Äôs list and select an Environment. b. Click on the Cloud icon to display the domain detail.  Create an Alias for a web service workload and delegate it‚Äã  a. Access the Workload‚Äôs list and then, the Web services section. b. Select and Workload and click on the Three Dots button. c. Choose the Detail option d. Create the Alias if it doesn‚Äôt exist by clicking the Associate new Domain and completing the form. e. Click on the Alias domain to display the detail.      ","version":"Next","tagName":"h3"},{"title":"2. Update Name Servers with Domain Registrar‚Äã","type":1,"pageTitle":"Overview","url":"/domain/setup#2-update-name-servers-with-domain-registrar","content":" Log in to the account where your domain is registered (e.g., GoDaddy, Namecheap, etc.).Locate the DNS settings for your domain.Replace the existing records with the ones provided.  ","version":"Next","tagName":"h3"},{"title":"3. Update Name Servers with Domain Registrar‚Äã","type":1,"pageTitle":"Overview","url":"/domain/setup#3-update-name-servers-with-domain-registrar","content":" It may take some time for the DNS changes to propagate globally (usually within a few hours).SleakOps periodically checks it but, if you want to manually verify it, you can click the yellow button Check Delegation to trigger the process.   ","version":"Next","tagName":"h3"},{"title":"Domain Levels & Strategies","type":0,"sectionRef":"#","url":"/domain","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#overview","content":"     ","version":"Next","tagName":"h2"},{"title":"1. Provider Domain (Root Level)‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#1-provider-domain-root-level","content":" What it is: Your organization's root domain.  Example: sleakops.com  What Sleakops creates:  ‚úÖ AWS Hosted Zone‚úÖ SSL Certificate  Use case:Establishes your main domain infrastructure. All environments and services will be organized under this domain.  When to use:  Setting up Sleakops for the first timeManaging your organization's primary domain    ","version":"Next","tagName":"h2"},{"title":"2. Environment Domain (Subdomain Level)‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#2-environment-domain-subdomain-level","content":" What it is: Subdomains representing different environments.  Examples:  qa.sleakops.comstaging.sleakops.comprod.sleakops.com  What Sleakops creates:  ‚úÖ AWS Hosted Zone‚úÖ SSL Certificate  Use case:Isolate and organize your deployment environments. Each environment gets its own subdomain with independent DNS management.  When to use:  Creating separate environments (development, staging, production)Isolating teams or projectsManaging multiple deployment stages    ","version":"Next","tagName":"h2"},{"title":"3. Webservice Domain (Auto-generated)‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#3-webservice-domain-auto-generated","content":" What it is: Automatic domain assignment for each webservice.  Pattern: [webservice-name].[environment-domain]  Example:  Webservice name: apiEnvironment: qa.sleakops.comResult: api.qa.sleakops.com  What Sleakops creates:  ‚úÖ CNAME record (automatically added to the environment's hosted zone)‚úÖ Points to Application Load Balancer (ALB)  Use case:Zero-configuration domain setup. Each service automatically gets a predictable, hierarchical domain.  When to use:  Default scenario for all webservicesWhen you want consistent, predictable URLsQuick deployments without custom domain configuration    ","version":"Next","tagName":"h2"},{"title":"4. Alias Domains (Custom Level)‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#4-alias-domains-custom-level","content":" What it is: Custom domains outside your standard hierarchy.  Examples:  api.external-domain.comwww.mycompany.ioanything.com  What Sleakops does:  ","version":"Next","tagName":"h2"},{"title":"Scenario A: Domain matches an existing hosted zone‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#scenario-a-domain-matches-an-existing-hosted-zone","content":" If external-domain.com is already a Provider or Environment in Sleakops:  ‚úÖ Provides DNS records for SSL certificate validation‚úÖ Provides ALB name for DNS configuration‚ö†Ô∏è You configure the DNS records yourself  ","version":"Next","tagName":"h3"},{"title":"Scenario B: Domain doesn't match any hosted zone‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#scenario-b-domain-doesnt-match-any-hosted-zone","content":" If anything.com is completely external:  ‚úÖ Creates SSL certificate‚úÖ Provides validation records for certificate‚úÖ Provides ALB name for DNS configuration‚ö†Ô∏è You manage DNS at your domain provider  Use case:  Custom branded domainsExternal domains pointed to your servicesMarketing or vanity URLsMulti-domain services  When to use:  The default webservice domain doesn't fit your needsYou need multiple domains for the same serviceConnecting external domains to your Sleakops services    ","version":"Next","tagName":"h3"},{"title":"Delegation Strategies‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#delegation-strategies","content":" Sleakops offers three delegation approaches, giving you flexibility based on your infrastructure needs and organizational policies.  ","version":"Next","tagName":"h2"},{"title":"Strategy A: Full Delegation (Recommended)‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#strategy-a-full-delegation-recommended","content":" Delegate the Provider (root domain) and let Sleakops manage everything    What you delegate:‚Äã  Provider domain nameservers to AWS Route 53  What Sleakops manages automatically:  ‚úÖ All environment domains (hosted zones + SSL certificates)‚úÖ All webservice domains (CNAME records + routing)‚úÖ DNS propagation and validation‚úÖ Complete SSL certificate lifecycle  Benefits:  üöÄ Zero DNS configuration after initial delegationüîí Automated SSL certificate managementüéØ Fully managed infrastructure‚ö° Fastest deployment experience  Best for:  New projects starting freshTeams wanting minimal DNS overheadOrganizations embracing fully-managed solutionsStartups and fast-moving teams  Setup:  Delegate your root domain (e.g., sleakops.com) to Sleakops Everything else is automatic    ","version":"Next","tagName":"h3"},{"title":"Strategy B: Per-Environment Delegation‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#strategy-b-per-environment-delegation","content":" Delegate individual environment domains while keeping root domain control    What you delegate:‚Äã  Individual environment domain nameservers (e.g., qa.sleakops.com, prod.sleakops.com)  What Sleakops manages automatically:  ‚úÖ All webservice domains within delegated environments‚úÖ SSL certificates for delegated environments‚úÖ DNS records within delegated zones  What you manage:  ‚öôÔ∏è Root domain DNS‚öôÔ∏è NS records pointing to each environment  Benefits:  üéõÔ∏è Control root domain for other purposes (email, marketing sites, etc.)üîí Isolated environment management‚úÖ Automatic webservice DNS within each environmentüè¢ Compliance with organizational DNS policies  Best for:  Organizations with existing root domain infrastructureTeams needing root domain for non-Sleakops servicesGradual migration to SleakopsMulti-team organizations with environment-level isolation  Setup:  Keep your root domain (e.g., sleakops.com) managed externally Delegate each environment (e.g., qa.sleakops.com) to Sleakops Add NS records in your root domain DNS for each environment    ","version":"Next","tagName":"h3"},{"title":"Strategy C: Full Control (Manual Management)‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#strategy-c-full-control-manual-management","content":" Retain complete DNS control and manually configure all records    What you delegate:‚Äã  Nothing - you manage all DNS  What Sleakops provides:  üìã DNS records for SSL certificate validationüìã ALB endpoints for traffic routing  What you manage:  ‚öôÔ∏è All DNS zones and records‚öôÔ∏è Certificate validation records‚öôÔ∏è CNAME records pointing to ALB‚öôÔ∏è All DNS updates and changes  Benefits:  üéõÔ∏è Complete DNS infrastructure controlüîê Keep DNS within existing security boundariesüìä Integration with existing DNS monitoringüè¢ Meet strict compliance requirements  Best for:  Organizations with strict DNS governanceExisting complex DNS infrastructureSecurity policies requiring DNS isolationEnterprises with dedicated DNS teams  Setup:  Create domains in Sleakops (no delegation) Sleakops provides validation records and ALB endpoints Manually add all required DNS records in your DNS provider    ","version":"Next","tagName":"h3"},{"title":"Delegation Strategy Comparison‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#delegation-strategy-comparison","content":" Aspect\tFull Delegation\tPer-Environment\tFull ControlSetup Complexity\t‚≠ê Easiest\t‚≠ê‚≠ê Moderate\t‚≠ê‚≠ê‚≠ê Complex Ongoing Maintenance\t‚≠ê None\t‚≠ê‚≠ê Minimal\t‚≠ê‚≠ê‚≠ê High Flexibility\t‚≠ê‚≠ê Limited\t‚≠ê‚≠ê‚≠ê Balanced\t‚≠ê‚≠ê‚≠ê‚≠ê Maximum Time to Deploy\t‚ö° Instant\t‚ö°‚ö° Minutes\t‚ö°‚ö°‚ö° Manual DNS Control\tSleakops\tShared\tYou Best for Teams\tSmall-Medium\tMedium-Large\tEnterprise    ","version":"Next","tagName":"h3"},{"title":"Domain Organization Patterns‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#domain-organization-patterns","content":" ","version":"Next","tagName":"h2"},{"title":"Pattern 1: Standard Hierarchy (Works with any delegation strategy)‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#pattern-1-standard-hierarchy-works-with-any-delegation-strategy","content":"   Benefits:  Clear environment separationEasy to understand and manageAutomatic SSL and DNS    ","version":"Next","tagName":"h3"},{"title":"Pattern 2: Mixed Hierarchy with Custom Domains‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#pattern-2-mixed-hierarchy-with-custom-domains","content":"   Benefits:  Professional customer-facing domainsMaintains internal structureFlexibility for white-labelingRequires manual DNS for aliases (all strategies)    ","version":"Next","tagName":"h3"},{"title":"Pattern 3: Environment per Team/Project‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#pattern-3-environment-per-teamproject","content":"   Benefits:  Clear team ownershipIndependent DNS management per teamScalable for large organizations    ","version":"Next","tagName":"h3"},{"title":"Quick Decision Guide‚Äã","type":1,"pageTitle":"Domain Levels & Strategies","url":"/domain#quick-decision-guide","content":" Need\tRecommended StrategyFastest setup\tFull Delegation Keep root for email/other services\tPer-Environment Delegation Maximum DNS control\tFull Control Gradual migration\tPer-Environment Delegation Strict compliance\tFull Control New project\tFull Delegation Enterprise with DNS team\tFull Control or Per-Environment  Task\tToolFirst time setup\tProvider Domain Create new environment\tEnvironment Domain Deploy a service\tWebservice (automatic) Custom branded URL\tAlias Domain External domain integration\tAlias Domain White-label solution\tAlias Domain ","version":"Next","tagName":"h2"},{"title":"Domain Delegation Guide","type":0,"sectionRef":"#","url":"/domain/delegation","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#overview","content":" When Sleakops creates a hosted zone, you need to delegate the domain by updating DNS records at your domain provider (registrar). This guide explains the process for each domain level.    ","version":"Next","tagName":"h2"},{"title":"Provider Domain Delegation‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#provider-domain-delegation","content":" ","version":"Next","tagName":"h2"},{"title":"What you're delegating‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#what-youre-delegating","content":" Your root domain (e.g., sleakops.com) to AWS Route 53 via Sleakops.    ","version":"Next","tagName":"h3"},{"title":"Steps‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#steps","content":" Create the Provider domain in Sleakops Sleakops creates a hosted zone in AWS Route 53You'll see a table with 4 nameserver (NS) records Locate the nameservers  ns-123.awsdns-12.com ns-456.awsdns-45.net ns-789.awsdns-78.org ns-012.awsdns-01.co.uk   Update your domain registrar Log into your domain registrar (GoDaddy, Namecheap, Google Domains, etc.)Find the DNS or Nameserver settingsReplace existing nameservers with the 4 AWS nameservers from SleakopsSave changes Verify delegation Click &quot;Check Delegate&quot; button in SleakopsWait for DNS propagation (can take up to 48 hours, usually faster)Green checkmark indicates successful delegation  ","version":"Next","tagName":"h3"},{"title":"Common Registrars‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#common-registrars","content":" GoDaddy Go to Domain ManagerClick on your domainScroll to &quot;Nameservers&quot; ‚Üí Click &quot;Change&quot;Select &quot;Enter my own nameservers (advanced)&quot;Add all 4 AWS nameserversSave  Namecheap Go to Domain ListClick &quot;Manage&quot; next to your domainSelect &quot;Custom DNS&quot; under NameserversAdd all 4 AWS nameserversClick the green checkmark  Cloudflare Note: If using Cloudflare, you must disable Cloudflare proxy for proper delegation. Remove the domain from Cloudflare, ORUpdate nameservers to AWS (removes Cloudflare DNS management)    ","version":"Next","tagName":"h3"},{"title":"Environment Domain Delegation‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#environment-domain-delegation","content":" ","version":"Next","tagName":"h2"},{"title":"What you're delegating‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#what-youre-delegating-1","content":" A subdomain (e.g., qa.sleakops.com) to its own hosted zone.    ","version":"Next","tagName":"h3"},{"title":"Two Scenarios‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#two-scenarios","content":" Scenario A: Parent domain IS managed by Sleakops‚Äã    Good news! Sleakops automatically creates the NS records in the parent hosted zone.  ‚úÖ No action needed - delegation is automatic    Scenario B: Parent domain is NOT managed by sleakops‚Äã    If sleakops.com is managed outside Sleakops, but you want qa.sleakops.com in Sleakops:  Create the Environment domain in Sleakops Sleakops creates a hosted zoneYou'll see 4 nameserver records Add NS records to parent domain Go to wherever sleakops.com DNS is managed (registrar, Cloudflare, etc.)Create 4 NS records for the subdomain:  Record Type: NS Name: qa Value: ns-123.awsdns-12.com Record Type: NS Name: qa Value: ns-456.awsdns-45.net (repeat for all 4 nameservers)   Verify delegation Click &quot;Check Delegate&quot; in SleakopsWait for DNS propagation (usually 5-30 minutes)    ","version":"Next","tagName":"h3"},{"title":"Webservice Domain Configuration‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#webservice-domain-configuration","content":" ","version":"Next","tagName":"h2"},{"title":"What happens‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#what-happens","content":" Webservice domains (e.g., api.qa.sleakops.com) are automatically configured.    ‚úÖ No delegation needed - Sleakops automatically:  Creates CNAME record in the environment's hosted zonePoints to the Application Load Balancer (ALB)Configures SSL certificate  You don't need to do anything!    ","version":"Next","tagName":"h3"},{"title":"Alias Domain Configuration‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#alias-domain-configuration","content":" Alias domains require manual DNS configuration at your domain provider.  ","version":"Next","tagName":"h2"},{"title":"Scenario A: Alias matches existing hosted zone‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#scenario-a-alias-matches-existing-hosted-zone","content":" Example: Your alias is api.external.com and you already have external.com as a Provider or Environment in Sleakops.    For SSL Certificate Validation‚Äã  Sleakops provides validation records You'll see CNAME records for certificate validationExample:  _acme-challenge.api.external.com ‚Üí _validation123.acme.aws.com   Add validation records to your DNS Go to the hosted zone for external.com (in Sleakops or wherever it's managed)Add the CNAME records exactly as shownWait for certificate validation (usually 5-15 minutes)  For Traffic Routing‚Äã  Sleakops provides ALB endpoint You'll see the ALB DNS name:  ALB: my-alb-123456.us-east-1.elb.amazonaws.com   Create CNAME record Go to your DNS management for external.comCreate a CNAME record:  Record Type: CNAME Name: api Value: my-alb-123456.us-east-1.elb.amazonaws.com TTL: 300   Verify Test the domain: curl https://api.external.comShould return your service response    ","version":"Next","tagName":"h3"},{"title":"Scenario B: Alias doesn't match any hosted zone‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#scenario-b-alias-doesnt-match-any-hosted-zone","content":" Example: Your alias is anything.com and this domain is not managed in Sleakops.    For SSL Certificate‚Äã  Sleakops creates SSL certificate Certificate validation records are providedExample:  _acme-challenge.anything.com ‚Üí _validation456.acme.aws.com  Add validation records Log into your domain provider for anything.comAdd the CNAME records for certificate validationWait for validation (5-15 minutes)  For Traffic Routing‚Äã  Sleakops provides ALB endpoint  ALB: my-alb-789012.us-east-1.elb.amazonaws.com  Configure DNS at your provider Option 1: CNAME (for subdomains)  Record Type: CNAME Name: www (or subdomain) Value: my-alb-789012.us-east-1.elb.amazonaws.com   Option 2: A Record with ALIAS (for root domain)  Some providers support ALIAS records (Route 53, Cloudflare)  Record Type: A (ALIAS) Name: @ (root) Value: my-alb-789012.us-east-1.elb.amazonaws.com   Option 3: A Record with IP (not recommended)  Lookup ALB IPs and create A records‚ö†Ô∏è IPs may change - use CNAME when possible  Verify Test: curl https://anything.comEnsure SSL certificate is valid    ","version":"Next","tagName":"h3"},{"title":"Verification Checklist‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#verification-checklist","content":" ","version":"Next","tagName":"h2"},{"title":"Provider/Environment Domain‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#providerenvironment-domain","content":"  Nameservers updated at registrar &quot;Check Delegate&quot; button shows success DNS lookup returns correct nameservers: dig NS yourdomain.com  ","version":"Next","tagName":"h3"},{"title":"Webservice Domain‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#webservice-domain","content":"  Webservice is deployed and running Domain resolves: curl https://api.qa.sleakops.com SSL certificate is valid (no browser warnings) DNS lookup returns correct CNAME: dig CNAME api.qa.sleakops.com  ","version":"Next","tagName":"h3"},{"title":"Alias Domain‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#alias-domain","content":"  Certificate validation records added Certificate shows as validated in Sleakops CNAME/A record points to ALB Domain resolves: curl https://your-alias.com DNS lookup returns correct CNAME: dig CNAME your-alias.com SSL certificate is valid    ","version":"Next","tagName":"h3"},{"title":"Troubleshooting‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"\"Check Delegate\" fails‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#check-delegate-fails","content":"   Issue: Nameservers not properly delegated  Solutions:  Verify you added ALL 4 nameserversCheck for typos in nameserver valuesWait longer (DNS propagation can take up to 48 hours)Clear DNS cache: dig @8.8.8.8 yourdomain.comVerify at registrar that changes were saved    ","version":"Next","tagName":"h3"},{"title":"Certificate validation stuck‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#certificate-validation-stuck","content":" Issue: SSL certificate not validating  Solutions:  Verify CNAME records are added correctly (no extra dots, correct values)Check TTL hasn't expiredRemove any conflicting DNS recordsWait 15-30 minutes for DNS propagationCheck DNS: dig _acme-challenge.yourdomain.com    ","version":"Next","tagName":"h3"},{"title":"Domain not resolving‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#domain-not-resolving","content":" Issue: Domain doesn't load your service  Solutions:  Verify CNAME/A record points to correct ALB endpointCheck ALB is healthy and receiving trafficVerify webservice is deployed and runningTest with curl -v https://yourdomain.com for detailed errorsCheck security groups allow traffic on port 443    ","version":"Next","tagName":"h3"},{"title":"SSL certificate errors in browser‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#ssl-certificate-errors-in-browser","content":" Issue: Browser shows &quot;Not Secure&quot; or certificate warnings  Solutions:  Verify certificate is validated in SleakopsCheck certificate includes your domain nameClear browser cacheVerify correct certificate is attached to ALB listenerCheck certificate hasn't expired    ","version":"Next","tagName":"h3"},{"title":"DNS Propagation Time‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#dns-propagation-time","content":" Change Type\tTypical Time\tMaximum TimeNameserver update\t15-30 minutes\t48 hours CNAME record\t5-15 minutes\t24 hours A record\t5-15 minutes\t24 hours Certificate validation\t5-15 minutes\t30 minutes  üí° Tip: Use https://dnschecker.org to check DNS propagation globally    ","version":"Next","tagName":"h2"},{"title":"Need Help?‚Äã","type":1,"pageTitle":"Domain Delegation Guide","url":"/domain/delegation#need-help","content":" Check Domain Levels &amp; StrategiesContact Sleakops support with your domain configuration details ","version":"Next","tagName":"h2"},{"title":"Network Resources","type":0,"sectionRef":"#","url":"/network","content":"","keywords":"","version":"Next"},{"title":"1. Overview of the Architecture‚Äã","type":1,"pageTitle":"Network Resources","url":"/network#1-overview-of-the-architecture","content":" The SleakOps network infrastructure is based on the following key components:  VPC (Virtual Private Cloud): Segregates networks by environment (Management, Production, Development).Subnets: Public: exposed to the Internet.Private: restricted access, Internet access via NAT Gateway.Persistence: for databases and storage. Internet Gateway: Enables communication between the VPC and the Internet.Route Tables: Define routing paths between subnets and to/from the Internet.Security Groups: Virtual firewalls that control inbound and outbound traffic for resources.Internal DNS: Allows internal resources to communicate using hostnames instead of IP addresses.External-DNS: Runs inside each Kubernetes (EKS) cluster and automatically manages public DNS records in Route53 for exposed services.  ","version":"Next","tagName":"h2"},{"title":"2. Typical Communication Flow‚Äã","type":1,"pageTitle":"Network Resources","url":"/network#2-typical-communication-flow","content":" The following illustrates a typical flow of network traffic in SleakOps:  Access from the Internet: A user accesses a publicly exposed service (e.g., an API). Traffic reaches the Internet Gateway and is routed to the public subnet. Access Control: The Security Group associated with the resource evaluates whether the connection is allowed. Internal Communication: Internal services (in private or persistence subnets) communicate using internal DNS, under Security Group rules. Service Exposure: If a service within a Kubernetes cluster needs to be publicly accessible (e.g., an API), it is exposed via an Application Load Balancer, and External-DNS registers the public domain automatically in Route53.  This segmentation and control ensure that only necessary services are exposed while keeping sensitive data protected.    ","version":"Next","tagName":"h2"},{"title":"3. External-DNS and Route53‚Äã","type":1,"pageTitle":"Network Resources","url":"/network#3-external-dns-and-route53","content":" An automated solution is used to manage public DNS records for deployed services, integrating the infrastructure with external DNS providers like Route53.  External-DNS does not expose services directly. It automates DNS record management for resources that are already exposed (e.g., via an Application Load Balancer).This allows services to be securely and easily accessible from the Internet.  ","version":"Next","tagName":"h2"},{"title":"4. Cross-Environment Connectivity via VPC Peering‚Äã","type":1,"pageTitle":"Network Resources","url":"/network#4-cross-environment-connectivity-via-vpc-peering","content":" To enable controlled communication between environments (e.g., between Management and Production), SleakOps sets up VPC Peering connections between the different VPCs.  VPC Peering enables two VPCs to exchange internal traffic as if they were part of the same network.It does not require Internet, NAT Gateway, or VPN traffic routing.It is a direct connection between two networks.  üí° Besides Internet Gateway access, SleakOps also supports other connectivity options such as Pritunl VPN, NAT Gateway, and Transit Gateway, depending on use case and required isolation level. ","version":"Next","tagName":"h2"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/gettingstarted","content":"","keywords":"","version":"Next"},{"title":"Sign in with your email‚Äã","type":1,"pageTitle":"Getting Started","url":"/gettingstarted#sign-in-with-your-email","content":" Sign in to our¬†web app.    info In case you do not have an account with us, you need to subscribe using AWS. Follow How to subscribe to SleakOps using AWS.  ","version":"Next","tagName":"h2"},{"title":"Requirements to Join‚Äã","type":1,"pageTitle":"Getting Started","url":"/gettingstarted#requirements-to-join","content":" You need to have a root user on AWS. It is the initial account created with full permissions to manage all resources and services, serving as the primary account for AWS Organizations. Go to AWS Organizations.You need access to your code repositories (GitLab, Bitbucket or GitHub).You need your services in Docker files.You need to be able to manage your domains. ","version":"Next","tagName":"h3"},{"title":"Build","type":0,"sectionRef":"#","url":"/project/build","content":"","keywords":"","version":"Next"},{"title":"Build creation‚Äã","type":1,"pageTitle":"Build","url":"/project/build#build-creation","content":" To create a Build you only need four parameters, only the Project field is required as the other three are, if not set, wait until this access is automatically enabled are chosen by default:  Project: Refers to what we call ProjectEnv, here you choose which ProjectEnv you want to build.Branch: Lets you choose any branch of the repository that you've chosen as Project. Defaults to Environment name.Commit hash: You can also choose the commit has to build a specific commit and not the last one as we do by default. Defaults to last commit.Tag: Just a tag to differentiate builds. Defaults to 'latest'.  ","version":"Next","tagName":"h3"},{"title":"Why do we need to Build a Docker image?‚Äã","type":1,"pageTitle":"Build","url":"/project/build#why-do-we-need-to-build-a-docker-image","content":" As we use Helm charts we need the image because is what they use to deploy a Kubernetes Release.  info Remember that you need a Build to update the code that the Deployment runs inside the Kubernetes Cluster.  CI/CD integration with SleakOps SleakOps has its own CLI Tool that you can use to automate Builds and Deployments in your CI/CD. More info here. ","version":"Next","tagName":"h2"},{"title":"Chart","type":0,"sectionRef":"#","url":"/project/chart","content":"","keywords":"","version":"Next"},{"title":"Accessing the Chart Configuration‚Äã","type":1,"pageTitle":"Chart","url":"/project/chart#accessing-the-chart-configuration","content":" You can find the Chart section by navigating to Project ‚Üí Settings:    NodePool Requirements SleakOps uses NodePools to determine where resources are deployed. You must configure the tolerations parameter to target an existing NodePool for all deployed resources.    ","version":"Next","tagName":"h2"},{"title":"Default Values‚Äã","type":1,"pageTitle":"Chart","url":"/project/chart#default-values","content":" SleakOps automatically applies default values to its Charts. You can view these values by clicking the designated area:    This opens a drawer on the right side displaying the default values for all Project workloads.  ","version":"Next","tagName":"h2"},{"title":"Workload-Specific Values‚Äã","type":1,"pageTitle":"Chart","url":"/project/chart#workload-specific-values","content":" For example, here are the default values for an 'api' WebService:    ","version":"Next","tagName":"h3"},{"title":"Global Valuess‚Äã","type":1,"pageTitle":"Chart","url":"/project/chart#global-valuess","content":" Values that apply across the entire Project:    ","version":"Next","tagName":"h3"},{"title":"Frequently Asked Questions‚Äã","type":1,"pageTitle":"Chart","url":"/project/chart#frequently-asked-questions","content":" Where can I find my Project's Chart?‚Äã Currently, Charts are not viewable directly in the platform. However, you can download the Chart from the ECR repository created for your Project in the corresponding AWS Account.  Can I modify the Chart deployed by a Project?‚Äã Yes, with some limitations. You can: Add custom templates using Extra TemplatesAdd chart dependencies using Chart Dependencies, similar to Helm Chart Dependencies   Can I add a custom Ingress to my Project?‚Äã Yes, this is one of the primary use cases for the Extra Templates feature. See the Extra Templates documentation for detailed instructions.  Can I modify existing Kubernetes Service templates?‚Äã No, modifying SleakOps built-in templates is not currently supported. We are working on enabling modifications to built-in templates in future releases. ","version":"Next","tagName":"h2"},{"title":"Chart Dependencies","type":0,"sectionRef":"#","url":"/project/chart/chart_dependencies","content":"","keywords":"","version":"Next"},{"title":"Supported Chart Repositories‚Äã","type":1,"pageTitle":"Chart Dependencies","url":"/project/chart/chart_dependencies#supported-chart-repositories","content":" Currently, SleakOps supports Bitnami Charts exclusively. You can browse available charts on ArtifactHub to find suitable dependencies for your project.  ","version":"Next","tagName":"h2"},{"title":"Adding Chart Dependencies‚Äã","type":1,"pageTitle":"Chart Dependencies","url":"/project/chart/chart_dependencies#adding-chart-dependencies","content":" To add a new Chart Dependency, click the Create button in the Chart Configuration section:    ","version":"Next","tagName":"h2"},{"title":"Configuration Steps‚Äã","type":1,"pageTitle":"Chart Dependencies","url":"/project/chart/chart_dependencies#configuration-steps","content":" Search and Select: Use the first two fields to search for the chart name and select your desired versionConfigure Values: Modify the values section below to customize the deploymentSet Tolerations: Critical - Update all tolerations fields in the chart values to target your NodePool    Important Ensure every tolerations field in the chart values is properly configured to use a NodePool. Without this configuration, Kubernetes cannot determine where to schedule the pods, leading to deployment failures.  ","version":"Next","tagName":"h3"},{"title":"Frequently Asked Questions‚Äã","type":1,"pageTitle":"Chart Dependencies","url":"/project/chart/chart_dependencies#frequently-asked-questions","content":" My deployment succeeded but pods aren't working. What's wrong?‚Äã The most common cause is incorrect NodePool configuration. Verify that: All tolerations fields are properly set to target existing NodePoolsThe NodePool has sufficient resourcesThe NodePool is in a healthy state  I can't find the chart I need. What are my options?‚Äã Currently, only Bitnami repository charts are supported. If you need a chart not available in Bitnami's repository, please contact our support team to discuss alternatives or request additional repository support.  How do I troubleshoot dependency deployment issues?‚Äã Common troubleshooting steps: Verify NodePool tolerations are correctly configuredCheck that the chart version is compatibleEnsure required values are properly setReview pod logs for specific error messages ","version":"Next","tagName":"h2"},{"title":"Project","type":0,"sectionRef":"#","url":"/project","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Project","url":"/project#faqs","content":" What is a Project or Namespace?‚Äã A Project is the combination of a repository and an environment. It acts as a Kubernetes Namespace in the cluster and contains all the services and resources related to the project within that environment.  How do I create a Project in SleakOps?‚Äã The Project and the Project will be created at the same time. You need to connect your Git account (e.g., GitHub, GitLab, or Bitbucket) to SleakOps, select a repository, choose a branch, and specify the location of the Dockerfile. Once these steps are completed, a ProjectEnv is created, and the first image build is triggered. Follow the steps below.  What happens when I create a Project?‚Äã When a Project is created, the following resources are set up: AWS Elastic Container Registry (ECR) to store container images and Helm charts.A Kubernetes Namespace to manage services in isolation.A Service Account to handle permissions and secure connections with AWS resources.A Dockerfile analysis to verify its correctness and build a container image using Kaniko  How do I add the Dockerfile Args?‚Äã If any build-time Docker Args (arguments) are required, SleakOps will prompt you to enter them before running the initial build. These arguments can be modified for future builds.  What is the purpose of the Dockerfile in my project?‚Äã The Dockerfile defines how your application is built into a container image. During Project creation, SleakOps analyzes the Dockerfile to ensure it's correctly configured, and then builds the image using Kaniko.  Where are the Docker images stored?‚Äã Docker images are stored in the AWS ECR (Elastic Container Registry) associated with your project. The images are named after the Project, which combines the environment name and project name.  What is the role of the Service Account?‚Äã The Service Account manages permissions for resources inside the Kubernetes cluster. It allows services deployed in the Project to securely interact with AWS resources like S3, RDS, or any other service your application may require.  Can I update the repository, branch, or Dockerfile path after creating a Project?‚Äã Just the Branch and the Dockerfile Path can be updated. If you need to work with a different Environment, Repository or Name, you will need to create a new record.  How does SleakOps handle the initial image build?‚Äã SleakOps automates the first image build as part of the Project creation process. This initial build ensures faster deployment by utilizing the existing infrastructure. Afterward, future image builds are triggered when services are published in deployments or manually via the Build Form.  How do I control project‚Äôs expenses?‚Äã SleakOps allows you to see all your project expenses in one place, classified by account, resources, dates. Access Projects, select one and click the button:  How do I monitor my project?‚Äã You can monitor your project by accessing Projects, selecting one **and clicking into the button:   How do I create a Kubernetes Volume?‚Äã When editing a Project, you can enable and define Kubernetes Volumes by specifying the mount path and storage capacity. SleakOps uses the AWS EFS CSI Driver to manage these volumes as EFS file systems in the EKS cluster.  üö©¬†How do I manage future builds and deployments?‚Äã Future builds and deployments can be managed manually via the SleakOps interface or automated using the SleakOps CLI.  ","version":"Next","tagName":"h2"},{"title":"Lets create your first Project on SleakOps‚Äã","type":1,"pageTitle":"Project","url":"/project#lets-create-your-first-project-on-sleakops","content":" warning You must have your Git Repository connected. See Connect your Git Account  ","version":"Next","tagName":"h2"},{"title":"1. Navigate to Create Project section‚Äã","type":1,"pageTitle":"Project","url":"/project#1-navigate-to-create-project-section","content":" Into the Left Pane, access Projects option and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Set up your Project‚Äã","type":1,"pageTitle":"Project","url":"/project#2-set-up-your-project","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Setting\tDescriptionName\tIdentify your Project. Environment\tThe environment represents a specific stage or setup within your infrastructure where your project will be deployed (e.g., Development, Staging, Production). You are associating your project‚Äôs code with a particular environment in your Kubernetes cluster. Repository\tThe repository is the Git repository that holds the codebase for your project. SleakOps will access this repository to manage code updates, builds, and deployments. Ensure that you have connected your Git provider (e.g., GitHub, GitLab, Bitbucket) and that the selected repository contains all necessary files for your project. Nodepool\tIs the resource in charge of provision the server where your services will run. More info here. Branch\tThe branch represents a specific version or line of development within the repository. This allows you to deploy a particular version of your code (e.g., main, develop, or a feature-specific branch). The branch you select will determine the code that gets built and deployed within the associated environment. Dockerfile Path\tThe Dockerfile is a critical component used to build your project into a container. The Dockerfile Path field requires the relative file path to your Dockerfile within the repository (e.g., /Dockerfile, /src/Dockerfile, or /app/Dockerfile). This file contains the instructions needed to create the container image, which SleakOps will build and later use for deployments.  Once you‚Äôve completed the form, click on Submit in order to trigger the Dockerfile validation and then the build. ","version":"Next","tagName":"h3"},{"title":"Configure your Dockerfile","type":0,"sectionRef":"#","url":"/project/configure_your_dockerfile","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Configure your Dockerfile","url":"/project/configure_your_dockerfile#faqs","content":" What do I need to specify when configuring the Dockerfile in SleakOps?‚Äã When configuring your Dockerfile in SleakOps, you need to: Set the Dockerfile Path: Provide the relative path of the Dockerfile within your repository.Provide Docker Arguments: If your Dockerfile requires specific build arguments (e.g., environment variables, configurations), you must provide these values during the Docker image build process.  How do I add the Dockerfile arguments?‚Äã Once you specify the Dockerfile path, SleakOps analyzes it to identify any build arguments that are required. If necessary, SleakOps will prompt you to provide values for these arguments. You can update these arguments at any time through the SleakOps interface.  What are Docker build arguments?‚Äã Docker build arguments are variables that are passed during the Docker build process to customize the build according to different environments or configurations. They are defined in the Dockerfile using the ARG keyword. SleakOps will identify these arguments and ask you to provide the required values. You can also update these arguments later if needed.  How do I update the Dockerfile path and arguments?‚Äã You can add the by following the steps below.  ","version":"Next","tagName":"h2"},{"title":"Set up your Dockerfile‚Äã","type":1,"pageTitle":"Configure your Dockerfile","url":"/project/configure_your_dockerfile#set-up-your-dockerfile","content":" ","version":"Next","tagName":"h2"},{"title":"1. Access to your project settings‚Äã","type":1,"pageTitle":"Configure your Dockerfile","url":"/project/configure_your_dockerfile#1-access-to-your-project-settings","content":" Complete the Dockerfile Path: To enable SleakOps to search for the needed arguments, specify the Dockerfile Path and save the changes. SleakOps will then analyze your Dockerfile and render the required build arguments for you to provide.  Dockerfile Path\tThe Dockerfile is a critical component used to build your project into a container. The Dockerfile Path field requires the relative file path to your Dockerfile within the repository (e.g., /Dockerfile, /src/Dockerfile, or /app/Dockerfile). This file contains the instructions needed to create the container image, which SleakOps will build and later use for deployments.  Add Arguments Before Saving: If you already know the required arguments, you can enter them before saving. This allows you to provide necessary values upfront rather than waiting for SleakOps to analyze the Dockerfile.  tip If you choose to add the argument using the text option: Each argument should be added on a new line, separated by an equal sign (=), with no extra spaces. ARGUMENT_NAME = VALUE ARGUMENT_TWO = VALUE ARGUMENT_ONE = VALUE ","version":"Next","tagName":"h3"},{"title":"Extra Templates","type":0,"sectionRef":"#","url":"/project/chart/extra_templates","content":"","keywords":"","version":"Next"},{"title":"Use Cases‚Äã","type":1,"pageTitle":"Extra Templates","url":"/project/chart/extra_templates#use-cases","content":" Custom Ingress configurations for specialized routingTesting pods for debugging and developmentAdditional ConfigMaps or SecretsCustom networking policiesSpecialized monitoring or logging components  NodePool Configuration Required Since SleakOps uses NodePools for resource placement, all custom resources must include proper tolerations configuration targeting an existing NodePool.  ","version":"Next","tagName":"h2"},{"title":"Deploying a Custom Ingress‚Äã","type":1,"pageTitle":"Extra Templates","url":"/project/chart/extra_templates#deploying-a-custom-ingress","content":" To deploy a custom Ingress, add your Kubernetes YAML configuration in the Templates section. Here's a complete example you can use as a starting point:  apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: coreexampledjangocelerysleakopscom namespace: example-django-celery-myenv labels: app.kubernetes.io/name: example-django-celery annotations: alb.ingress.kubernetes.io/certificate-arn: &gt;- arn:aws:acm:REGION:ACCOUNT_ID:certificate/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX alb.ingress.kubernetes.io/group.name: example-django-celery-public alb.ingress.kubernetes.io/healthcheck-path: /health alb.ingress.kubernetes.io/healthcheck-port: '8000' alb.ingress.kubernetes.io/listen-ports: '[{&quot;HTTP&quot;: 80}, {&quot;HTTPS&quot;:443}]' alb.ingress.kubernetes.io/ssl-redirect: '443' alb.ingress.kubernetes.io/success-codes: '200' alb.ingress.kubernetes.io/target-type: ip meta.helm.sh/release-name: example-django-celery-myenv meta.helm.sh/release-namespace: example-django-celery-myenv spec: ingressClassName: alb-ingressclass-public tls: - hosts: - core.example-django-celery.sleakops.com rules: - host: core.example-django-celery.sleakops.com http: paths: - path: /api/public/ pathType: Prefix backend: service: name: example-django-celery-myenv-api-public-svc port: number: 8000   Once you have defined your Ingress template, you can deploy it using the interface:    ","version":"Next","tagName":"h2"},{"title":"Frequently Asked Questions‚Äã","type":1,"pageTitle":"Extra Templates","url":"/project/chart/extra_templates#frequently-asked-questions","content":" How do I use custom values in my templates?‚Äã You can define custom values in the Values section on the right side of the interface. These values can then be referenced in your templates using Helm templating syntax. The example above shows how to create a Pod template that uses custom values and how to reference them in your YAML configuration.  What types of Kubernetes resources can I deploy?‚Äã SleakOps supports namespace-scoped resources only due to security and isolation requirements. Each project operates within its own namespace with namespace-scoped permissions. Supported resources include: Pods, Deployments, ServicesIngresses, NetworkPoliciesConfigMaps, SecretsPersistentVolumeClaimsJobs, CronJobs Not supported: ClusterRoles, ClusterRoleBindingsCustomResourceDefinitionsPersistentVolumesAny cluster-scoped resources  How do I troubleshoot template deployment issues?‚Äã Common troubleshooting steps: Validate YAML syntax - Ensure your template is valid Kubernetes YAMLCheck NodePool tolerations - Verify tolerations target existing NodePoolsReview resource quotas - Ensure sufficient resources are availableValidate references - Check that referenced services, secrets, or configmaps existCheck logs - Review deployment logs for specific error messages ","version":"Next","tagName":"h2"},{"title":"Dependencies: Integrating Databases, Caching, and Messaging Services","type":0,"sectionRef":"#","url":"/project/dependency","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#faqs","content":" What types of dependencies are included in SleakOps?‚Äã Here‚Äôs the updated list of dependencies included in SleakOps: Databases Amazon RDS: Managed relational databases such as MySQL, PostgreSQL, and others. Caching Services Amazon ElastiCache for Redis: In-memory data store for caching frequently accessed data.Amazon ElastiCache for Memcached: In-memory caching service for improved performance and reduced database load. Object Storage Amazon S3: Scalable and secure object storage for storing and retrieving any amount of data. Search and Analytics Amazon OpenSearch: A powerful search and analytics engine for exploring and visualizing data, enabling real-time insights and decision-making. Message Queuing Amazon SQS: Fully managed message queuing service that enables you to decouple components and enhance application scalability and reliability.RabbitMQ: A widely used open-source message broker that facilitates reliable messaging and integration between application components. These dependencies integrate seamlessly with SleakOps, providing a comprehensive suite of AWS and open-source services to enhance your application's functionality, performance, and scalability.  Can I modify dependency configurations after initial setup?‚Äã Yes, you can update dependency configurations at any time. Make sure to save any changes in the SleakOps interface to apply the updates.  Can the same dependency be used for multiple Projects?‚Äã At the moment this is not possible, you need one dependency per each project.  How do I delete a Dependency?‚Äã By accessing the Dependency Listing and clicking the delete option.  What happens when I delete a dependency?‚Äã By deleting a dependency, SleakOps will remove all the information related to it and all what is related to it will stop working. To solve that SleakOps create a Deployment in PENDING_APPROVAL status, that must be run manually ASAP to stop the downtime. In case you delete a database, SleakOps will generate a final snapshot before its deletion.  ","version":"Next","tagName":"h2"},{"title":"Lets add a Dependency for your Project‚Äã","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#lets-add-a-dependency-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Dependency section‚Äã","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#1-navigate-to-create-dependency-section","content":" Into the Left Pane, access Dependencies option and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select the kind of dependency you want to create‚Äã","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#2-select-the-kind-of-dependency-you-want-to-create","content":"   ","version":"Next","tagName":"h3"},{"title":"3. Select the kind of dependency you want to create‚Äã","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#3-select-the-kind-of-dependency-you-want-to-create","content":" In Sleakops all dependecies start with the same step. So, complete these attributes and click Next to continue.  Setting\tDescriptionName\tIdentify your Project. Project\tSelect between the existent projects.    ","version":"Next","tagName":"h3"},{"title":"4. Follow each dependency guide‚Äã","type":1,"pageTitle":"Dependencies: Integrating Databases, Caching, and Messaging Services","url":"/project/dependency#4-follow-each-dependency-guide","content":" To move forward choose between the following guides.  S3 Bucket.MySQL.PostgreSQL.Redis.Memcached.OpenSearch.SQS. ","version":"Next","tagName":"h3"},{"title":"AWS Aurora PostgreSQL","type":0,"sectionRef":"#","url":"/project/dependency/aurora-postgresql-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS Aurora PostgreSQL","url":"/project/dependency/aurora-postgresql-aws#faqs","content":" How does SleakOps manage Aurora PostgreSQL credentials?‚Äã When you create an Aurora PostgreSQL dependency in SleakOps, it automatically generates a Vargroup for your database cluster. This Variable Group securely stores the Aurora PostgreSQL credentials and other important configuration details, such as the cluster endpoint and user access information. You'll be able to manage them from the Vargroups section.  Can I change the Aurora PostgreSQL version after the cluster is deployed?‚Äã Yes, Aurora PostgreSQL supports engine version upgrades. However, the upgrade process requires careful planning and may involve downtime. It's recommended to test the upgrade process in a non-production environment first.  What happens if I need more storage for my Aurora PostgreSQL cluster?‚Äã Aurora PostgreSQL automatically scales storage from 10 GB up to 128 TB without requiring you to provision storage in advance. The storage scales automatically as your data grows, and you only pay for the storage you use.  How do I create an Aurora PostgreSQL database dump?‚Äã To create a dump of your Aurora PostgreSQL database: Run the pg_dump Command: pg_dump -h AURORA_POSTGRESQL_ADDRESS -U AURORA_POSTGRESQL_USERNAME -W &gt; dump.sql Replace AURORA_POSTGRESQL_ADDRESS, AURORA_POSTGRESQL_USERNAME, and dump.sql with the appropriate values. Consult Documentation: For more information on how to create a dump, refer to the official PostgreSQL documentation .  How do I import an existent dump using docker?‚Äã To import a database dump into your Aurora PostgreSQL cluster: Connect to the VPN: Ensure you are connected to the VPN of the AWS account where the Aurora cluster is located.Run Docker Container (Recommended): Install Docker on your local machine if not already installed.Run a PostgreSQL Docker container with the following command: docker run -it --name aurora-postgresql-container -v ./initial_data/:/tmp/data/ -e POSTGRESQL_ROOT_PASSWORD=AURORA_POSTGRESQL_PASSWORD -d postgres bash Attach to the container's terminal: docker exec -t -i aurora-postgresql-container bash Import the dump file: psql -h AURORA_POSTGRESQL_ADDRESS -U AURORA_POSTGRESQL_USERNAME -W -f /tmp/data/dump.sql Replace AURORA_POSTGRESQL_ADDRESS, AURORA_POSTGRESQL_USERNAME, and dump.sql with your specific details.  How do I import an existent dump to my local machine?‚Äã Alternatively, you can use a PostgreSQL client installed on your local machine to import the dump: psql -h AURORA_POSTGRESQL_ADDRESS -U AURORA_POSTGRESQL_USERNAME -W -f /tmp/data/dump.sql   What should I do if I encounter connection issues with my Aurora PostgreSQL cluster?‚Äã Check the following: Ensure the cluster endpoint, username, and password are correct.Verify that your security groups and firewall rules allow access.Ensure the cluster is running and has enough resources (CPU, memory).Check if the cluster is in an available state. Otherwise, contact us.  What are the benefits of Aurora PostgreSQL over standard RDS PostgreSQL?‚Äã Aurora PostgreSQL offers several advantages: Performance: Up to 3x faster than standard PostgreSQL on RDSScalability: Automatic storage scaling up to 128 TBAvailability: Continuous backup to S3 with point-in-time recoveryDurability: 6-way replication across 3 Availability ZonesCompatibility: PostgreSQL-compatible with minimal code changes  info AWS documentation: Amazon Aurora PostgreSQL Documentation  ","version":"Next","tagName":"h2"},{"title":"Set up your Aurora PostgreSQL‚Äã","type":1,"pageTitle":"AWS Aurora PostgreSQL","url":"/project/dependency/aurora-postgresql-aws#set-up-your-aurora-postgresql","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add Aurora PostgreSQL as a Dependency‚Äã","type":1,"pageTitle":"AWS Aurora PostgreSQL","url":"/project/dependency/aurora-postgresql-aws#1-add-aurora-postgresql-as-a-dependency","content":" To integrate Aurora PostgreSQL with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;Aurora PostgreSQL&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your Aurora PostgreSQL.‚Äã","type":1,"pageTitle":"AWS Aurora PostgreSQL","url":"/project/dependency/aurora-postgresql-aws#2-set-up-your-aurora-postgresql","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionDatabase Name\tName for the Aurora PostgreSQL database. Must follow pattern: lowercase letters and numbers, cannot be &quot;db&quot; or &quot;database&quot;. Database Engine Version\tSelect the specific version of the Aurora PostgreSQL database engine. Choose from versions supports. Each version includes specific PostgreSQL features and security updates. Database Engine Mode\tAurora PostgreSQL is available only in Serverless mode, which provides automatic scaling based on your application's needs. This mode scales compute capacity up and down automatically, making it cost-effective for variable workloads. Database Master Username\tMaster username for the Aurora PostgreSQL cluster. This is the main user with administrative privileges. Cannot be &quot;admin&quot;, &quot;user&quot;, &quot;database&quot;, or &quot;name&quot;. Must start with a letter and contain only alphanumeric characters. Auto-generate Password\tIf enabled, the backend will generate a strong password automatically for enhanced security. This is recommended for production environments. Database Master Password\tPassword for the master user. Required if auto-generate is disabled. Must be at least 8 characters long and cannot contain @, ', &quot;, or / characters. Minimum Aurora Capacity\tMinimum Aurora Capacity Units (0.5-256) for the serverless cluster. Each unit is approximately equal to 2GB of RAM. This sets the baseline performance level and ensures minimum resources are always available. Maximum Aurora Capacity\tMaximum Aurora Capacity Units (1-256) for the serverless cluster. Each unit is approximately equal to 2GB of RAM. This prevents the cluster from scaling beyond your budget limits while allowing performance optimization. Backup Retention Period\tNumber of days (1-35) for which automatic backups are kept. Aurora PostgreSQL automatically backs up your database and stores the backups in Amazon S3. Longer retention periods provide more recovery options but increase storage costs. Backup Window\tTime period for automated backups in HH:MM-HH:MM format (UTC). Choose a time when your database activity is typically low to minimize performance impact. Aurora performs backups during this window without affecting your application. Read Replicas\tConfiguration for database read replicas to improve read performance and provide additional availability. Each replica requires a unique name and can be configured as publicly accessible or private. Read replicas help distribute read traffic and provide failover capabilities.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable's name for your Aurora PostgreSQL cluster.‚Äã","type":1,"pageTitle":"AWS Aurora PostgreSQL","url":"/project/dependency/aurora-postgresql-aws#3-customize-your-variables-name-for-your-aurora-postgresql-cluster","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS Memcached","type":0,"sectionRef":"#","url":"/project/dependency/memcached-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS Memcached","url":"/project/dependency/memcached-aws#faqs","content":" What are the key use cases for Memcached?‚Äã Memcached is ideal for caching frequently accessed database queries, storing temporary user session data, and caching API responses to reduce database load.  When should I use Memcached?‚Äã Memcached is ideal for: Simple Caching Needs: If you need a basic, high-speed cache for frequently accessed data.Non-Persistent Data: When you don‚Äôt need data to be persisted and can tolerate data loss upon node failure or restart.Horizontal Scalability: For applications that benefit from adding multiple caching nodes to distribute load efficiently.Cost-Sensitive Applications: Memcached is more cost-effective than Redis because it lacks advanced features like persistence and replication.  Why should I choose Memcached over Redis?‚Äã Memcached is a simpler and more cost-effective caching solution if you don't need data persistence, replication, or advanced data types. It's suitable for applications that prioritize fast, distributed caching.  How does Memcached scale in SleakOps?‚Äã Memcached scales horizontally by adding more nodes to your cluster, allowing you to distribute the caching load across multiple nodes.  Does Memcached offer data persistence?‚Äã No, Memcached does not support data persistence. All cached data is stored in memory and will be lost if the node is restarted or fails.  What happens to the cached data if a node fails?‚Äã Cached data in Memcached is volatile, meaning it will be lost if a node fails or is restarted. For critical applications, Redis (which supports data persistence) might be a better choice.  ","version":"Next","tagName":"h2"},{"title":"Set up your AWS Memcached‚Äã","type":1,"pageTitle":"AWS Memcached","url":"/project/dependency/memcached-aws#set-up-your-aws-memcached","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add AWS Memcached as a Dependency‚Äã","type":1,"pageTitle":"AWS Memcached","url":"/project/dependency/memcached-aws#1-add-aws-memcached-as-a-dependency","content":" To integrate Memcached with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;AWS Redis&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your Memcached database.‚Äã","type":1,"pageTitle":"AWS Memcached","url":"/project/dependency/memcached-aws#2-set-up-your-memcached-database","content":" When adding Memcached as a dependency in SleakOps, you need to configure several key attributes:    Attribute\tDescriptionNode Type\tInstance class that determines the performance and memory capacity of the Redis instance. Examples: cache.t3.micro, cache.m5.large, cache.r6g.large Nodes Quantity\tDefines the number of Memcached nodes for horizontal scaling. Adding more nodes increases scalability. Example: 1 or more Port\tThe communication port used by Redis to interact with your application. Default: 11121 (can be customized)  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable names for your Memcached.‚Äã","type":1,"pageTitle":"AWS Memcached","url":"/project/dependency/memcached-aws#3-customize-your-variable-names-for-your-memcached","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS Aurora MySQL","type":0,"sectionRef":"#","url":"/project/dependency/aurora-mysql-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS Aurora MySQL","url":"/project/dependency/aurora-mysql-aws#faqs","content":" How does SleakOps manage Aurora MySQL credentials?‚Äã When you create an Aurora MySQL dependency in SleakOps, it automatically generates a Vargroup for your database cluster. This Variable Group securely stores the Aurora MySQL credentials and other important configuration details, such as the cluster endpoint and user access information. You'll be able to manage them from the Vargroups section.  Can I change the Aurora MySQL version after the cluster is deployed?‚Äã Yes, Aurora MySQL supports engine version upgrades. However, the upgrade process requires careful planning and may involve downtime. It's recommended to test the upgrade process in a non-production environment first.  What happens if I need more storage for my Aurora MySQL cluster?‚Äã Aurora MySQL automatically scales storage from 10 GB up to 128 TB without requiring you to provision storage in advance. The storage scales automatically as your data grows, and you only pay for the storage you use.  How do I create an Aurora MySQL database dump?‚Äã To create a dump of your Aurora MySQL database: Run the mysqldump Command: mysqldump -h AURORA_MYSQL_ADDRESS -u AURORA_MYSQL_USERNAME -p --all-databases &gt; dump.sql Replace AURORA_MYSQL_ADDRESS, AURORA_MYSQL_USERNAME, and dump.sql with the appropriate values. Consult Documentation: For more information on how to create a dump, refer to the official MySQL documentation .  How do I import an existent dump using docker?‚Äã To import a database dump into your Aurora MySQL cluster: Connect to the VPN: Ensure you are connected to the VPN of the AWS account where the Aurora cluster is located.Prepare the dump file: Place your database dump file (e.g., dump.sql) in the ./initial_data/ directory on your local machine.Run Docker Container (Recommended): Install Docker on your local machine if not already installed.Leave your dump in an &quot;initial_data&quot; folder.Run a MySQL Docker container with the following command: docker run -it --name aurora-mysql-container -v ./initial_data/:/tmp/data/ -e MYSQL_ROOT_PASSWORD=AURORA_MYSQL_PASSWORD -d mysql bash Attach to the container's terminal: docker exec -t -i aurora-mysql-container bash Import the dump file: mysql -h AURORA_MYSQL_ADDRESS -u AURORA_MYSQL_USERNAME -p &lt; /tmp/data/dump.sql Replace AURORA_MYSQL_ADDRESS, AURORA_MYSQL_USERNAME, and dump.sql with your specific details.  How do I import an existent dump to my local machine?‚Äã Alternatively, you can use a MySQL client installed on your local machine to import the dump: mysql -h AURORA_MYSQL_ADDRESS -u AURORA_MYSQL_USERNAME -p &lt; dump.sql   What should I do if I encounter connection issues with my Aurora MySQL cluster?‚Äã Check the following: Ensure the cluster endpoint, username, and password are correct.Verify that your security groups and firewall rules allow access.Ensure the cluster is running and has enough resources (CPU, memory).Check if the cluster is in an available state. Otherwise, contact us.  What are the benefits of Aurora MySQL over standard RDS MySQL?‚Äã Aurora MySQL offers several advantages: Performance: Up to 5x faster than standard MySQL on RDSScalability: Automatic storage scaling up to 128 TBAvailability: Continuous backup to S3 with point-in-time recoveryDurability: 6-way replication across 3 Availability ZonesCompatibility: MySQL-compatible with minimal code changesCost-effective: Pay only for the storage you use  info AWS documentation: Amazon Aurora MySQL Documentation  ","version":"Next","tagName":"h2"},{"title":"Set up your Aurora MySQL‚Äã","type":1,"pageTitle":"AWS Aurora MySQL","url":"/project/dependency/aurora-mysql-aws#set-up-your-aurora-mysql","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add Aurora MySQL as a Dependency‚Äã","type":1,"pageTitle":"AWS Aurora MySQL","url":"/project/dependency/aurora-mysql-aws#1-add-aurora-mysql-as-a-dependency","content":" To integrate Aurora MySQL with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;Aurora MySQL&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your Aurora MySQL.‚Äã","type":1,"pageTitle":"AWS Aurora MySQL","url":"/project/dependency/aurora-mysql-aws#2-set-up-your-aurora-mysql","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionDatabase Name\tName for the Aurora MySQL database. Must follow pattern: lowercase letters and numbers, cannot be &quot;db&quot; or &quot;database&quot;. Database Engine Version\tSelect the specific version of the Aurora MySQL database engine. Version 3 is compatible with MySQL 8, Version 2 with MySQL 5. Database Engine Mode\tChoose between Serverless (auto-scaling, pay-per-use for unpredictable workloads) or Provisioned (fixed capacity, better for consistent workloads). Database Master Username\tMaster username for the Aurora MySQL cluster. Cannot be &quot;admin&quot;, &quot;user&quot;, &quot;database&quot;, or &quot;name&quot;. Auto-generate Password\tIf enabled, the backend will generate a strong password automatically. Database Master Password\tPassword for the master user. Required if auto-generate is disabled. Cannot contain @, ', &quot;, or / characters. Database Instance Class\tServerless mode: Fixed to db.serverless Provisioned mode: Choose from db.t3.medium, db.t4g.medium, db.t3.large, db.t4g.large, db.r8g.large, db.r8g.xlarge, db.r7i.large, db.r7i.xlarge.t3.medium. Minimum Aurora Capacity\t(Serverless only) Minimum Aurora Capacity Units (0.5-256). Each unit ‚âà 2GB RAM. Maximum Aurora Capacity\t(Serverless only) Maximum Aurora Capacity Units (1-256). Each unit ‚âà 2GB RAM. Create a RDS from a snapshot\tMark this if restoring from a database snapshot. Snapshot Identifier\t(Required if restoring from snapshot) RDS snapshot identifier to restore from. Backup Retention Period\tNumber of days (1-35) for which automatic backups are kept. Backup Window\tPeriod for automated backups in HH:MM-HH:MM format (UTC). Read Replicas\tConfiguration for database read replicas. Each replica requires a name and publicly accessible setting.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable's name for your Aurora MySQL cluster.‚Äã","type":1,"pageTitle":"AWS Aurora MySQL","url":"/project/dependency/aurora-mysql-aws#3-customize-your-variables-name-for-your-aurora-mysql-cluster","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS MariaDB","type":0,"sectionRef":"#","url":"/project/dependency/mariadb-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS MariaDB","url":"/project/dependency/mariadb-aws#faqs","content":" How does SleakOps manage MariaDB credentials?‚Äã When you create a MariaDB dependency in SleakOps, it automatically generates a Vargroup for your database. This Variable Group securely stores the MariaDB credentials and other important configuration details, such as the database endpoint and user access information. You'll be able to manage them from the Vargroups section.  What is Multi-AZ deployment and should I enable it?‚Äã Multi-AZ (Availability Zone) deployment ensures high availability and failover support by replicating your database in another availability zone. It's recommended for production environments to prevent downtime. Keep in mind that it increases costs.  Can I change the MariaDB version after the database is deployed?‚Äã Yes, MariaDB supports engine version upgrades. However, the upgrade process requires careful planning and may involve downtime. It's recommended to test the upgrade process in a non-production environment first.  What happens if I need more storage for my MariaDB database?‚Äã You can adjust the storage size when configuring your database. If you need more storage after deployment, SleakOps allows you to scale the storage size without downtime.  How do I create a MariaDB database dump?‚Äã To create a dump of your MariaDB database: Run the mysqldump Command: mysqldump -h MARIADB_ADDRESS -u MARIADB_USERNAME -p --all-databases &gt; dump.sql Replace MARIADB_ADDRESS, MARIADB_USERNAME, and dump.sql with the appropriate values. Consult Documentation: For more information on how to create a dump, refer to the official MariaDB documentation .  How do I import an existent dump using docker?‚Äã To import a database dump into your MariaDB RDS instance: Connect to the VPN: Ensure you are connected to the VPN of the AWS account where the RDS instance is located.Run Docker Container (Recommended): Install Docker on your local machine if not already installed.Run a MariaDB Docker container with the following command: docker run -it --name mariadb-container -v ./initial_data/:/tmp/data/ -e MARIADB_ROOT_PASSWORD=MARIADB_PASSWORD -d mariadb bash Attach to the container's terminal: docker exec -t -i mariadb-container bash Import the dump file: mysql -h MARIADB_ADDRESS -u MARIADB_USERNAME -p &lt; /tmp/data/dump.sql Replace MARIADB_ADDRESS, MARIADB_USERNAME, and dump.sql with your specific details.  How do I import an existent dump to my local machine?‚Äã Alternatively, you can use a MariaDB client installed on your local machine to import the dump: mysql -h MARIADB_ADDRESS -u MARIADB_USERNAME -p &lt; dump.sql   What should I do if I encounter connection issues with my MariaDB database?‚Äã Check the following: Ensure the database endpoint, username, and password are correct.Verify that your security groups and firewall rules allow access.Ensure the database is running and has enough resources (CPU, memory). Otherwise, contact us.  info AWS documentation: Amazon RDS MariaDB Documentation  ","version":"Next","tagName":"h2"},{"title":"Set up your MariaDB‚Äã","type":1,"pageTitle":"AWS MariaDB","url":"/project/dependency/mariadb-aws#set-up-your-mariadb","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add MariaDB as a Dependency‚Äã","type":1,"pageTitle":"AWS MariaDB","url":"/project/dependency/mariadb-aws#1-add-mariadb-as-a-dependency","content":" To integrate MariaDB with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;MariaDB&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your MariaDB.‚Äã","type":1,"pageTitle":"AWS MariaDB","url":"/project/dependency/mariadb-aws#2-set-up-your-mariadb","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionDatabase Name\tName for the MariaDB database. Must follow pattern: lowercase letters and numbers, cannot be &quot;db&quot; or &quot;database&quot;. This identifies your specific database within the MariaDB instance. Database Engine Version\tSelect the specific version of the MariaDB database engine. Choose from versions supported. Each version includes specific MariaDB features, performance improvements, and security updates. Database Master Username\tMaster username for the MariaDB database instance. This is the main user with administrative privileges. Cannot be &quot;admin&quot;, &quot;user&quot;, &quot;database&quot;, or &quot;name&quot;. Must start with a letter and contain only alphanumeric characters. Auto-generate Password\tIf enabled, the backend will generate a strong password automatically for enhanced security. This is recommended for production environments to ensure password complexity. Database Master Password\tPassword for the master user. Required if auto-generate is disabled. Must be at least 8 characters long and cannot contain @, ', &quot;, or / characters. Create a RDS from a snapshot\tMark this if restoring from a database snapshot. When enabled, you'll need to provide the snapshot identifier and some fields become read-only. Snapshot Identifier\t(Required if restoring from snapshot) RDS snapshot identifier to restore from. This allows you to restore your database from a previous backup point. Database Instance Class\tDefine the instance class that specifies the hardware configuration for your MariaDB database. Choose from t4g/t3 (burstable performance) or m7i/m8g (memory optimized) instance types. This controls CPU, memory, and network performance. Database Storage\tSpecify the amount of storage allocated for the database in GiB (20-6144 GB). MariaDB uses General Purpose SSD storage by default. This is the initial storage allocation for your database. Storage Autoscaling Enabled\tEnable automatic storage scaling for the RDS instance. When enabled, AWS will automatically increase storage when needed, up to the maximum allocated storage limit. Maximum Allocated Storage\t(Required if storage autoscaling is enabled) Maximum storage size in GiB (20-65536 GB) when storage autoscaling is enabled. This prevents unexpected costs by setting an upper limit for automatic scaling. Database Multi-AZ\tEnable Multi-Availability Zone deployment for high availability. This creates a standby replica in a different AZ and provides automatic failover capability. Recommended for production environments. Automated Backup\tEnable automatic backups for the RDS instance. When enabled, MariaDB will perform daily snapshots and transaction log backups, providing point-in-time recovery capabilities. Backup Retention Period\t(Required if automated backup is enabled) Number of days (1-35) for which automatic backups are kept. Longer retention periods provide more recovery options but increase storage costs. Backup Window\t(Required if automated backup is enabled) Time period for automated backups in HH:MM-HH:MM format (UTC). Choose a time when your database activity is typically low to minimize performance impact. Read Replicas\t(Required if automated backup is enabled) Configuration for database read replicas to improve read performance and provide additional availability. Each replica requires a name, instance class, and publicly accessible setting.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable's name for your MariaDB database.‚Äã","type":1,"pageTitle":"AWS MariaDB","url":"/project/dependency/mariadb-aws#3-customize-your-variables-name-for-your-mariadb-database","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS MSK (Managed Streaming for Apache Kafka)","type":0,"sectionRef":"#","url":"/project/dependency/msk-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS MSK (Managed Streaming for Apache Kafka)","url":"/project/dependency/msk-aws#faqs","content":" How does SleakOps manage MSK credentials?‚Äã When you create an MSK dependency in SleakOps, it automatically generates a Vargroup for your Kafka cluster. This Variable Group securely stores the MSK credentials and other important configuration details, such as the cluster endpoints and authentication information. You'll be able to manage them from the Vargroups section.  Can I change the Kafka version after the cluster is deployed?‚Äã Yes, Amazon MSK supports Kafka version upgrades. Choose from the versions supported. However, the upgrade process requires careful planning and may involve downtime. It's recommended to test the upgrade process in a non-production environment first.  What happens if I need more storage for my MSK cluster?‚Äã For Provisioned mode: You configure the storage size per broker node (1-16384 GB) during cluster creation. This storage is automatically provisioned and managed by AWS. If you need more storage, you'll need to modify the cluster configuration, which may require downtime. For Serverless mode: Storage is automatically managed and scaled by AWS based on your usage patterns. You don't need to configure storage size as it scales automatically.  How do I connect to my MSK cluster?‚Äã To connect to your MSK cluster: Get the Bootstrap Servers: Use the bootstrap server endpoints provided by SleakOps in the vargroup.Configure Authentication: MSK supports various authentication methods including SASL/SCRAM, IAM, and TLS.Use Kafka Clients: Connect using standard Kafka clients and libraries.VPN Connection: Ensure you are connected to the VPN of the AWS account where the MSK cluster is located. Note: Both Provisioned and Serverless modes use the same connection methods, but Serverless mode may have different performance characteristics and scaling behavior.  How do I create topics in my MSK cluster?‚Äã To create topics in your MSK cluster: Using Kafka Tools: kafka-topics.sh --create --topic my-topic --bootstrap-server MSK_BOOTSTRAP_SERVERS --partitions 3 --replication-factor 3 Using Kafka Admin Client: Use the Kafka Admin Client in your application code.Replace Variables: Replace MSK_BOOTSTRAP_SERVERS with the actual bootstrap server endpoints from your vargroup. Note: For Provisioned mode, set the replication factor to match your number of broker nodes (minimum 3 for production). For Serverless mode, AWS manages the replication automatically.  How do I produce and consume messages?‚Äã To produce and consume messages: Producer Example: kafka-console-producer.sh --topic my-topic --bootstrap-server MSK_BOOTSTRAP_SERVERS Consumer Example: kafka-console-consumer.sh --topic my-topic --bootstrap-server MSK_BOOTSTRAP_SERVERS --from-beginning Application Integration: Use Kafka clients in your application code for production use.  What should I do if I encounter connection issues with my MSK cluster?‚Äã Check the following: Ensure the bootstrap server endpoints, username, and password are correct.Verify that your security groups and firewall rules allow access.Ensure the cluster is running and has enough resources.Check if the cluster is in an available state.Verify your authentication configuration (SASL/SCRAM, IAM, or TLS). Otherwise, contact us.  What are the benefits of Amazon MSK over self-managed Kafka?‚Äã Amazon MSK offers several advantages: Managed Operations: No need to manage Kafka infrastructureHigh Availability: Built-in replication and failover capabilitiesSecurity: Integrated with AWS security servicesMonitoring: CloudWatch integration for monitoring and alertingScalability: Easy scaling of broker instances and storageCompatibility: Fully compatible with Apache Kafka  info AWS documentation: Amazon MSK Documentation  ","version":"Next","tagName":"h2"},{"title":"Set up your MSK‚Äã","type":1,"pageTitle":"AWS MSK (Managed Streaming for Apache Kafka)","url":"/project/dependency/msk-aws#set-up-your-msk","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add MSK as a Dependency‚Äã","type":1,"pageTitle":"AWS MSK (Managed Streaming for Apache Kafka)","url":"/project/dependency/msk-aws#1-add-msk-as-a-dependency","content":" To integrate MSK with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;MSK&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your MSK.‚Äã","type":1,"pageTitle":"AWS MSK (Managed Streaming for Apache Kafka)","url":"/project/dependency/msk-aws#2-set-up-your-msk","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionDeployment Mode\tChoose between Provisioned (fixed capacity, full control over configuration) or Serverless (auto-scaling, pay-per-use for variable workloads). Provisioned mode is ideal for consistent workloads with specific performance requirements, while Serverless is perfect for development and variable traffic patterns. Kafka Version\t(Required for Provisioned mode) Select the specific version of Apache Kafka for your MSK cluster. Choose from versions 2.8.1, 3.2.0, 3.3.2, 3.4.0, or 3.5.1. Each version includes specific Kafka features, performance improvements, and security updates. Instance Type\t(Required for Provisioned mode) Define the instance type that specifies the hardware configuration for your Kafka brokers. Choose from t3 (burstable performance) or m5 (general purpose) instance types. This controls CPU, memory, and network performance for your streaming workloads. Broker Nodes\t(Required for Provisioned mode) Number of broker nodes in your MSK cluster (2-15 nodes). Use 2 nodes for development environments. For production, use 3 or more nodes (must be multiple of 3) to ensure high availability and fault tolerance. More nodes provide better performance and availability. Storage Size (GB)\t(Required for Provisioned mode) Storage size in GB per broker node (1-16384 GB). This determines how much data each broker can store locally. Consider your data retention requirements and throughput needs when setting this value.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable's name for your MSK cluster.‚Äã","type":1,"pageTitle":"AWS MSK (Managed Streaming for Apache Kafka)","url":"/project/dependency/msk-aws#3-customize-your-variables-name-for-your-msk-cluster","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS OpenSearch","type":0,"sectionRef":"#","url":"/project/dependency/opensearch-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS OpenSearch","url":"/project/dependency/opensearch-aws#faqs","content":" What are the use cases for OpenSearch?‚Äã OpenSearch is commonly used for full-text search, real-time analytics, monitoring and observability, and log analysis. It is also ideal for powering search functionalities on websites and applications.  What does &quot;Dedicated Master Enabled&quot; mean?‚Äã When enabled, SleakOps sets up dedicated master nodes that help with managing the OpenSearch domain. They provide enhanced stability by separating management tasks from data nodes. This is highly recommended for production workloads.  What is the recommended configuration for master nodes in production?‚Äã For production environments, SleakOps recommends using 3 dedicated master nodes to improve the stability and performance of your OpenSearch cluster.  ","version":"Next","tagName":"h2"},{"title":"Set up your OpenSearch‚Äã","type":1,"pageTitle":"AWS OpenSearch","url":"/project/dependency/opensearch-aws#set-up-your-opensearch","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add AWS OpenSearch as a Dependency‚Äã","type":1,"pageTitle":"AWS OpenSearch","url":"/project/dependency/opensearch-aws#1-add-aws-opensearch-as-a-dependency","content":" To integrate OpenSearch with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;SQS&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your OpenSearch.‚Äã","type":1,"pageTitle":"AWS OpenSearch","url":"/project/dependency/opensearch-aws#2-set-up-your-opensearch","content":" When adding OpenSearch as a dependency in SleakOps, you need to configure several key attributes:    Attribute\tDescriptionFIFO Queue\tSpecifies the type of SQS queue. Where Standard Queue (for most use cases) or FIFO Queue (if message ordering is required)` FIFO Deduplication\tOnly for FIFO Queues, in order to avoid duplicates. Message Retention Period\tSpecifies the amount of time a message will be retained in the queue if it hasn't been consumed. Maximum Message Size\tThe maximum size of a message that can be sent to the SQS queue. Delivery Delay in Seconds\tThe delay between the message being sent to SQS and it being visible in the queue. No delay by default. Receive Message Wait Time\tDetermines how long a ReceiveMessage call waits if no messages are available in the queue. Visibility Timeout\tThe duration that a message remains invisible after a receiving component reads it from the queue. Dead-Letter Queue (DLQ)\tAdd a queue where messages that fail to be processed multiple times are sent for additional analysis.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable names for your SQS.‚Äã","type":1,"pageTitle":"AWS OpenSearch","url":"/project/dependency/opensearch-aws#3-customize-your-variable-names-for-your-sqs","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS Oracle","type":0,"sectionRef":"#","url":"/project/dependency/oracle-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS Oracle","url":"/project/dependency/oracle-aws#faqs","content":" License‚Äã When creating an Oracle DB using Sleakops License Included (LI). Currently Bring Your Own License (BYOL) is not supported, however, contact support for more information.  How does SleakOps manage Oracle credentials?‚Äã When you create an Oracle dependency in SleakOps, it automatically generates a Vargroup for your database. This Variable Group securely stores the Oracle credentials and other important configuration details, such as the database endpoint and user access information. You'll be able of manage them from Vargroups section.  What is Multi-AZ deployment and should I enable it?‚Äã Multi-AZ (Availability Zone) deployment ensures high availability and failover support by replicating your database in another availability zone. It's recommended for production environments to prevent downtime. Keep in mind that it increases costs.  Can I change the Oracle version after the database is deployed?‚Äã No, the database engine version cannot be changed after deployment. You would need to create a new Oracle instance with the desired version and migrate your data. Or change it manually into the AWS Console.  What happens if I need more storage for my Oracle database?‚Äã You can adjust the storage size when configuring your database. If you need more storage after deployment, you can scale modifying the settings in AWS as at the moment SleakOps does not support it.  How do I create a Oracle database dump?‚Äã warning The client is only available for x86-64 Linux distributions. tip Follow this link to install the client To create a dump of your Oracle database, use the following command: exp ${ORACLE_USERNAME}/${ORACLE_PASSWORD}@${ORACLE_ENDPOINT}/${ORACLE_NAME} FILE=exp_file.dmp LOG=exp_file.log Replace ORACLE_USERNAME, ORACLE_ENDPOINT, ORACLE_NAME and ORACLE_PASSWORD with the appropriate values. For additional information on creating an Oracle dump, refer to the official Oracle documentation . Another option is creating it directly from the AWS Console and then import it. See Restoring to a DB instance.  How do I import an existent dump ?‚Äã warning The client is only available for x86-64 Linux distributions. tip Follow this link to install the client You can use a Oracle client installed on your local machine to import the dump. imp ${ORACLE_USERNAME}/${ORACLE_PASSWORD}@${ORACLE_ENDPOINT}/${ORACLE_NAME} FROMUSER=cust_schema TOUSER=cust_schema FILE=exp_file.dmp LOG=imp_file.log Replace ORACLE_USERNAME, ORACLE_ENDPOINT, ORACLE_NAME and ORACLE_PASSWORD with the appropriate values. For additional information on importing an Oracle dump, refer to the official Oracle documentation . Another option is creating it directly from the AWS Console and then import it. See Restoring to a DB instance.  What should I do if I encounter connection issues with my Oracle database?‚Äã Ensure the database endpoint, username, and password are correct.Verify that your security groups and firewall rules allow access.Ensure the database is running and has enough resources (CPU, memory). Otherwise, contact us.  info AWS documentation: Amazon RDS Oracle Documentation  ","version":"Next","tagName":"h2"},{"title":"Set up your Oracle‚Äã","type":1,"pageTitle":"AWS Oracle","url":"/project/dependency/oracle-aws#set-up-your-oracle","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add Oracle as a Dependency‚Äã","type":1,"pageTitle":"AWS Oracle","url":"/project/dependency/oracle-aws#1-add-oracle-as-a-dependency","content":" To integrate Oracle with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;Oracle&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your Oracle.‚Äã","type":1,"pageTitle":"AWS Oracle","url":"/project/dependency/oracle-aws#2-set-up-your-oracle","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionDatabase Engine Version\tSelect the specific version of the Oracle database engine you wish to use. This ensures compatibility with your application requirements. Example: 19.0.0.0.ru-2024-01.rur-2024-01.r1 Database Instance Class\tDefine the instance class that specifies the hardware configuration for your Oracle database. This controls CPU, memory, and network performance. Example: db.m6g.large, db.t3.medium. See AWS detail. Database Storage\tSpecify the amount of storage allocated for the database. Example: 100 GB, 500 GB. Username\tProvide the master username for the Oracle database. This is the main user with administrative privileges. Example: admin, root. Password\tPassword for the master user to access the database. Multi-Availability Zone\tEnable or disable Multi-AZ deployment. This ensures high availability and failover support by replicating the database across multiple availability zones. Recommended for production environments. Automated Backup\tConfigure automated backups for the Oracle database. This ensures data protection by enabling daily snapshots and transaction log backups. Set up the Backup Retention Period and the Backup Window. Recommended for production environments. Backup Retention Period\tSet the number of days to retain automated backups. Backup Window\tPeriod of time while the backup will be done.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable's name for your Oracle data base.‚Äã","type":1,"pageTitle":"AWS Oracle","url":"/project/dependency/oracle-aws#3-customize-your-variables-name-for-your-oracle-data-base","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS MySQL","type":0,"sectionRef":"#","url":"/project/dependency/mysql-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS MySQL","url":"/project/dependency/mysql-aws#faqs","content":" How does SleakOps manage MySQL credentials?‚Äã When you create a MySQL dependency in SleakOps, it automatically generates a Vargroup for your database. This Variable Group securely stores the MySQL credentials and other important configuration details, such as the database endpoint and user access information. You‚Äôll be able of manage them from Vargroups section.  What is Multi-AZ deployment and should I enable it?‚Äã Multi-AZ (Availability Zone) deployment ensures high availability and failover support by replicating your database in another availability zone. It‚Äôs recommended for production environments to prevent downtime. Keep in mind that it increases costs.  Can I change the MySQL version after the database is deployed?‚Äã No, the database engine version cannot be changed after deployment. You would need to create a new MySQL instance with the desired version and migrate your data. Or change it manually into the AWS Console.  What happens if I need more storage for my MySQL database?‚Äã You can adjust the storage size when configuring your database. If you need more storage after deployment, you can scale modifying the settings in AWS as at the moment SleakOps does not support it.  How do I create a MySQL database dump?‚Äã To create a dump of your MySQL database, use the following command: sh mysqldump -h MYSQL_ADDRESS -u MYSQL_USERNAME -p MYSQL_PASSWORD &gt; dump.sql Replace MYSQL_ADDRESS, MYSQL_USERNAME, and MYSQL_PASSWORD with the appropriate values. For additional information on creating a MySQL dump, refer to the official MySQL documentation. Another option is creating it directly from the AWS Console and then import it. See Restoring to a DB instance.  How do I import an existent dump using docker?‚Äã For more details: MySQL Dump Documentation To import a database dump into your MySQL RDS instance: Connect to the VPN: Ensure you are connected to the VPN of the AWS account where the RDS instance is located.Run Docker Container (Recommended): Install Docker on your local machine if not already installed.Run a MySQL Docker container with the following command: sh docker run -it --name mysql-container -v ./initial_data/:/tmp/data/ -e MYSQL_ROOT_PASSWORD=MYSQL_PASSWORD -d mysql bash Attach to the container‚Äôs terminal: sh docker exec -t -i mysql-container bash Import the dump file: sh mysql -h MYSQL_ADDRESS -u MYSQL_USERNAME -p &lt; /tmp/data/dump.sql Replace MYSQL_ADDRESS, MYSQL_USERNAME, and MYSQL_PASSWORD with your RDS instance details.  How do I import an existent dump to my local machine ?‚Äã Alternatively, you can use a MySQL client installed on your local machine to import the dump. mysql -h MYSQL_ADDRESS -u MYSQL_USERNAME -p MYSQL_PASSWORD &lt; /tmp/data/dump.sql   What should I do if I encounter connection issues with my MySQL database?‚Äã Ensure the database endpoint, username, and password are correct.Verify that your security groups and firewall rules allow access.Ensure the database is running and has enough resources (CPU, memory). Otherwise, contact us.  What is an RDS Read Replica?‚Äã An RDS Read Replica is a read-only copy of your primary database instance in Amazon RDS. It helps distribute read-heavy workloads and improves the performance and scalability of your database by offloading read operations from the primary database. RDS Read Replicas are ideal when you need to: Offload read-heavy operations from your primary instance.Scale your read operations as your application grows.Distribute database reads across multiple geographic locations.Have a backup solution that can quickly be promoted to a primary instance in case of failure. info Keep in mind that Read replicas have a delay performing updates.  How do I configure a Read Replica in SleakOps?‚Äã In SleakOps, when creating a read replica for your RDS database, you will need to provide the following information: Name of the replicaReplica Instance Class, which determines the instance type for the replica.Replica Publicly Accessible, to decide if the replica should have a public IP or be accessible only within your private network.  Can I delete a replica?‚Äã At the moment, the only way is to delete the dependency.  info AWS documentation: Amazon RDS MySQL Documentation  ","version":"Next","tagName":"h2"},{"title":"Set up your MySQL‚Äã","type":1,"pageTitle":"AWS MySQL","url":"/project/dependency/mysql-aws#set-up-your-mysql","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add MySQL as a Dependency‚Äã","type":1,"pageTitle":"AWS MySQL","url":"/project/dependency/mysql-aws#1-add-mysql-as-a-dependency","content":" To integrate MySQL with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;MySQL&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your MySQL.‚Äã","type":1,"pageTitle":"AWS MySQL","url":"/project/dependency/mysql-aws#2-set-up-your-mysql","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionDatabase Engine Version\tSelect the specific version of the MySQL database engine you wish to use. This ensures compatibility with your application requirements. Example: MySQL 8.0.2, MySQL 5.7.1 Database Instance Class\tDefine the instance class that specifies the hardware configuration for your MySQL database. This controls CPU, memory, and network performance. Example: db.m6g.large, db.t3.medium. See AWS detail. Database Storage\tSpecify the amount of storage allocated for the database. Example: 100 GB, 500 GB. Username\tProvide the master username for the MySQL database. This is the main user with administrative privileges. Example: admin, root. Password\tPassword for the master user to access the database. Multi-Availability Zone\tEnable or disable Multi-AZ deployment. This ensures high availability and failover support by replicating the database across multiple availability zones. Recommended for production environments. Automated Backup\tConfigure automated backups for the MySQL database. This ensures data protection by enabling daily snapshots and transaction log backups. Set up the Backup Retention Period and the Backup Window. Recommended for production environments. Backup Retention Period\tSet the number of days to retain automated backups. Backup Window\tPeriod of time while the backup will be done.  warning SleakOps allow the creation of replicas only during the creation of the dependency.  After that basic data, you need to decide if a replica will be created. To do that:  Into the form, look for the section Definition of RDS Read Replicas and click on + Add Item.Complete the following data:  Setting\tDescriptionName\tA name for the replica Replica Instance Class\tDefine the instance class that specifies the hardware configuration for your MySQL database. This controls CPU, memory, and network performance. Example: db.m6g.large, db.t3.medium. Replica Publicly Accessible\tDecide if the replica should have a public IP or be accessible only within your private network.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable's name for your MySQL data base.‚Äã","type":1,"pageTitle":"AWS MySQL","url":"/project/dependency/mysql-aws#3-customize-your-variables-name-for-your-mysql-data-base","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS Redis","type":0,"sectionRef":"#","url":"/project/dependency/redis-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS Redis","url":"/project/dependency/redis-aws#faqs","content":" What features make Redis a good choice for my application?‚Äã Redis offers several advanced features that make it suitable for a wide range of applications: Data Persistence: Redis can save data to disk, ensuring that information is not lost in the event of a restart.Advanced Data Structures: Redis supports more complex data structures than simple key-value stores, such as lists, sets, hashes, sorted sets, and more.High Availability: Through replication and automatic failover, Redis ensures that your application remains operational even if a node fails.Scalability: Redis can be scaled both vertically (with larger instances) and horizontally (using sharding and clusters).Pub/Sub Messaging: Redis offers native support for publish/subscribe messaging patterns, useful for building real-time applications.  What are the common use cases for Redis?‚Äã Redis is versatile and can be used in a variety of scenarios, including: Session Management: Redis is commonly used for storing user session data due to its low-latency data access and persistence features.Caching: Redis is ideal for caching frequently accessed data, reducing load on primary databases and improving response times.Real-Time Analytics: Redis's fast in-memory processing capabilities make it perfect for real-time analytics, leaderboards, and counters.Message Queues: With Redis‚Äôs pub/sub functionality, you can use it for messaging systems and event streaming.Job Queues: Redis is used for managing background job queues in large-scale applications.  How does Redis differ from Memcached?‚Äã Redis is more feature-rich than Memcached. Redis supports a variety of data structures like lists, sets, and hashes, while Memcached is limited to simple key-value pairs. Redis also supports data persistence and replication, making it suitable for applications where durability and high availability are critical. However, Memcached is typically more lightweight and faster for basic caching scenarios.  ","version":"Next","tagName":"h2"},{"title":"Set up your AWS Redis‚Äã","type":1,"pageTitle":"AWS Redis","url":"/project/dependency/redis-aws#set-up-your-aws-redis","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add AWS Redis as a Dependency‚Äã","type":1,"pageTitle":"AWS Redis","url":"/project/dependency/redis-aws#1-add-aws-redis-as-a-dependency","content":" To integrate Redis with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;AWS Redis&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your Redis database.‚Äã","type":1,"pageTitle":"AWS Redis","url":"/project/dependency/redis-aws#2-set-up-your-redis-database","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionNode Type\tInstance class that determines the performance and memory capacity of the Redis instance. Examples: cache.t3.micro, cache.m5.large, cache.r6g.large Port\tThe communication port used by Redis to interact with your application. Default: 6379 (can be customized)  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable names for your Redis.‚Äã","type":1,"pageTitle":"AWS Redis","url":"/project/dependency/redis-aws#3-customize-your-variable-names-for-your-redis","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"AWS S3 Bucket","type":0,"sectionRef":"#","url":"/project/dependency/s3bucket-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS S3 Bucket","url":"/project/dependency/s3bucket-aws#faqs","content":" When should I use Amazon S3?‚Äã You should use Amazon S3 when you need: Scalable storage for files, media, backups, or static assets.Highly durable and available storage for critical data.Data archiving solutions or disaster recovery.A cost-effective solution for storing large amounts of unstructured data.  How do I create a S3 dump?‚Äã To create a backup of your S3 bucket data, follow these steps: Ensure Admin Access: Verify that you have admin access to the AWS account where the S3 bucket is located.Install AWS CLI: Ensure AWS CLI is installed on your local machine. For installation instructions, refer to the official AWS CLI documentation .Configure AWS CLI: Ensure that your Access and Secret keys are configured in the default profile. If they are under a different profile, use the --profile PROFILE_NAME option with the commands.Run Backup Commands: ```sh aws sts assume-role --role-arn arn:aws:iam::ACCOUNT_ID:role/SleakopsAdminRole aws s3 sync s3://BUCKET_NAME /path/to/local/directory``` Replace ACCOUNT_ID, BUCKET_NAME, and /path/to/local/directory with your specific details.  Does SleakOps create Variable Groups for S3 dependencies?‚Äã Yes, when you configure an S3 bucket in SleakOps, it will automatically create a Variable Group. This securely stores the access keys and other sensitive information needed to manage and interact with your S3 bucket.  What is an S3 Access Control List (ACL)?‚Äã An S3 Access Control List (ACL) defines the permissions for who can access your S3 bucket and its contents. It controls the level of access granted to users, groups, or predefined AWS entities. You can choose an ACL that defines who can access the bucket and what level of permission they have. The available options are: private: Only the bucket owner has full access. (Default)public-read: Anyone can read the objects in the bucket.public-read-write: Anyone can read and write to the bucket.aws-exec-read: Grants read access to AWS services like CloudFront.authenticated-read: Grants read access to authenticated AWS users.log-delivery-write: Grants write access to the bucket for logging purposes. For further details, you can refer to AWS S3 ACL documentation .  What is Amazon CloudFront?‚Äã Amazon CloudFront is a Content Delivery Network (CDN) that speeds up the delivery of your static and dynamic content (like HTML, CSS, images, and videos) by caching it at edge locations closer to your users around the world.  Can I use a custom domain name with CloudFront?‚Äã Yes, SleakOps allows you to set a custom alias (subdomain) for your CloudFront distribution. For example, you could use cdn.mydomain.com as the URL for your CloudFront distribution instead of the default CloudFront URL.  What is a Custom Header in CloudFront?‚Äã Custom Headers allow you to include additional information in every request that CloudFront makes to your S3 bucket. This feature provides more control over how your S3 content is accessed and delivered by attaching specific metadata to the requests. SleakOps allows you to define custom headers for CloudFront, which will be included in all requests sent to your S3 bucket. This is useful for adding security measures, managing permissions, or tracking requests. custom_headers: - key: &quot;X-Custom-Header&quot; value: &quot;MyCustomValue&quot;   ","version":"Next","tagName":"h2"},{"title":"Set up your S3 Bucket‚Äã","type":1,"pageTitle":"AWS S3 Bucket","url":"/project/dependency/s3bucket-aws#set-up-your-s3-bucket","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add S3 Bucket as a Dependency‚Äã","type":1,"pageTitle":"AWS S3 Bucket","url":"/project/dependency/s3bucket-aws#1-add-s3-bucket-as-a-dependency","content":" To integrate S3 Bucket with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;S3 Bucket&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your S3 Bucket.‚Äã","type":1,"pageTitle":"AWS S3 Bucket","url":"/project/dependency/s3bucket-aws#2-set-up-your-s3-bucket","content":" When adding an S3 bucket as a dependency in SleakOps, you will be required to configure the following attributes:  Attribute\tDescriptionS3 Access Control List (ACL)\tSpecifies the level of access for the S3 bucket and its contents. Options: private, public-read, public-read-write, aws-exec-read, authenticated-read, log-delivery-write Enable CloudFront\tAllows you to enable Amazon CloudFront, a CDN for delivering S3 bucket content globally. Alias\tThe alias for the S3 bucket or CloudFront distribution (e.g., cdn.mydomain.com). Price Class\tDefines the set of global CloudFront edge locations used to serve content. Options: Use all edge locations (best performance), Use North America, Europe, Asia, Middle East, and Africa, Use only North America and Europe. Check AWS Price Class . Custom Headers\tCustom headers that CloudFront includes in all requests sent to the S3 bucket. - Key: The header key, such as X-Custom-Header. - Value: The value you wish to assign to the header, like MyCustomValue. Override\tSpecify whether CloudFront should override existing headers in requests from the origin.    After that basic data, if you activated the Cloudfront, you will be able of customize its headers. To do that:  Into the form, look for the field CloudFront custom headers and click on + Add Item.Complete Key and ValueYou can add as many as you need.    ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable names for your S3.‚Äã","type":1,"pageTitle":"AWS S3 Bucket","url":"/project/dependency/s3bucket-aws#3-customize-your-variable-names-for-your-s3","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes.    In this step you can change the name of the attributes in case it is needed but SleakOps completes the values automatically. After this step, your dependency is created. ","version":"Next","tagName":"h3"},{"title":"AWS PosgreSQL","type":0,"sectionRef":"#","url":"/project/dependency/postgresql-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS PosgreSQL","url":"/project/dependency/postgresql-aws#faqs","content":" How does SleakOps manage PostgreSQL credentials?‚Äã When you create a PostgreSQL dependency in SleakOps, it automatically generates a Vargroup for your database. This Variable Group securely stores the PostgreSQL credentials and other important configuration details, such as the database endpoint and user access information. You‚Äôll be able of manage them from Vargroups section.  What is Multi-AZ deployment and should I enable it?‚Äã Multi-AZ (Availability Zone) deployment ensures high availability and failover support by replicating your database in another availability zone. It‚Äôs recommended for production environments to prevent downtime. Keep in mind that it increases costs.  Can I change the PostgreSQL version after the database is deployed?‚Äã No, the database engine version cannot be changed after deployment. You would need to create a new PostgreSQL instance with the desired version and migrate your data. Or change it manually into the AWS Console.  What happens if I need more storage for my PostgreSQL database?‚Äã You can adjust the storage size when configuring your database. If you need more storage after deployment, SleakOps allows you to scale the storage size without downtime.  How do I create a PostgreSQL database dump?‚Äã To create a dump of your PostgreSQL database: Run the pg_dump Command: sh pg_dump -h POSTGRESQL_ADDRESS -U POSTGRESQL_USERNAME -W &gt; dump.sql Replace POSTGRESQL_ADDRESS, POSTGRESQL_USERNAME, and dump.sql with the appropriate values. 2. Consult Documentation: For more information on how to create a dump, refer to the official PostgreSQL documentation..  How do I import an existent dump using docker?‚Äã To import a database dump into your PostgreSQL RDS instance: Connect to the VPN: Ensure you are connected to the VPN of the AWS account where the RDS instance is located.Run Docker Container (Recommended): Install Docker on your local machine if not already installed.Run a PostgreSQL Docker container with the following command: sh docker run -it --name postgresql-container -v ./initial_data/:/tmp/data/ -e POSTGRESQL_ROOT_PASSWORD=POSTGRESQL_PASSWORD -d postgres bash Attach to the container‚Äôs terminal: sh docker exec -t -i postgresql-container bash Import the dump file: pg_dump -h POSTGRESQL_ADDRESS -U POSTGRESQL_USERNAME -W &lt; /tmp/data/dump.sql Replace POSTGRESQL_ADDRESS, POSTGRESQL_USERNAME, and dump.sql with your specific details.  How do I import an existent dump to my local machine ?‚Äã Alternatively, you can use a PostgreSQL client installed on your local machine to import the dump. sh psql -h POSTGRESQL_ADDRESS -U POSTGRESQL_USERNAME -W -f /tmp/data/dump.sql   What should I do if I encounter connection issues with my PostgreSQL database?‚Äã Check the following: Ensure the database endpoint, username, and password are correct.Verify that your firewall rules allow access.Ensure the database is running and has enough resources (CPU, memory). Otherwise, contact us.  What is an RDS Read Replica?‚Äã An RDS Read Replica is a read-only copy of your primary database instance in Amazon RDS. It helps distribute read-heavy workloads and improves the performance and scalability of your database by offloading read operations from the primary database. RDS Read Replicas are ideal when you need to: Offload read-heavy operations from your primary instance.Scale your read operations as your application grows.Distribute database reads across multiple geographic locations.Have a backup solution that can quickly be promoted to a primary instance in case of failure. info Keep in mind that Read replicas have a delay performing updates.  How do I configure a Read Replica in SleakOps?‚Äã In SleakOps, when creating a read replica for your RDS database, you will need to provide the following information: Name of the replicaReplica Instance Class, which determines the instance type for the replica.Replica Publicly Accessible, to decide if the replica should have a public IP or be accessible only within your private network.  Can I delete a replica?‚Äã At the moment, the only way is to delete the dependency.  info AWS documentation: Amazon RDS PostgreSQL Documentation  ","version":"Next","tagName":"h2"},{"title":"Set up your PostgreSQL‚Äã","type":1,"pageTitle":"AWS PosgreSQL","url":"/project/dependency/postgresql-aws#set-up-your-postgresql","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add PostgreSQL as a Dependency‚Äã","type":1,"pageTitle":"AWS PosgreSQL","url":"/project/dependency/postgresql-aws#1-add-postgresql-as-a-dependency","content":" To integrate PostgreSQL with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;PostgreSQL&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your PostgreSQL database.‚Äã","type":1,"pageTitle":"AWS PosgreSQL","url":"/project/dependency/postgresql-aws#2-set-up-your-postgresql-database","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionDatabase Engine Version\tSelect the specific version of the MySQPostgreSQLL database engine you wish to use. This ensures compatibility with your application requirements. Example: PostgreSQL 14.9, PostgreSQL 16.5 Database Instance Class\tDefine the instance class that specifies the hardware configuration for your PostgreSQL database. This controls CPU, memory, and network performance. Example: db.m6g.large, db.t3.medium. See AWS detail. Database Storage\tSpecify the amount of storage allocated for the database. Example: 100 GB, 500 GB. Username\tProvide the master username for the PostgreSQL database. This is the main user with administrative privileges. Example: admin, root. Password\tPassword for the master user to access the database. Multi-Availability Zone\tEnable or disable Multi-AZ deployment. This ensures high availability and failover support by replicating the database across multiple availability zones. Recommended for production environments. Automated Backup\tConfigure automated backups for the PostgreSQL database. This ensures data protection by enabling daily snapshots and transaction log backups. Set up the Backup Retention Period and the Backup Window. Recommended for production environments. Backup Retention Period\tSet the number of days to retain automated backups. Backup Window\tPeriod of time while the backup will be done.  warning SleakOps allow the creation of replicas only during the creation of the dependency.  After that basic data, you need to decide if a replica will be created. To do that:  Into the form, look for the section Definition of RDS Read Replicas and click on + Add Item.Complete the following data:  Setting\tDescriptionName\tA name for the replica Replica Instance Class\tDefine the instance class that specifies the hardware configuration for your PostgreSQL database. This controls CPU, memory, and network performance. Example: db.m6g.large, db.t3.medium. Replica Publicly Accessible\tDecide if the replica should have a public IP or be accessible only within your private network.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable‚Äôs name for your PostgreSQL data base.‚Äã","type":1,"pageTitle":"AWS PosgreSQL","url":"/project/dependency/postgresql-aws#3-customize-your-variables-name-for-your-postgresql-data-base","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/project/deployment","content":"","keywords":"","version":"Next"},{"title":"Deploying your ProjectEnv‚Äã","type":1,"pageTitle":"Deployment","url":"/project/deployment#deploying-your-projectenv","content":" Upon creating a Deployment, we fetch the image corresponding to the Build to be deployed. Before this, we execute the Helm Release in the appropriate cluster namespace. This Helm release includes necessary Kubernetes services, ingresses, workers, and other services.  Various methods exist to generate a Deployment. These are outlined below. Note that we force certain Deployments. For details, refer to More on Deployment.  Workloads: Provides a switcher allowing you to decide whether to run a new Deployment.VariableGroup: Operates similarly to Workloads but doesn't create a new Release. Instead, it only updates the values of the Deployment.Dependency: Triggers a Deployment automatically. Delve deeper into this at More on Deployment.    ","version":"Next","tagName":"h2"},{"title":"Manual Deployment‚Äã","type":1,"pageTitle":"Deployment","url":"/project/deployment#manual-deployment","content":" If you skip deploying your changes immediately, or if your modification doesn't enforces a deployment, you have three methods to execute a Deployment:  Build section: Utilizing the Deploy button, you can determine which Build to deploy.    Unpublished banner‚Äã    Unpublished Changes Banner: This banner is shown when there's content pending that was not yet deployed on the cluster. Through this banner, you have the choice to deploy only the VariableGroups or if you want to deploy everything, including the Services in the 'draft' state.    Via the CLI . ","version":"Next","tagName":"h3"},{"title":"More on Deployments","type":0,"sectionRef":"#","url":"/project/deployment/more_on_deployment","content":"","keywords":"","version":"Next"},{"title":"How SleakOps Handles Deployments‚Äã","type":1,"pageTitle":"More on Deployments","url":"/project/deployment/more_on_deployment#how-sleakops-handles-deployments","content":" To execute a deployment, SleakOps utilizes the Build images stored in your project's image repository (AWS ECR), which are created either with the ProjectEnv entity during the Initial Build or with the creation of a Build entity that pushes to the ECR. Whenever a Deployment is initiated, we fetch the image corresponding to the designated Build.  The next phase involves constructing and deploying the Helm chart. This is accomplished using generally purpose-built templates. Once constructed, we upload the Helm chart to the same ECR utilized for the Build images and proceed to deploy a Helm Release into the Kubernetes cluster, specifically within the ProjectEnv namespace.  info All these resources reside in your own AWS Accounts. Sleakops does not exclusively store any data.  ","version":"Next","tagName":"h2"},{"title":"Forced Deployments‚Äã","type":1,"pageTitle":"More on Deployments","url":"/project/deployment/more_on_deployment#forced-deployments","content":" Forced Deployment Hace in mind that under certain circumstances, SleakOps forces a Deploy.  While multiple methods for generating a Deployment were highlighted in the primary Deployment documentation, it's crucial to understand that SleakOps sometimes enforces Deployments. The rationale behind this is to optimize uptime, safeguard the current state of the deployed infrastructure, and mitigate potential service downtimes on the Cluster. This imperative arises because Helm templates should always synchronize with the Kubernetes Secrets present in the namespace to avert deployment failures.  As you may already know, if it's not a 'forced' deployment, you'll be presented with an option (switcher) to determine if you wish to deploy your modifications. Deployments are forced in the following scenarios.  Workloads Alias Configuration Changes: A Deployment is forced if any alterations are made to the 'alias' configuration.Dependency: Always forces a Deployment to synchronize its associated VariableGroup state with the templates of the Helm Chart ensuring that Services operation is not affected.VariableGroup Deletion: Same case as Dependency deletion. ","version":"Next","tagName":"h3"},{"title":"AWS SQS","type":0,"sectionRef":"#","url":"/project/dependency/sqs-aws","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"AWS SQS","url":"/project/dependency/sqs-aws#faqs","content":" What is the difference between a Standard Queue and a FIFO Queue?‚Äã A Standard Queue supports high throughput with at-least-once message delivery. Message ordering is not guaranteed but it's ideal for scenarios where message order isn't critical.A FIFO Queue ensures message ordering and exactly-once delivery. It's suitable for applications where message order is crucial.  What is Multi-AZ deployment and should I enable it?‚Äã Multi-AZ (Availability Zone) deployment ensures high availability and failover support by replicating your database in another availability zone. It‚Äôs recommended for production environments to prevent downtime. Keep in mind that it increases costs.  What is a Dead-Letter Queue (DLQ), and when should I use it?‚Äã A DLQ is a secondary queue where messages that can't be processed successfully after a certain number of attempts are sent. You should configure a DLQ to help with error handling and to prevent message loss. See AWS SQS DLQ .  What is Deduplication in a FIFO Queue?‚Äã In an SQS FIFO Queue, deduplication ensures that duplicate messages are automatically removed, preserving strict message ordering.  ","version":"Next","tagName":"h2"},{"title":"Set up your SQS‚Äã","type":1,"pageTitle":"AWS SQS","url":"/project/dependency/sqs-aws#set-up-your-sqs","content":" ","version":"Next","tagName":"h2"},{"title":"1. Add SQS as a Dependency‚Äã","type":1,"pageTitle":"AWS SQS","url":"/project/dependency/sqs-aws#1-add-sqs-as-a-dependency","content":" To integrate SQS with SleakOps:  In the SleakOps console, go to the &quot;Dependencies&quot; sectionChoose &quot;SQS&quot; from the list of available dependency types. For more detail see Dependencies: Integrating Databases, Caching, and Messaging Services.  ","version":"Next","tagName":"h3"},{"title":"2. Set up your SQS database.‚Äã","type":1,"pageTitle":"AWS SQS","url":"/project/dependency/sqs-aws#2-set-up-your-sqs-database","content":" You will access the following form:    Here the parameters that SleakOps allows you to customize during the creation:  Attribute\tDescriptionFifo Queue\tSelect if the queue is standard or fifo. Read more on FAQs Fifo Deduplication\tSelect if deduplication is activated. Read more on FAQs Message Delay Seconds\tSeconds of delivery delay to messages on queue. Message Max Size\tBytes maximum limit per message Message Retention Seconds\tSeconds that a message is retained by SQS Receive WaitTime Seconds\tThe time for which a ReceiveMessage call will wait for a message to arrive (long polling) before returning Visibility Timeout Seconds\tSeconds that a message is retained by SQS Dead Letter Queue\tSelect if dead letter queue is enabled. Read more on FAQs  By activating the Dead Letter Queue feature, you need to complete the same information for the second queue.  ","version":"Next","tagName":"h3"},{"title":"3. Customize your variable names for your SQS.‚Äã","type":1,"pageTitle":"AWS SQS","url":"/project/dependency/sqs-aws#3-customize-your-variable-names-for-your-sqs","content":" As explained, when a dependency is created, SleakOps generates a vargroup to hold all the needed attributes. In this step you can change the name of the attributes in case it is needed. SleakOps completes the values automatically. After this step, your dependency is created.   ","version":"Next","tagName":"h3"},{"title":"Release","type":0,"sectionRef":"#","url":"/project/deployment/release","content":"","keywords":"","version":"Next"},{"title":"What is a Release?‚Äã","type":1,"pageTitle":"Release","url":"/project/deployment/release#what-is-a-release","content":" In Sleakops, a release represents a deployable state of all the Workloads (web services, workers, cron jobs, hooks) of a project in an environment.  ","version":"Next","tagName":"h2"},{"title":"Release Creation‚Äã","type":1,"pageTitle":"Release","url":"/project/deployment/release#release-creation","content":" Sleakops administers releases for you. Every time you modify, delete, or add a web service, worker, hook, or cron job, Sleakops gives you the option to publish the changes. Each time you publish those changes, Sleakops creates a new release with auto-incremented versions.  ","version":"Next","tagName":"h2"},{"title":"Helm Chart Resources‚Äã","type":1,"pageTitle":"Release","url":"/project/deployment/release#helm-chart-resources","content":" Web Service:‚Äã  A Kubernetes deploymentA Kubernetes serviceA Kubernetes HPA (Horizontal Pod Autoscaler)A Kubernetes ingress  The ingress generates its hosts using &lt;service_name&gt;.&lt;environment_name&gt;.&lt;organization_name&gt;.&lt;yourdomain.com&gt;  Worker:‚Äã  A Kubernetes deploymentA Kubernetes HPA  Hook:‚Äã  A Kubernetes job  This job uses Kubernetes hooks to start.  Cron Job:‚Äã  A Kubernetes cron job ","version":"Next","tagName":"h3"},{"title":"VariableGroups","type":0,"sectionRef":"#","url":"/project/vargroup","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"VariableGroups","url":"/project/vargroup#faqs","content":" Can I edit a VariableGroup created by SleakOps?‚Äã Yes, you can access and manage the Variable Group for your MySQL instance from the SleakOps console. You can modify values such as usernames, passwords, or any other environment-specific credentials.  How are Vargroups used by my applications?‚Äã Vargroups are securely injected into your application's environment when it is deployed. Your applications can access these credentials and other variables without exposing sensitive information in the code.  How does SleakOps ensure the security of Vargroups?‚Äã SleakOps securely stores vargroups as Kubernetes secrets inside your EKS cluster. Access is controlled via Kubernetes Service Accounts, ensuring that only authorized components can access the sensitive information.  Can I delete a Variable Group?‚Äã Yes, they can be deleted or updated as needed. However, be cautious when deleting them, as it may disrupt your application.  What is the difference between a Global and a Workload-Scoped Vargroup?‚Äã Global: Available to all workload within the namespace. It is created without selecting a specific workload. To create them select ‚Äúglobal‚ÄùWorkload-Scoped Variable Group: Only applies to the selected workload within the project and environment. It overrides global var group values if they have the same key.  What happens if there are duplicate keys in different Vargroups?‚Äã If duplicate keys exist across different var groups: If the key exists in both a global and a workload-scoped var group, the workload-scoped value takes precedence.If two global vargroups have the same key, the most recently created one will be used.  ","version":"Next","tagName":"h2"},{"title":"Create a VarGroup‚Äã","type":1,"pageTitle":"VariableGroups","url":"/project/vargroup#create-a-vargroup","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Vargroup section‚Äã","type":1,"pageTitle":"VariableGroups","url":"/project/vargroup#1-navigate-to-create-vargroup-section","content":" Into the Left Pane, access Vargroups option under Projects and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and complete the needed attributes‚Äã","type":1,"pageTitle":"VariableGroups","url":"/project/vargroup#2-select-a-project-and-complete-the-needed-attributes","content":" Complete the following attributes to create a new vargroup:  Attribute\tDescriptionProject\tThe specific application or workload within SleakOps. Determines the scope of the variable group. Workload\tA microservice or component within the project. If selected, the vargroup is limited to it; otherwise, by selecting global it'll be accessible into the namespace. Name\tA unique identifier for the var group, used to differentiate it within the project. Should be descriptive of the group's purpose. Deploy\tEnable this option if you want SleakOps to automatically publish and deploy your workloads into the project.  info If you choose to add the argument using the text option: Each argument should be added on a new line, separated by an equal sign (=), with no extra spaces. ARGUMENT_NAME = VALUE ARGUMENT_TWO = VALUE ARGUMENT_ONE = VALUE    Submit to create and Deploy your vargroup. ","version":"Next","tagName":"h3"},{"title":"Volumes","type":0,"sectionRef":"#","url":"/project/volumes","content":"Volumes In the context of Sleakops, volumes refer to AWS/Kubernetes storage resources attached to a cluster. They serve as general storage for specified containers. Pods can only interact with these volumes if they are explicitly attached to them. You can define the volumes in the Project form: Whenever a volume is needed, Sleakops deploys an EFS CSI Driver within the cluster. This allows every Project to have its own unique volume mount, storing folders based on the paths you specify. A practical use-case for Volumes is when you want all your pods to access the same files, such as a shared folder that requires its content to be persistent. For more details on how EFS is used for volumes, refer to the EFS documentation.","keywords":"","version":"Next"},{"title":"Workloads","type":0,"sectionRef":"#","url":"/project/workload","content":"","keywords":"","version":"Next"},{"title":"Which workload type is right for me?‚Äã","type":1,"pageTitle":"Workloads","url":"/project/workload#which-workload-type-is-right-for-me","content":"   Web Service: Choose this if you need your application or service to be available 24/7 to respond to HTTP requests.Worker: Use this for background processing tasks, such as message queues or data pipelines, with no direct HTTP interaction.CronJob: Ideal for recurring maintenance or reporting tasks scheduled at specific times.Job: Suitable for one-time or on-demand tasks (e.g., manual database migrations).Hook: Perfect if you want to automate certain actions (like database migrations or analytics) on every deployment. ","version":"Next","tagName":"h2"},{"title":"Job","type":0,"sectionRef":"#","url":"/project/workload/job","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Job","url":"/project/workload/job#faqs","content":" How can I configure memory and CPU settings for my Job?‚Äã You can configure the CPU Request and CPU Limit values to set the minimum and maximum CPU resources each instance in your cluster can use. Similarly, you set Memory Request and Memory Limit for memory allocation per instance.  ","version":"Next","tagName":"h2"},{"title":"Lets add a Job for your Project‚Äã","type":1,"pageTitle":"Job","url":"/project/workload/job#lets-add-a-job-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Job section‚Äã","type":1,"pageTitle":"Job","url":"/project/workload/job#1-navigate-to-create-job-section","content":" Into the Left Pane, access Workloads. Then select the Job tab and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and a Name for the job‚Äã","type":1,"pageTitle":"Job","url":"/project/workload/job#2-select-a-project-and-a-name-for-the-job","content":" Start with the basic information, complete these attributes and click Next to continue.  Attribute\tDescriptionName\tIdentify your job. Project\tSelect between the existent projects. Command\tThe command that runs the job. Image\tBy default the job usage the image of your project, but you can override that with another Image tag\tYou can specify the tag of image.  Once those attributes are completed, click the Next button to move forward.    ","version":"Next","tagName":"h3"},{"title":"3. Finish the set up‚Äã","type":1,"pageTitle":"Job","url":"/project/workload/job#3-finish-the-set-up","content":" This step outlines the key attributes for configuring the resources of a Job in SleakOps, allowing for flexible management of CPU, memory, and scaling behaviors.  Attribute\tDescriptionCPU Request\tThe minimum amount of CPU resources allocated for each instance in the cluster. This ensures that each instance always has this amount of CPU available. CPU Limit\tThe maximum CPU resources that each instance in the cluster can use. This cap helps prevent any single instance from consuming too much CPU. Memory Request\tThe minimum amount of memory allocated for each instance in the cluster. This guarantees that the instance has enough memory to operate efficiently. Memory Limit\tThe maximum amount of memory each instance in the cluster can utilize. It limits the memory usage to prevent any single instance from overconsuming resources.    Submit to create and Deploy your job. ","version":"Next","tagName":"h3"},{"title":"Cronjobs","type":0,"sectionRef":"#","url":"/project/workload/cronjob","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#faqs","content":" How can I configure memory and CPU settings for my Cronjob?‚Äã You can configure the CPU Request and CPU Limit values to set the minimum and maximum CPU resources each instance in your cluster can use. Similarly, you set Memory Request and Memory Limit for memory allocation per instance.  ","version":"Next","tagName":"h2"},{"title":"Lets add a Cronjob for your Project‚Äã","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#lets-add-a-cronjob-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Cronjob section‚Äã","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#1-navigate-to-create-cronjob-section","content":" Into the Left Pane, access Workloads. Then select the Cronjob tab and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and a Name for the cronjob‚Äã","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#2-select-a-project-and-a-name-for-the-cronjob","content":" Start with the basic information, complete these attributes and click Next to continue.  Attribute\tDescriptionName\tIdentify your cronjob. Project\tSelect between the existent projects. Command\tThe command that runs the cronjob.  Once those attributes are completed, click the Next button to move forward.    ","version":"Next","tagName":"h3"},{"title":"3. Define the periodicity‚Äã","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#3-define-the-periodicity","content":" Select how will be your connection and click on Next.  Attribute\tDescriptionCrontab\tCron expresion to determine the schedule to execute the cronjob    ","version":"Next","tagName":"h3"},{"title":"5. Finish the set up‚Äã","type":1,"pageTitle":"Cronjobs","url":"/project/workload/cronjob#5-finish-the-set-up","content":" This step outlines the key attributes for configuring the resources of a Cronjob in SleakOps, allowing for flexible management of CPU, memory, and scaling behaviors.  Attribute\tDescriptionCPU Request\tThe minimum amount of CPU resources allocated for each instance in the cluster. This ensures that each instance always has this amount of CPU available. CPU Limit\tThe maximum CPU resources that each instance in the cluster can use. This cap helps prevent any single instance from consuming too much CPU. Memory Request\tThe minimum amount of memory allocated for each instance in the cluster. This guarantees that the instance has enough memory to operate efficiently. Memory Limit\tThe maximum amount of memory each instance in the cluster can utilize. It limits the memory usage to prevent any single instance from overconsuming resources.    Submit to create and Deploy your cronjob. ","version":"Next","tagName":"h3"},{"title":"Web Service","type":0,"sectionRef":"#","url":"/project/workload/webservice","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#faqs","content":" What is the difference between Public, Private, and Internal service schemas?‚Äã Public: Accessible over the internet and is open to anyone.Private: Restricted access, available only when connected to the VPN.Internal: Only accessible within the same Kubernetes cluster and is used for internal communication between services.  How do I set up auto-scaling for my Web Service?‚Äã To enable auto-scaling, you can set the Autoscaling option to enabled and define the Memory Target and CPU Target. These targets determine the resource usage thresholds that trigger auto-scaling. You must also specify the minimum and maximum number of replicas to be maintained when auto-scaling is enabled.  What are the default success codes for a Web Service, and can I change them?‚Äã The default success code is 200, indicating the service is healthy. You can change this code based on your application‚Äôs requirements, as some services might return different success codes based on specific actions.  What happens if my health check fails repeatedly?‚Äã If the health check fails consecutively and reaches the Failure Threshold (default is 60), the service is marked as unhealthy, and Kubernetes might restart or terminate the service instance to attempt a recovery.  How can I configure memory and CPU settings for my Web Service?‚Äã You can configure the CPU Request and CPU Limit values to set the minimum and maximum CPU resources each instance in your cluster can use. Similarly, you set Memory Request and Memory Limit for memory allocation per instance.  What are some best practices when configuring a Web Service in SleakOps?‚Äã Always set a minimum of 2 replicas to avoid downtime.Ensure your health check paths and success codes are correctly configured to reflect the true health of your service.Use auto-scaling where possible to optimize resources dynamically based on demand.Review and set memory and CPU usage targets appropriately to prevent overloading your infrastructure.  What should I do if my service shows response times longer than 10 seconds?‚Äã Long response times may indicate issues such as resource constraints, application inefficiencies, or network problems. You should check your service logs, ensure your resources (CPU, memory) are adequately allocated, and review your application code for potential optimizations.  How can I deploy my static web service?‚Äã At the moment, Sleakops doesn‚Äôt natively support static sites, but you can still deploy them using the same flow as other sites, by containerizing them with a web server like Nginx. Below is a simple example of a Dockerfile and the corresponding nginx.conf to serve your static content. FROM node:20.11.0-alpine AS base WORKDIR /app FROM base AS build ARG BACKEND_URL WORKDIR /app COPY package.json package-lock.json ./ RUN npm install COPY . ./ RUN npm run build FROM nginx:1.25.3-alpine AS production COPY --from=build /app/config/nginx.conf /etc/nginx/conf.d/default.conf COPY --from=build /app/dist /usr/share/nginx/html EXPOSE 80 CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] And here is a sample config/nginx.conf: server { listen 80; location = /health { access_log off; add_header 'Content-Type' 'application/json'; return 200 '{&quot;status&quot;:&quot;OK&quot;}'; } location / { root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html =404; add_header Last-Modified $date_gmt; add_header Cache-Control 'no-store, no-cache'; if_modified_since off; expires off; etag off; } } Using this Docker-based approach, you can serve your static site with Nginx, all within a container.  ","version":"Next","tagName":"h2"},{"title":"Lets add a Web Service for your Project‚Äã","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#lets-add-a-web-service-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Web Service section‚Äã","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#1-navigate-to-create-web-service-section","content":" Into the Left Pane, access Workloads. Then select the Web Services tab and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and a Name for the web service‚Äã","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#2-select-a-project-and-a-name-for-the-web-service","content":" Start with the basic information, complete these attributes and click Next to continue.  Attribute\tDescriptionName\tIdentify your web service. Project\tSelect between the existent projects. Command\tThe command that runs the service. Port\tThe port number where the service runs. Default: 8000  Once those attributes are completed, click the Next button to move forward.    ","version":"Next","tagName":"h3"},{"title":"3. Define the connection‚Äã","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#3-define-the-connection","content":" Select how will be your connection and click on Next.  Attribute\tDescriptionService Schema\tDefines the accessibility of the service: public, private, or internal. URL\tThe URL assigned to the service based on the environment and project settings. Format: name.myenv.sleakops.com.    ","version":"Next","tagName":"h3"},{"title":"4. Specify your workload settings‚Äã","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#4-specify-your-workload-settings","content":" You‚Äôll see the following for to specify the conditions.    Attribute\tDescriptionPath\tThe path where Kubernetes checks if the web service is operational. Default: / Success Code\tThe HTTP success code indicating the service‚Äôs health. Default: 200. Initial Delay Seconds\tNumber of seconds after startup before health checks begin. Default: 10. Timeout Seconds\tNumber of seconds after startup before health checks begin. Default: 1. Period Seconds\tInterval (in seconds) between each health check probe. Default: 5. Success Threshold\tMinimum number of consecutive successes required for the probe to be considered successful after it has failed. Default: 1. Failure Threshold\tNumber of consecutive failures before the probe is considered to have failed. Default: 60.  Once those attributes are completed, click the Next button to move to the next step.  ","version":"Next","tagName":"h3"},{"title":"5. Finish the set up‚Äã","type":1,"pageTitle":"Web Service","url":"/project/workload/webservice#5-finish-the-set-up","content":" This step outlines the key attributes for configuring the resources of a Web Service in SleakOps, allowing for flexible management of CPU, memory, and scaling behaviors.  Attribute\tDescriptionCPU Request\tThe minimum amount of CPU resources allocated for each instance in the cluster. This ensures that each instance always has this amount of CPU available. CPU Limit\tThe maximum CPU resources that each instance in the cluster can use. This cap helps prevent any single instance from consuming too much CPU. Memory Request\tThe minimum amount of memory allocated for each instance in the cluster. This guarantees that the instance has enough memory to operate efficiently. Memory Limit\tThe maximum amount of memory each instance in the cluster can utilize. It limits the memory usage to prevent any single instance from overconsuming resources. Autoscaling\tToggle to enable or disable auto-scaling. When enabled, it allows the service to adjust the number of replicas based on demand and resource usage. CPU Target\tThe CPU usage percentage target that initiates auto-scaling. If usage exceeds this target, additional replicas may be deployed to balance the load. Memory Target\tThe memory usage percentage target that triggers auto-scaling adjustments. When instances exceed this target, the system scales up to accommodate demand. Replicas Min\tThe minimum number of replicas to maintain when auto-scaling is active. A minimum of 2 replicas ensures high availability and prevents downtime. Replicas Max\tThe maximum number of replicas that can be deployed when auto-scaling is enabled. It sets an upper limit on the number of instances to avoid over-provisioning.    Submit to create and Deploy your web service. ","version":"Next","tagName":"h3"},{"title":"Hooks","type":0,"sectionRef":"#","url":"/project/workload/hook","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#faqs","content":" How can I configure memory and CPU settings for my Hook?‚Äã You can configure the CPU Request and CPU Limit values to set the minimum and maximum CPU resources each instance in your cluster can use. Similarly, you set Memory Request and Memory Limit for memory allocation per instance.  ","version":"Next","tagName":"h2"},{"title":"Lets add a Hook for your Project‚Äã","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#lets-add-a-hook-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Hook section‚Äã","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#1-navigate-to-create-hook-section","content":" Into the Left Pane, access Workloads. Then select the Hook tab and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and a Name for the hook‚Äã","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#2-select-a-project-and-a-name-for-the-hook","content":" Start with the basic information, complete these attributes and click Next to continue.  Attribute\tDescriptionName\tIdentify your web service. Project\tSelect between the existent projects. Command\tThe command that the workload runs.  Once those attributes are completed, click the Next button to move forward.    ","version":"Next","tagName":"h3"},{"title":"3. Define the deploy event‚Äã","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#3-define-the-deploy-event","content":" Select when your hook will execute and click Next.  Attribute\tDescriptionEvent\tDefine when execute the hook. Check available events    ","version":"Next","tagName":"h3"},{"title":"5. Finish the set up‚Äã","type":1,"pageTitle":"Hooks","url":"/project/workload/hook#5-finish-the-set-up","content":" This step outlines the key attributes for configuring the resources of a Hook in SleakOps, allowing for flexible management of CPU, memory, and scaling behaviors.  Attribute\tDescriptionCPU Request\tThe minimum amount of CPU resources allocated for each instance in the cluster. This ensures that each instance always has this amount of CPU available. CPU Limit\tThe maximum CPU resources that each instance in the cluster can use. This cap helps prevent any single instance from consuming too much CPU. Memory Request\tThe minimum amount of memory allocated for each instance in the cluster. This guarantees that the instance has enough memory to operate efficiently. Memory Limit\tThe maximum amount of memory each instance in the cluster can utilize. It limits the memory usage to prevent any single instance from overconsuming resources.    Submit to create and Deploy your hook. ","version":"Next","tagName":"h3"},{"title":"Worker","type":0,"sectionRef":"#","url":"/project/workload/worker","content":"","keywords":"","version":"Next"},{"title":"FAQs‚Äã","type":1,"pageTitle":"Worker","url":"/project/workload/worker#faqs","content":" How do I set up auto-scaling for my Worker?‚Äã To enable auto-scaling, you can set the Autoscaling option to enabled and define the Memory Target and CPU Target. These targets determine the resource usage thresholds that trigger auto-scaling. You must also specify the minimum and maximum number of replicas to be maintained when auto-scaling is enabled.  How can I configure memory and CPU settings for my Worker?‚Äã You can configure the CPU Request and CPU Limit values to set the minimum and maximum CPU resources each instance in your cluster can use. Similarly, you set Memory Request and Memory Limit for memory allocation per instance.  ","version":"Next","tagName":"h2"},{"title":"Lets add a Worker for your Project‚Äã","type":1,"pageTitle":"Worker","url":"/project/workload/worker#lets-add-a-worker-for-your-project","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to create Worker section‚Äã","type":1,"pageTitle":"Worker","url":"/project/workload/worker#1-navigate-to-create-worker-section","content":" Into the Left Pane, access Workloads. Then select the Worker tab and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Select a Project and a Name for the worker‚Äã","type":1,"pageTitle":"Worker","url":"/project/workload/worker#2-select-a-project-and-a-name-for-the-worker","content":" Start with the basic information, complete these attributes and click Next to continue.  Attribute\tDescriptionName\tIdentify your worker. Project\tSelect between the existent projects. Command\tThe command that runs the worker.  Once those attributes are completed, click the Next button to move forward.    ","version":"Next","tagName":"h3"},{"title":"3. Finish the set up‚Äã","type":1,"pageTitle":"Worker","url":"/project/workload/worker#3-finish-the-set-up","content":" This step outlines the key attributes for configuring the resources of a Worker in SleakOps, allowing for flexible management of CPU, memory, and scaling behaviors.  Attribute\tDescriptionCPU Request\tThe minimum amount of CPU resources allocated for each instance in the cluster. This ensures that each instance always has this amount of CPU available. CPU Limit\tThe maximum CPU resources that each instance in the cluster can use. This cap helps prevent any single instance from consuming too much CPU. Memory Request\tThe minimum amount of memory allocated for each instance in the cluster. This guarantees that the instance has enough memory to operate efficiently. Memory Limit\tThe maximum amount of memory each instance in the cluster can utilize. It limits the memory usage to prevent any single instance from overconsuming resources. Autoscaling\tToggle to enable or disable auto-scaling. When enabled, it allows the workload to adjust the number of replicas based on demand and resource usage. CPU Target\tThe CPU usage percentage target that initiates auto-scaling. If usage exceeds this target, additional replicas may be deployed to balance the load. Memory Target\tThe memory usage percentage target that triggers auto-scaling adjustments. When instances exceed this target, the system scales up to accommodate demand. Replicas Min\tThe minimum number of replicas to maintain when auto-scaling is active. A minimum of 2 replicas ensures high availability and prevents downtime. Replicas Max\tThe maximum number of replicas that can be deployed when auto-scaling is enabled. It sets an upper limit on the number of instances to avoid over-provisioning.    Submit to create and Deploy your worker. ","version":"Next","tagName":"h3"},{"title":"Providers","type":0,"sectionRef":"#","url":"/provider","content":"","keywords":"","version":"Next"},{"title":"Let's create your provider on SleakOps‚Äã","type":1,"pageTitle":"Providers","url":"/provider#lets-create-your-provider-on-sleakops","content":" ","version":"Next","tagName":"h2"},{"title":"1. Navigate to the providers section‚Äã","type":1,"pageTitle":"Providers","url":"/provider#1-navigate-to-the-providers-section","content":" Into the Left Pane, access the Setting option and then Providers and then, at the top right corner, click on the Create button.    ","version":"Next","tagName":"h3"},{"title":"2. Set up basic Information‚Äã","type":1,"pageTitle":"Providers","url":"/provider#2-set-up-basic-information","content":"   These are the settings you must define:  Setting\tDescriptionName\tSelect a name for the Organizative Unit into AWS under the needed accounts will be created. Region\tAWS region to use. If you want to know more about them, you can visit this documentation¬†https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html. Domain\tHere you must provide the domain you own in which the different environments will be deployed. It must be delegated to the Primary Route53 of SleakOps manually. Follow the steps described on this¬†https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/MigratingDNS.html. Email\tBy default SleakOps uses the email from the root account provided, If you want to use other email to register your SleakOps accounts in AWS, fill in this field.  Once you've completed the form, click on Next to move forward.  ","version":"Next","tagName":"h3"},{"title":"3. Connect to your AWS Root Account‚Äã","type":1,"pageTitle":"Providers","url":"/provider#3-connect-to-your-aws-root-account","content":" warning You must be logged into your AWS Root Account.  To start installing your application, we need to connect to your AWS Root Account. Here's how to do it:  By clicking the Next button, you‚Äôll be redirected to AWS to create an IAM role in your main account called &quot;SleakopsIntegrationRole&quot;.  This role lets us access necessary resources, making installation quick and smooth.After installation, we will remove this role to keep your account secure.    ","version":"Next","tagName":"h3"},{"title":"4. Ongoing Organizative Unit process started‚Äã","type":1,"pageTitle":"Providers","url":"/provider#4-ongoing-organizative-unit-process-started","content":" note Creating your Organizative Unit doesn't generate any cost on your AWS account üòÉ  Once the connection is established role is created, SleakOps will automatically start the Organizative Unit creation.  This process will take a few minutes.    ","version":"Next","tagName":"h3"},{"title":"5. Learn about the infrastructure architecture created by SleakOps for you.‚Äã","type":1,"pageTitle":"Providers","url":"/provider#5-learn-about-the-infrastructure-architecture-created-by-sleakops-for-you","content":" In order to understand what was created on your AWS, please see Accounts. ","version":"Next","tagName":"h3"},{"title":"Accounts","type":0,"sectionRef":"#","url":"/provider/accounts","content":"","keywords":"","version":"Next"},{"title":"Provider's Accounts‚Äã","type":1,"pageTitle":"Accounts","url":"/provider/accounts#providers-accounts","content":" Sleakops implements a well-defined infrastructure architecture designed to optimize operational excellence while ensuring a secure and scalable environment for users. The architecture consists of four accounts, each serving distinct purposes and isolated from one another.  Each account has a VPN instance generated upon the creation of the first cluster.  Once the Accounts are up, we set to each one of them what we call Network Module it contains a lot of different AWS services that are used to make the network connections inside accounts.    ","version":"Next","tagName":"h2"},{"title":"Security Account‚Äã","type":1,"pageTitle":"Accounts","url":"/provider/accounts#security-account","content":" The Security Account serves as a centralized hub for managing IAM users and their access to the system. Learn how to switch between accounts in AWS Console Autentication.  ","version":"Next","tagName":"h3"},{"title":"Management Account‚Äã","type":1,"pageTitle":"Accounts","url":"/provider/accounts#management-account","content":" Designed to maintain internal services used for application maintenance, regardless of whether they are shared across accounts. Example: Sentry.  Contains an EKS cluster with integrated CI/CD (GitHub and HashiCorp Vault).Vault manages credentials for CloudWatch, enhancing monitoring capabilities.VPC Peering enables private connections to other accounts.  ","version":"Next","tagName":"h3"},{"title":"Development Account‚Äã","type":1,"pageTitle":"Accounts","url":"/provider/accounts#development-account","content":" For the different stages of your application before it goes into production.  Contains three environments: dev, QA, and staging.Replicas of the prod environment for code writing, testing, and pre-releases.Ensures isolated testing to prevent issues for external users.Similar architecture to prod but without RDS Slave for reduced high availability requirements.  ","version":"Next","tagName":"h3"},{"title":"Production Account‚Äã","type":1,"pageTitle":"Accounts","url":"/provider/accounts#production-account","content":" This account is intended for your application to be installed in a production environment, isolated from the rest of your application's stages.  Supports external users and requires a fully functional database (RDS Master).Utilizes Private DB Subnet for RDS Master, RDS Slave, and ElastiCache, each on different Availability Zones (AZs) for high availability.Backend Deployment with replicas distributed across different AZs.Frontend Deployment with LoadBalancer for even distribution of network load.Route53 serves as DNS and performs health checks for the application.AWS CloudFront serves static frontend content from an S3 bucket.RDS Slave acts as a replica of RDS Master for failover scenarios, maximizing uptime.  ","version":"Next","tagName":"h3"},{"title":"Selecting an Account in SleakOps‚Äã","type":1,"pageTitle":"Accounts","url":"/provider/accounts#selecting-an-account-in-sleakops","content":" To select an account and be able to work on it, select it from the left pane. The left icon refers to the Provider that groups the accounts. ","version":"Next","tagName":"h3"},{"title":"Common errors on Providers","type":0,"sectionRef":"#","url":"/provider/common-errors","content":"","keywords":"","version":"Next"},{"title":"1. The Account ID set does not have root access‚Äã","type":1,"pageTitle":"Common errors on Providers","url":"/provider/common-errors#1-the-account-id-set-does-not-have-root-access","content":" In this case, by clicking the Fix button, you‚Äôll be redirected to AWS again.  Be sure to be logged as a Root user.    ","version":"Next","tagName":"h3"},{"title":"2. Maximum Number of AWS Accounts Reached‚Äã","type":1,"pageTitle":"Common errors on Providers","url":"/provider/common-errors#2-maximum-number-of-aws-accounts-reached","content":" AWS has an account limit that can prevent new ones.  Before retrying the process, increase that limit. Otherwise, the process will fail again.    ","version":"Next","tagName":"h3"},{"title":"3. Other errors‚Äã","type":1,"pageTitle":"Common errors on Providers","url":"/provider/common-errors#3-other-errors","content":" Other issues might happen and usually, they'll be solved by running the Connection to AWS again.  If the error remains and you've tried deleting the provider and creating a new one. Please, do not hesitate and report us an issue.   ","version":"Next","tagName":"h3"},{"title":"Deleting a Provider","type":0,"sectionRef":"#","url":"/provider/deleting-a-provider","content":"","keywords":"","version":"Next"},{"title":"How to delete a Provider‚Äã","type":1,"pageTitle":"Deleting a Provider","url":"/provider/deleting-a-provider#how-to-delete-a-provider","content":" ","version":"Next","tagName":"h2"},{"title":"1. Select the provider to be deleted‚Äã","type":1,"pageTitle":"Deleting a Provider","url":"/provider/deleting-a-provider#1-select-the-provider-to-be-deleted","content":" Once you are in the Providers section, choose a provider and click on the Three Dots **button to display the Delete option. Click on it.    ","version":"Next","tagName":"h3"},{"title":"2. Confirm the procedure‚Äã","type":1,"pageTitle":"Deleting a Provider","url":"/provider/deleting-a-provider#2-confirm-the-procedure","content":" You'll see a modal to confirm the action. Remember this action will remove all the infrastructure created on AWS under this Provider.    ","version":"Next","tagName":"h3"},{"title":"3. Manually remove the Organization and its Accounts‚Äã","type":1,"pageTitle":"Deleting a Provider","url":"/provider/deleting-a-provider#3-manually-remove-the-organization-and-its-accounts","content":" As it was mentioned before the created Organization and its accounts (management, development, production, and security) will not be automatically deleted.  Access your AWS Root Account to manually remove them by going to AWS Organizations.   ","version":"Next","tagName":"h3"},{"title":"Designing your Infra","type":0,"sectionRef":"#","url":"/provider/schemas","content":"","keywords":"","version":"Next"},{"title":"Single Schema Vs. Multi Schema‚Äã","type":1,"pageTitle":"Designing your Infra","url":"/provider/schemas#single-schema-vs-multi-schema","content":" SleakOps provides the flexibility and control necessary to build infrastructure tailored to your specific requirements.  While we recommend adopting a Multiple Schema configuration to align with best practices, we understand that different stages of your project may require alternative schema configurations.    Here a comparative with two options:  \tMulti Schema ‚≠êÔ∏è\tSingle SchemaDescription\tAligned with best practices. You'll set first the Development account.\tCentralizes your environments within a single cluster. Account to be used\tUse all accounts as described here\tOnly the Production account Pros\tIncreases Security by granting access per account Production remains isolated.\tReduces costs, as it is just one cluster. Cons\tMore expensive as each environment'll have it own cluster and VPN.\tLess secure, ass all environments shares the account.  These are just two options; you have the freedom to create the schema that best suits your needs.  ","version":"Next","tagName":"h2"},{"title":"Multi-Schema Example‚Äã","type":1,"pageTitle":"Designing your Infra","url":"/provider/schemas#multi-schema-example","content":"   ","version":"Next","tagName":"h2"},{"title":"Single-Schema Example‚Äã","type":1,"pageTitle":"Designing your Infra","url":"/provider/schemas#single-schema-example","content":"  ","version":"Next","tagName":"h2"},{"title":"Shared Responsibility Model","type":0,"sectionRef":"#","url":"/responsability-model","content":"Shared Responsibility Model We transparently utilize all AWS services, which means the responsibility extends to the guidelines set by AWS. To safeguard your data and ensure it doesn't get lost due to potential service disruptions, it's advisable to have backup policies in place. Currently, we don't support backups for our dependencies (RDS, S3, RabbitMQ, etc.). To set this up, you can access via your AWS client and define your backup policies. [Learn more about AWS's Shared Responsibility Model] (https://aws.amazon.com/compliance/shared-responsibility-model/)","keywords":"","version":"Next"},{"title":"User","type":0,"sectionRef":"#","url":"/user","content":"","keywords":"","version":"Next"},{"title":"User Creation‚Äã","type":1,"pageTitle":"User","url":"/user#user-creation","content":" Sleakops has three fundamental fields of user permissions:  Viewer (Read Only)‚Äã  Objective: Provide read-only access to monitor and review infrastructure without making changesScope: View all projects, clusters, and environmentsAccess monitoring dashboards and logsReview deployment history and configurationsCannot create, modify, or delete any resources AWS IAM Policy: ReadOnlyAccess Examples: DevOps engineers who need to monitor production environmentsSecurity auditors reviewing infrastructure complianceTeam leads who need visibility into project status  Editor (Power User)‚Äã  Objective: Enable infrastructure management and deployment capabilities while maintaining security boundariesScope: Create and manage projects, clusters, and environmentsDeploy applications and manage infrastructure resourcesConfigure environments, dependencies, and networkingCannot manage other users and billing AWS IAM Policy: PowerUserAccess Examples: Senior developers deploying applicationsDevOps engineers managing infrastructureTeam members responsible for specific project deployments  Admin (Administrator)‚Äã  Objective: Provide complete platform control including user management and billingScope: All Editor capabilities plus user management and billing sectionCreate, modify, and delete user accountsAssign roles and permissions to other usersAccess to all AWS accounts without restrictionsPlatform configuration and settings management AWS IAM Policy: AdministratorAccess Examples: Platform administratorsTeam leads managing multiple projectsDevOps managers with cross-team responsibilities  ","version":"Next","tagName":"h2"},{"title":"Access Configuration‚Äã","type":1,"pageTitle":"User","url":"/user#access-configuration","content":" AWS Account Access: This field shows you every account, here you select to which accounts the user (Editor or Read-only) will have access.  VPN Account Access: It's similar to the AWS account accesses field but here you set if a user it's also created on the VPN Server of the account you give. More information can be checked on VPN documentation    For access into the AWS accounts SleakOps initially sets a random password and sends it to the email of the created user. The user can login with that password but it will be obligated to change its password on the first login. For SleakOps platform access we use the password that was set on the User form.  After this user creation an AWS User will be created on the 'security' Account, this account is where we control acesses to all the SleakOps AWS accounts. We will also create, depending on the configuration, users on the VPN servers, read how to use them on the corresponding documentation and on the SleakOps user.  ","version":"Next","tagName":"h3"},{"title":"Users Without SleakOps Access‚Äã","type":1,"pageTitle":"User","url":"/user#users-without-sleakops-access","content":" Some users in your organization may not have direct access to the SleakOps platform but still interact with the infrastructure:  ","version":"Next","tagName":"h2"},{"title":"External Collaborators‚Äã","type":1,"pageTitle":"User","url":"/user#external-collaborators","content":" Scenario: Third-party contractors or consultants who need temporary access to specific AWS resourcesAccess Method: Direct AWS console access with limited IAM permissionsManagement: Handled through AWS IAM directly, not through SleakOps user management  ","version":"Next","tagName":"h3"},{"title":"Read-Only Observers‚Äã","type":1,"pageTitle":"User","url":"/user#read-only-observers","content":" Scenario: Stakeholders who need visibility into infrastructure costs and usage without operational accessAccess Method: AWS Cost Explorer, CloudWatch dashboards, or custom reporting toolsManagement: AWS billing and monitoring permissions only  ","version":"Next","tagName":"h3"},{"title":"VPN-Only Consumers‚Äã","type":1,"pageTitle":"User","url":"/user#vpn-only-consumers","content":" Scenario: Users who need to consume workloads and applications within the VPN but don't require infrastructure administration or monitoring capabilitiesAccess Method: VPN access only, with limited permissions to access specific applications and servicesManagement: VPN user accounts with restricted access to designated workloads and environments ","version":"Next","tagName":"h3"},{"title":"AWS Console Authentication","type":0,"sectionRef":"#","url":"/user/aws_console_authentication","content":"AWS Console Authentication ::: tip[VIDEO] How to enter to any of your Accounts. ::: As described in the Architecture Overview. You'll have to enter the 'security' account, then, assume the role on the account you want. The easiest way to do this is by using the Sleakops Dashboard: First, use the AWS Login button: This will open the AWS login form. The Account ID field should be automatically filled with the 'security' account ID. If this doesn't happen, it might be because another service is attempting to fill the fields. Once logged into the 'security' account, it will appear as shown in the following image: Now, in the AWS console, you need to return to the SleakOps dashboard, select 'Get Access' and use the drawer, on it, select the account you want to log in to. This will prompt a new AWS tab to switch the role from your 'security' Account into the account you've selected, you'll leave the 'security account' and enter the selected one. info If you're in 'security' or another account you can directly use the account switchers, AWS understands that you are already inside the 'security' account. For more information about this process, you can read its AWS documentation .","keywords":"","version":"Next"},{"title":"Django + Celery","type":0,"sectionRef":"#","url":"/quickstart/django_celery","content":"","keywords":"","version":"Next"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#prerequisites","content":" Account in SleakopsA Cluster in this account. If you don't have it, here is the documentation on how to do it.A Environment configured. If you don't have it, here is the documentation on how to do itDjango project configured with celery (This project needs to have docker).  ","version":"Next","tagName":"h2"},{"title":"Let's Start‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#lets-start","content":" For this example, we are going to use this project . It is a Django project with Celery that already has Docker configured to run. We are also going to configure a Postgresql database, S3 bucket and Rabbitmq needed for this project.  ","version":"Next","tagName":"h2"},{"title":"Create a project‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-a-project","content":" To start, we are going to create a new project. To do this, click the &quot;Projects&quot; button in the left panel:    Inside the Projects panel you will be able to see all the projects you have and manage them from here. We want to create a new one so let's click on the ‚Äúcreate‚Äù button at the top right:    In the project creation screen we have the following fields:  Setting\tDescriptionEnvironment\tWe have to select the previously created environment. Nodepool\tWe will leave the default one. Repositories\tWe will select our repository that we want to deploy. In our case example-django-celery. Project Name\tWe can define a project name. For the example we will leave the default. Branch\tIt has to coincide with the one we have in our project. In our case it is ‚ÄúMain‚Äù. Dockerfile path\tIt is the relative path to the dockerfile in your project.  Once configured all that we create the project with the ‚ÄúSubmit‚Äù button at the bottom right:    With that, the project begins to be created. In the meantime we go to the workloads with the ‚ÄúWorkloads‚Äù button in the left panel:    ","version":"Next","tagName":"h3"},{"title":"Create a Web Service‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-a-web-service","content":" Here what we are going to do is to create a web service so we go to the web service section and create one:    In this page we are going to complete the first form with the following fields:  Setting\tDescriptionProject\tWe select the project we created previously, in our case ‚Äúexample-django-celery‚Äù. Name\tWe define a name for the web service. Command\tBy default this will take the value that is in the dockerfile, in our case this is fine. Port\tThe same as the command.  Then we continue by clicking on the ‚ÄúNext‚Äù button up to step 3:    In step 3 we have to edit the path field and put the endpoint of healthcheck which in our case is ‚Äú/healthcheck/‚Äù. Then click on the ‚ÄúNext‚Äù button until the web service is created:    ","version":"Next","tagName":"h3"},{"title":"Deploy celery worker‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#deploy-celery-worker","content":" Well, with this we can see our web service deploying. Now we are going to deploy the celery. For this we have to go to the workers section inside the same workloads screen:    And click on the ‚ÄúCreate‚Äù button to create a new one:    In the workers creation screen we will have to complete the following fields:  Setting\tDescriptionProject\tSelect the previously created project. In our case ‚Äúexample-django-celery‚Äù. Name\tWe define the name that we are going to give to the worker. In our case ‚Äúcelery‚Äù. Command\tHere we set the command to run celery, in our case it is: bash celery -A core.celery_app worker -l INFO --concurrency 1 --max-tasks-per-child 1 --prefetch-multiplier 1 -n celery@%h --queues default,build,deployment,cluster,canvas,billing  With these fields filled in we will click on the ‚ÄúNext‚Äù button at the bottom right and then ‚ÄúSubmit‚Äù as we do not need to edit anything else:    With this we will see our celery published. Now we have to configure the hooks. For this we go to the hooks section:    ","version":"Next","tagName":"h3"},{"title":"Create a migration hook‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-a-migration-hook","content":" In the hook creation screen we will have the following fields:  Setting\tDescriptionProject\tSelect the previously created project. In our case ‚Äúexample-django-celery‚Äù. Name\tWe define the name that we are going to give to the worker. In our case ‚Äúmigrations‚Äù. Command\tHere we set the command to run celery, in our case it is: bash python manage.py migrate --no-input   With these fields filled in we will click on the ‚ÄúNext‚Äù button at the bottom right and then ‚ÄúSubmit‚Äù as we do not need to edit anything else:    ","version":"Next","tagName":"h3"},{"title":"Create a collect static hook‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-a-collect-static-hook","content":" Now we proceed to create another hook that we need for the statics:    In this form we are going to do the same as the previous one but modifying the command. We click next until we create the hook (without modifying anything else):    The command we use is as follows:  python manage.py collectstatic --no-input   ","version":"Next","tagName":"h3"},{"title":"Create a Postgresql Database‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-a-postgresql-database","content":" Once we have created the hooks we have to go to create our database. To do this we go to the ‚ÄúDependencies‚Äù section:    Inside this section we click on the ‚ÄúCreate‚Äù button at the top right and then select ‚ÄúPostgresql‚Äù:      In the 1st postgresql creation form we will have to select our previously created project and define a name for it, then click on the ‚ÄúNext‚Äù button at the bottom right:    In the 2nd form we are going to have a lot of fields, the only ones that matter to us are the following:  Setting\tDescriptionDatabase Master Username\tHere we assign a root user name to our database. Database Master Password\tA password for this root user.  With these fields filled in, we are ready to move on. Click on the ‚ÄúNext‚Äù button at the bottom right to proceed to the third form:    In this last form, we are going to adjust the environment variables we have in our project with respect to the database. To do this, we need to change the following variables to our own:  Before\tAfter*_POSTGRESQL_NAME\tDB_NAME *_POSTGRESQL_USERNAME\tDB_USER *_POSTGRESQL_PASSWORD\tDB_PASSWORD *_POSTGRESQL_ADDRESS\tDB_HOST *_POSTGRESQL_PORT\tDB_PORT  It should look something like the image below. Then click on the ‚ÄúSubmit‚Äù button and your database should be created:    ","version":"Next","tagName":"h3"},{"title":"Create S3 Bucket‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-s3-bucket","content":" In the same page of dependencies we have to create our s3 bucket, for it we are going to go to the ‚ÄúCreate‚Äù button again:    And select S3 Bucket:    In the first form we have to select our previously created project and define a name for the bucket, we have to take into account that the name of the bucket is global so it has to be unique. Now click on the ‚ÄúNext‚Äù button and go to step 3:    Here we are going to see some environment variables defined for the bucket. We are going to edit the one that says COLLECTSTATICEXAMPLEDJANGOCELERY_BUCKET_NAME and we are going to call it DJANGO_AWS_STORAGE_BUCKET_NAME. With this simple change we click on the ‚ÄúSubmit‚Äù button at the bottom right to finish creating the bucket:    ","version":"Next","tagName":"h3"},{"title":"Create Rabbitmq‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-rabbitmq","content":" Now we need one more dependency. Our Rabbitmq to queue celery tasks, so let's get to it:    And select Rabbitmq:    In the first form we will have to select our project and define a name for it. Then click on the ‚ÄúNext‚Äù button at the bottom right:    In the following form we have several fields but the only ones that matter to us for this example are the username and password, we can define whatever we want. For this example I chose admin as username and for the password I generated it randomly with the dice button. Then we click on the ‚ÄúNext‚Äù button to go to the next form:    In this last form we have to change the name of the variable that ends in *_BROKER_AUTH_URL to CELERY_BROKER_URL (as shown in the image). Then we click on the ‚ÄúSubmit‚Äù button at the bottom right to finish creating rabbitmq:    ","version":"Next","tagName":"h3"},{"title":"Create yours environment variables‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#create-yours-environment-variables","content":" Once the dependencies are deployed we have to configure our environment variables. We are going to go to the Vargroups section:    Here you will see all your environment variables that you created grouped in groups, for example you should have created one with the data for the database (which is the one you see in the image). Now we are going to create another one for our django environment variables, for this we click on the ‚ÄúCreate‚Äù button at the top right:    In this form we have the following fields:  Project: we select the project we created previously.Workload: We select ‚Äúglobal‚Äù that makes reference to be used by all our workloads.Name: We define a name for this group of variables.Type: If we want to load it by file or by variable.Vars: Here we enable the textmode and copy the following environment variables:  CELERY_RESULT_BACKEND=django-db DJANGO_ADMIN_URL=admin/ DJANGO_DEBUG=False DJANGO_SECRET_KEY=secret_key DJANGO_SETTINGS_MODULE=core.settings.production DJANGO_STATIC_STORAGE=storages.backends.s3boto3.S3StaticStorage DB_ENGINE=django.db.backends.postgresql_psycopg2 ENVIRONMENT=production LOGS_LEVEL=INFO PYTHONPATH=.   These environment variables are required for our example project. Finally click on the ‚ÄúSubmit‚Äù button at the bottom right to create the variable group.    ","version":"Next","tagName":"h3"},{"title":"Deployments‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#deployments","content":" As last step we are going to see our project deployed, for this we go to the ‚ÄúDeployments‚Äù section of the left panel:    Here we are going to see all the deploys that we do. In our case it is the first one and we can see that it has been created correctly, in case you see any error if you click on ‚Äúerror‚Äù you can see a description of it. If we do not see any error then it means that the project is already deployed, we could begin to use it from the url that the web service provided us.    This concludes our project deployment process. We leave you an optional step which is to configure the ci with github.  ","version":"Next","tagName":"h3"},{"title":"Optional‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#optional","content":" ","version":"Next","tagName":"h2"},{"title":"CI with Github‚Äã","type":1,"pageTitle":"Django + Celery","url":"/quickstart/django_celery#ci-with-github","content":" Every time you make a change in your code and want to deploy it you will have to do a build and a deploy, this eventually becomes tedious. That's why to avoid this we have to implement ci on github.  For this we are going to go to ‚ÄúProjects‚Äù in the left panel:    Let's locate our project and click on the gear to access the project configuration:    In the project configuration we locate the one that says ‚ÄúGit pipelines‚Äù and click on it:    Here we are going to find what we need to do this. Basically we need to set up a file in the root of our project .github/workflows/ called ci_sleakops_demo.yml and in that file we are going to paste the content that appears in this page.    This needs to have an environment variable SLEAKOPS_KEY, if you don't have it you have to go to the link that appears there Settings -&gt; CLI, get it and save it as an environment variable.  With this configured and deployed every time you do a push to your ‚Äúmain‚Äù branch a new version of your application will be launched automatically. ","version":"Next","tagName":"h3"},{"title":"Connect into the VPN","type":0,"sectionRef":"#","url":"/user/vpn","content":"Connect into the VPN To handle VPN connections we use Pritunl. You can download the client here. Once you've created a Provider , go to SleakOps dashboard and select the account for which you want to get VPN access. Remember, to do this, you need to have access to the VPN of that account, but the VPN must also be created. We create the VPN of a specific account when the first cluster of that account is created. This will prompt you with what is called the URI profile. It has a validation period of 24 hours, and you must load it into the Pritunl client. Copy it and import it into the Pritunl client, and you'll be able to connect to it:","keywords":"","version":"Next"},{"title":"Welcome to SleakOps!","type":0,"sectionRef":"#","url":"/","content":"","keywords":"","version":"Next"},{"title":"Guiding principles‚Äã","type":1,"pageTitle":"Welcome to SleakOps!","url":"/#guiding-principles","content":" Respect the AWS¬†well architected bases.Keeping always an eye on costs.You have full control, it‚Äôs your repo and your cloud.  ","version":"Next","tagName":"h2"},{"title":"Main Features‚Äã","type":1,"pageTitle":"Welcome to SleakOps!","url":"/#main-features","content":" GitHub, Bitbucket and Gitlab integrationBased on your repo and DockerfilesManage multiple environments using our proposed structure (dev, staging and production) or customize your own.Configure your CI/CD pipeline.Complete Observability stack for logging, monitoring and tracing.Secrets and env vars managment.Secure connections by TLS.Automated configuration for your services behind a load balancer and secure¬†ingress.Easy add AddOns to your cluster.Lot of dependencies ready to go (RDS, S3, Redis, SQL, rRabbit, etc.).Users access to services management.Automated VPNs.  Providers A cloud provider account. Getting Started Clusters A set of worker machines, called nodes, that run containerized applications. Getting Started Environments Abstraction that let us isolate the different resources. Getting Started Projects Represents a codebase and it is managed by a git repository. Getting Started Dependencies Pieces of underlying infrastructure your apps need to run in the cloud, such as relational databases, storage services or caches. Getting Started Workloads An abstract way to expose an application running on a set of Pods as a network service. Getting Started Deployments ... Getting Started Build Represents a deployable state of all the services. Getting Started Var Group Dictionary that provides configuration for services. Getting Started ","version":"Next","tagName":"h2"}],"options":{"languages":["en","es"],"indexBaseUrl":true,"id":"default"}}